<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[iOS开发-传感器坐标系问题]]></title>
    <url>%2F2018%2F09%2F12%2FiOS%E5%BC%80%E5%8F%91-%E4%BC%A0%E6%84%9F%E5%99%A8%E5%9D%90%E6%A0%87%E7%B3%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[传感器坐标系移动设备内置的传感器采集得到的数据均源自传感器本体坐标系下。然而，在现实应用中，常常有如下需求：求取传感器数据在世界坐标系下的数据。例如，导航应用。 常见的手机本体坐标系如下图所示： 世界坐标系如下图所示： 为了区分与手机坐标系的区别，这里把地球坐标系加一个下标w，在此坐标系中： zw 垂直与地面向上yw与zw垂直，指向北。xw垂直与zw与yw确定的平面，指向东。 加速度计iOS的加速度传感器能够提供很多有用的数据，比如rotationMatrix和指向三轴方向的分量x、y、z。iOS SDK提供的传感器管理类CMDeviceMotion用来获取大多数的传感器数据，对于加速度来说，主要是userAccelerate分量数据。 1234567891011import CoreMotionvar motionManager = CMMotionManager()...motionManager.startDeviceMotionUpdatesUsingReferenceFrame(CMAttitudeReferenceFrameXTrueNorthZVertical, toQueue: NSOperationQueue.currentQueue(), withHandler: &#123; ... var acc: CMAcceleration = deviceMotion.userAcceleration var rot = deviceMotion.attitude.rotationMatrix self.ax = (acc.x*rot.m11 + acc.y*rot.m21 + acc.z*rot.m31)*9.81 self.ay = (acc.x*rot.m12 + acc.y*rot.m22 + acc.z*rot.m32)*9.81 self.az = (acc.x*rot.m13 + acc.y*rot.m23 + acc.z*rot.m33)*9.81&#125;) 使用startDeviceMotionUpdatesUsingReferenceFrame方法可以设置参考系，这个参考系只对CMDeviceMotion中的attitude变量有效。也就是说在Block中得到的attitude是相对于设定的参考系的，而参考系的设定可以根据自己的需求来进行，通常有一下集中可设定的值： 1234567891011121314151617181920212223242526272829303132/* * CMAttitudeReferenceFrame * * Discussion: * CMAttitudeReferenceFrame indicates the reference frame from which all CMAttitude * samples are referenced. * * Definitions of each reference frame is as follows: * - CMAttitudeReferenceFrameXArbitraryZVertical describes a reference frame in * which the Z axis is vertical and the X axis points in an arbitrary direction * in the horizontal plane. * - CMAttitudeReferenceFrameXArbitraryCorrectedZVertical describes the same reference * frame as CMAttitudeReferenceFrameXArbitraryZVertical with the following exception: * when available and calibrated, the magnetometer will be used to correct for accumulated * yaw errors. The downside of using this over CMAttitudeReferenceFrameXArbitraryZVertical * is increased CPU usage. * - CMAttitudeReferenceFrameXMagneticNorthZVertical describes a reference frame * in which the Z axis is vertical and the X axis points toward magnetic north. * Note that using this reference frame may require device movement to * calibrate the magnetometer. * - CMAttitudeReferenceFrameXTrueNorthZVertical describes a reference frame in * which the Z axis is vertical and the X axis points toward true north. * Note that using this reference frame may require device movement to * calibrate the magnetometer. */typedef NS_OPTIONS(NSUInteger, CMAttitudeReferenceFrame) __TVOS_PROHIBITED &#123; CMAttitudeReferenceFrameXArbitraryZVertical = 1 &lt;&lt; 0, CMAttitudeReferenceFrameXArbitraryCorrectedZVertical = 1 &lt;&lt; 1, CMAttitudeReferenceFrameXMagneticNorthZVertical = 1 &lt;&lt; 2, CMAttitudeReferenceFrameXTrueNorthZVertical = 1 &lt;&lt; 3&#125;; 在上面获取数据的实例中，使用的是x轴指向地球北极，z轴指向地心，也就是地理上的地心方向。 通过attitude得到一个旋转向量rot，然后只要将原来相对于设备的参考系向量旋转到世界参考系中就可以了。 这个旋转使用了矩阵v_world = M^-1 v_device = M^T v_device。这里的M就是旋转矩阵rot，因为矩阵旋转的特性，一个矩阵的转置矩阵和逆矩阵是相等的。 而在计算的使用分量都乘以了一个9.81，其实这个就是物理上的g，所有的加速度都是以g为单位的。 坐标系转换 参考 如何获取iOS世界坐标系中的加速度 — 关于坐标系的转换 安卓坐标系转换之一：从手机坐标系到地球坐标系]]></content>
      <categories>
        <category>编程</category>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-元素最近邻快速查找]]></title>
    <url>%2F2018%2F09%2F11%2F%E7%AE%97%E6%B3%95-%E5%85%83%E7%B4%A0%E6%9C%80%E8%BF%91%E9%82%BB%E5%BF%AB%E9%80%9F%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[背景需求：给定一个元素，在指定集合中输出n个最近邻的元素。如果集合中没有重复元素，那么只有一个元素输出；如果集合中由重复元素，那么可能存在多个最近邻元素输出。 求解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npfrom numpy import *import mathfrom mpl_toolkits.mplot3d import Axes3Dimport csvdef ReadCsvData(file_name): time = [] loc = [] with open(file_name,'r') as csvfile: reader = csv.reader(csvfile) rows= [row for row in reader] for data in rows: data = np.array(data) time.append(float(data[0])) loc.append([float(data[1]), float(data[2]), float(data[3])]) time = np.array(time) loc = np.array(loc) return time, locdef searchInsert(nums, target): start = 0 end = len(nums) - 1 while start &lt;= end: mid = (start + end) // 2 if nums[mid] == target: return mid elif nums[mid] &lt; target: start = mid + 1 else: end = mid - 1 return end + 1 def rIndex(nums, target): n = len(nums) if n == 0: return -1 mid = searchInsert(nums, target) rlist = [] # 保持索引 i, j = -1, n left, rigth = 0, 0 # 左右扩展的标志 mxg = float('-inf') if 0 &lt; mid &lt; n: # 如果找到了 i, j = mid-1, mid mxg = min(abs(nums[i] - target), abs(nums[j] - target)) left, rigth = 1, 1 elif mid == 0: # 小于最左边的数字 j = mid mxg = abs(nums[j] - target) left, rigth = 0, 1 elif mid == n: # 大于最右边的数字 i = mid-1 mxg = abs(nums[i] - target) left, rigth = 1, 0 while left == 1 or rigth == 1: # 两边查找 if i == -1: left = 0 if j == n: rigth = 0 if left == 1 and i &gt;= 0: le = abs(nums[i] - target) if le == mxg: rlist = [i] + rlist i -= 1 else: left = 0 if rigth == 1 and j &lt; len(nums): ri = abs(nums[j] - target) if mxg == ri: rlist = rlist + [j] j += 1 else: rigth = 0 return rlistif __name__ == '__main__': ################# File path ####################### gt_path = "../data/ADVIO/advio-02/ground-truth/pose.csv" ARKit_path = "../data/ADVIO/advio-02/iphone/arkit.csv" ################# Read data ####################### time_gt = [] loc_gt = [] time_ARKit = [] loc_ARKit = [] errors = [] time_gt, loc_gt = ReadCsvData(gt_path) time_ARKit, loc_ARKit = ReadCsvData(ARKit_path) print len(time_gt), len(time_ARKit) if len(time_gt) &lt; len(time_ARKit): for i in range(len(time_gt)): index = rIndex(time_ARKit, time_gt[i])[0] error = math.sqrt((loc_gt[i][0] - loc_ARKit[index][0])**2 + (loc_gt[i][1] - loc_ARKit[index][1])**2 + (loc_gt[i][2] - loc_ARKit[index][2])**2) errors.append(error) else: for i in range(len(time_ARKit)): index = rIndex(time_gt, time_ARKit[i])[0] error = math.sqrt((loc_gt[index][0] - loc_ARKit[i][0])**2 + (loc_gt[index][1] - loc_ARKit[i][1])**2 + (loc_gt[index][2] - loc_ARKit[i][2])**2) errors.append(error) errors = np.array(errors) print errors plt.figure(1) plt.plot(loc_gt[:, 0], loc_gt[:, 2], c = 'r') plt.plot(loc_ARKit[:, 0], loc_ARKit[:, 2], c = 'b') plt.xlabel("X (m)") plt.ylabel("Y (m)") plt.legend(["Groud-truth", "ARKit"]) plt.figure(2) plt.plot(errors, c = 'b') plt.xlabel('Index') plt.ylabel('Error (m)') plt.show()]]></content>
      <categories>
        <category>编程</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉领域数据集]]></title>
    <url>%2F2018%2F09%2F11%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>学术</category>
        <category>计算机视觉</category>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>数据集</tag>
        <tag>学术</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apolloScape竞赛]]></title>
    <url>%2F2018%2F09%2F09%2FapolloScape%E7%AB%9E%E8%B5%9B%2F</url>
    <content type="text"><![CDATA[背景介绍百度ApolloScape重磅发布了自动驾驶开放数据集。自动驾驶开发测试中，海量、高质的真实数据是必不可缺的“原料”。但是，少有团队有能力开发并维持一个适用的自动驾驶平台，定期校准并收集新数据。 据介绍，Apollo开放平台此次发布的ApolloScape不仅开放了比Cityscapes等同类数据集大10倍以上的数据量，包括感知、仿真场景、路网数据等数十万帧逐像素语义分割标注的高分辨率图像数据，进一步涵盖更复杂的环境、天气和交通状况等。从数据难度上来看，ApolloScape数据集涵盖了更复杂的道路状况（例如，单张图像中多达162辆交通工具或80名行人），同时开放数据集采用了逐像素语义分割标注的方式，是目前环境最复杂、标注最精准、数据量最大的自动驾驶数据集。 Apollo开放平台还将与加州大学伯克利分校在CVPR 2018（IEEE国际计算机视觉与模式识别会议）期间联合举办自动驾驶研讨会（Workshop on Autonomous Driving），并将基于ApolloScape的大规模数据集定义了多项任务挑战，为全球自动驾驶开发者和研究人员提供共同探索前沿领域技术突破及应用创新的平台。 参考一：PoseNet implementation for self-driving car localization using Pytorch on Apolloscape datasetThis article covers the very beginning of the journey and includes the reading and visualization of the Apolloscape dataset for localization task. Implement PoseNet [2] architecture for monocular image pose prediction and visualize results. I use Python and Pytorch for the task. NOTE: If you want to jump straight to the code here is the GitHub repo. It’s is still an ongoing work where I intend to implement Vidloc [7], Pose Graph Optimization [3,8] and Structure from Motion [9] pipelines for Apolloscape Dataset in the context of the localization task. Apolloscape Pytorch DatasetFor Pytorch I need to have a Dataset object that prepares and feeds the data to the loader and then to the model. I want to have a robust dataset class that can: support stereo and mono images support train/validation splits that came along with data or generate a new one support pose normalization support different pose representations (needed mainly for visualization and experiments with loss functions) support filtering by record id support general Apolloscape folder structure layout I am not putting here the full listing of the Apolloscape dataset and concentrate solely on how to use it and what data we can get from it. For the full source code, please refer to the Github file datasets/apolloscape.py. Here how to create a dataset: 12345678910111213from datasets.apolloscape import Apolloscapefrom torchvision import transforms# Path to unpacked data foldersAPOLLO_PATH = "./data/apolloscape"# Resize transform that is applied on every image readtransform = transforms.Compose([transforms.Resize(250)])apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road="zpark-sample", transform=transform, train=True, pose_format='quat', stereo=True)print(apollo_dataset) output: 123456789Dataset: Apolloscape Road: zpark-sample Record: None Train: None Normalize Poses: False Stereo: True Length: 1499 of 1499 Cameras: ['Camera_2', 'Camera_1'] Records: ['Record001', 'Record002', 'Record003', 'Record004', 'Record006', 'Record007', 'Record008', 'Record009', 'Record010', 'Record011', 'Record012', 'Record013', 'Record014'] POLLO_PATH is a folder with unpacked Apolloscape datasets, e.g. \$APOLLO_PATH/road02_seg or \$APOLLO_PATH/zpark. Download data from Apolloscape page and unpack iot. Let’s assume that we’ve also created a symlink ./data/apolloscape that points to $APOLLO_PATH folder. We can view the list of available records with a number of data samples in each: 12345# Show records with numbers of data pointsrecs_num = apollo_dataset.get_records_counts()recs_num = sorted(recs_num.items(), key=lambda kv: kv[1], reverse=True)print("Records:")print("\n".join(["\t&#123;&#125; - &#123;&#125;".format(r[0], r[1]) for r in recs_num ])) output: 1234567891011121314Records: Record008 - 122 Record007 - 121 Record006 - 121 Record012 - 121 Record001 - 121 Record009 - 121 Record010 - 121 Record003 - 121 Record013 - 120 Record004 - 120 Record002 - 120 Record011 - 120 Record014 - 50 We can draw a route for one record with a sampled camera image: 123456from utils.common import draw_record# Draw path of a record with a sampled datapointrecord = 'Record008'draw_record(apollo_dataset, record)plt.show() output: Alternatively, we can see all records at once in one chart: 123# Draw all records for current datasetdraw_record(apollo_dataset)plt.show() output: Another option is to see it in a video: 12345from utils.common import make_video# Generate and save video for the recordoutfile = "./output_data/videos/video_&#123;&#125;_&#123;&#125;.mp4".format(apollo_dataset.road, apollo_dataset.record)make_video(apollo_dataset, outfile=outfile) Output (cut gif version of the generated video): For the PoseNet training we will use mono images with zero-mean normalized poses and camera images center-cropped to 250px: 12345678910111213141516# Resize and CenterCroptransform = transforms.Compose([ transforms.Resize(260), transforms.CenterCrop(250)])# Create train dataset with mono images, normalized poses, enabled cache_transformtrain_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road="zpark-sample", transform=transform, train=True, pose_format='quat', normalize_poses=True, cache_transform=True, stereo=False)# Draw path of a single record (mono with normalized poses)record = 'Record008'draw_record(apollo_dataset, record)plt.show() Output: Implemented Apolloscape Pytorch dataset also supports cache_transform option which is when enabled saves all transformed pickled images to a disk and retrieves it later for the subsequent epochs without the need to redo convert and transform operations every image read event. Cache saves up to 50% of the time during training time though it’s not working with image augmentation transforms like torchvision.transforms.ColorJitter. Also, we can get the mean and the standard deviation that we need later to recover true poses translations: 1234poses_mean = train_dataset.poses_meanposes_std = train_dataset.poses_stdprint('Translation poses_mean = &#123;&#125; in meters'.format(poses_mean))print('Translation poses_std = &#123;&#125; in meters'.format(poses_std)) Output: 12Translation poses_mean = [ 449.95782055 -2251.24771214 40.17147932] in metersTranslation poses_std = [123.39589457 252.42350964 0.28021513] in meters You can find all mentioned examples in Apolloscape_View_Records.ipynb notebook. And now let’s turn to something useful and more interesting, for example, training PoseNet deep convolutional network to regress poses from camera images. PoseNet localization task参考：PoseNet implementation for self-driving car localization using Pytorch on Apolloscape dataset A Pytorch implementation of the PoseNet model using a mono image: 123456789101112131415161718192021222324252627282930313233import torchimport torch.nn.functional as Fclass PoseNet(torch.nn.Module): def __init__(self, feature_extractor, num_features=128, dropout=0.5, track_running_stats=False, pretrained=False): super(PoseNet, self).__init__() self.dropout = dropout self.feature_extractor = feature_extractor self.feature_extractor.avgpool = torch.nn.AdaptiveAvgPool2d(1) fc_in_features = self.feature_extractor.fc.in_features self.feature_extractor.fc = torch.nn.Linear(fc_in_features, num_features) # Translation self.fc_xyz = torch.nn.Linear(num_features, 3) # Rotation in quaternions self.fc_quat = torch.nn.Linear(num_features, 4) def extract_features(self, x): x_features = self.feature_extractor(x) x_features = F.relu(x_features) if self.dropout &gt; 0: x_features = F.dropout(x_features, p=self.dropout, training=self.training) return x_features def forward(self, x): x_features = self.extract_features(x) x_translations = self.fc_xyz(x_features) x_rotations = self.fc_quat(x_features) x_poses = torch.cat((x_translations, x_rotations), dim=1) return x_poses For further experiments I’ve also implemented stereo version (currently it’s simply processes two images in parallel without any additional constraints), option to switch off stats tracking for BatchNorm layers and Kaiming He normal for weight initialization [4]. Full source code is here models/posenet.py PoseNet Loss FunctionsFor more details on where it came from and intro to Bayesian Deep Learning (BDL) you can refer to an excellent post by Alex Kendall where he explains different types of uncertainties and its implications to the multi-task models. And even more results you can find in papers “Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.” [5] and “What uncertainties do we need in Bayesian deep learning for computer vision?.” [6]. Pytorch implementation for both versions of a loss function is the following: 123456789101112131415161718class PoseNetCriterion(torch.nn.Module): def __init__(self, beta = 512.0, learn_beta=False, sx=0.0, sq=-3.0): super(PoseNetCriterion, self).__init__() self.loss_fn = torch.nn.L1Loss() self.learn_beta = learn_beta if not learn_beta: self.beta = beta else: self.beta = 1.0 self.sx = torch.nn.Parameter(torch.Tensor([sx]), requires_grad=learn_beta) self.sq = torch.nn.Parameter(torch.Tensor([sq]), requires_grad=learn_beta) def forward(self, x, y): # Translation loss loss = torch.exp(-self.sx) * self.loss_fn(x[:, :3], y[:, :3]) # Rotation loss loss += torch.exp(-self.sq) * self.beta * self.loss_fn(x[:, 3:], y[:, 3:]) + self.sq return loss If learn_beta param is False it’s a simple weighted sum version of the loss and if learn_beta is True it’s using sx and sq params with enabled gradients that trains together with other network parameter with the same optimizer. PoseNet Training Implementation DetailsNow let’s combine it all to the training loop. I use torch.optim.Adam optimizer with learning rate 1e-5, ResNet34 pretrained on ImageNet as a feature extractor and 2048 features on the last FC layer before pose regressors. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from torchvision import transforms, modelsimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderfrom datasets.apolloscape import Apolloscapefrom utils.common import save_checkpointfrom models.posenet import PoseNet, PoseNetCriterionAPOLLO_PATH = "./data/apolloscape"# ImageNet normalization params because we are using pre-trained# feature extractornormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])# Resize data before usingtransform = transforms.Compose([ transforms.Resize(260), transforms.CenterCrop(250), transforms.ToTensor(), normalize])# Create datasetstrain_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road="zpark-sample", transform=transform, normalize_poses=True, pose_format='quat', train=True, cache_transform=True, stereo=False)val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road="zpark-sample", transform=transform, normalize_poses=True, pose_format='quat', train=False, cache_transform=True, stereo=False)# Dataloaderstrain_dataloader = DataLoader(train_dataset, batch_size=80, shuffle=True)val_dataloader = DataLoader(val_dataset, batch_size=80, shuffle=True)# Select primary deviceif torch.cuda.is_available(): device = torch.device('cuda')else: device = torch.device('cpu')# Create pretrained feature extractorfeature_extractor = models.resnet34(pretrained=True)# Num features for the last layer before pose regressornum_features = 2048# Create modelmodel = PoseNet(feature_extractor, num_features=num_features, pretrained=True)model = model.to(device)# Criterioncriterion = PoseNetCriterion(stereo=False, learn_beta=True)criterion = criterion.to(device)# Add all params for optimizationparam_list = [&#123;'params': model.parameters()&#125;]if criterion.learn_beta: # Add sx and sq from loss function to optimizer params param_list.append(&#123;'params': criterion.parameters()&#125;)# Create optimizeroptimizer = optim.Adam(params=param_list, lr=1e-5, weight_decay=0.0005)# Epochs to trainn_epochs = 2000# Main training loopval_freq = 200for e in range(0, n_epochs): train(train_dataloader, model, criterion, optimizer, e, n_epochs, log_freq=0, poses_mean=train_dataset.poses_mean, poses_std=train_dataset.poses_std, stereo=False) if e % val_freq == 0: validate(val_dataloader, model, criterion, e, log_freq=0, stereo=False)# Save checkpointsave_checkpoint(model, optimizer, criterion, 'zpark_experiment', n_epochs) A little bit simplified train function below with error calculation that is used solely for logging purposes: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768def train(train_loader, model, criterion, optimizer, epoch, max_epoch, log_freq=1, print_sum=True, poses_mean=None, poses_std=None, stereo=True): # switch model to training model.train() losses = AverageMeter() epoch_time = time.time() gt_poses = np.empty((0, 7)) pred_poses = np.empty((0, 7)) end = time.time() for idx, (batch_images, batch_poses) in enumerate(train_loader): data_time = (time.time() - end) batch_images = batch_images.to(device) batch_poses = batch_poses.to(device) out = model(batch_images) loss = criterion(out, batch_poses) # Training step optimizer.zero_grad() loss.backward() optimizer.step() losses.update(loss.data[0], len(batch_images) * batch_images[0].size(0) if stereo else batch_images.size(0)) # move data to cpu &amp; numpy bp = batch_poses.detach().cpu().numpy() outp = out.detach().cpu().numpy() gt_poses = np.vstack((gt_poses, bp)) pred_poses = np.vstack((pred_poses, outp)) # Get final times batch_time = (time.time() - end) end = time.time() if log_freq != 0 and idx % log_freq == 0: print('Epoch: [&#123;&#125;/&#123;&#125;]\tBatch: [&#123;&#125;/&#123;&#125;]\t' 'Time: &#123;batch_time:.3f&#125;\t' 'Data Time: &#123;data_time:.3f&#125;\t' 'Loss: &#123;losses.val:.3f&#125;\t' 'Avg Loss: &#123;losses.avg:.3f&#125;\t'.format( epoch, max_epoch - 1, idx, len(train_loader) - 1, batch_time=batch_time, data_time=data_time, losses=losses)) # un-normalize translation unnorm = (poses_mean is not None) and (poses_std is not None) if unnorm: gt_poses[:, :3] = gt_poses[:, :3] * poses_std + poses_mean pred_poses[:, :3] = pred_poses[:, :3] * poses_std + poses_mean # Translation error t_loss = np.asarray([np.linalg.norm(p - t) for p, t in zip(pred_poses[:, :3], gt_poses[:, :3])]) # Rotation error q_loss = np.asarray([quaternion_angular_error(p, t) for p, t in zip(pred_poses[:, 3:], gt_poses[:, 3:])]) if print_sum: print('Ep: [&#123;&#125;/&#123;&#125;]\tTrain Loss: &#123;:.3f&#125;\tTe: &#123;:.3f&#125;\tRe: &#123;:.3f&#125;\t Et: &#123;:.2f&#125;s\t&#123;criterion_sx:.5f&#125;:&#123;criterion_sq:.5f&#125;'.format( epoch, max_epoch - 1, losses.avg, np.mean(t_loss), np.mean(q_loss), (time.time() - epoch_time), criterion_sx=criterion.sx.data[0], criterion_sq=criterion.sq.data[0])) validate function is similar to train except model.eval()/model.train() modes, logging and error calculations. Please refer to /utils/training.py on GitHub for full-versions of train and validate functions. The training converges after about 1-2k epochs. On my machine, with GTX 1080 Ti it takes about 22 seconds per epoch on ZPark sample train dataset with 2242 images pre-processed and scaled to 250x250 pixels. Total training time – 6-12 hours. PoseNet Results on Apolloscape dataset. ZPark sample road.After 2k epochs of training, the model was managed to get a prediction of pose translation with a mean 40.6 meters and rotation with a mean 1.69 degrees. Further developmentEstablished results are far from one that can be used in autonomous navigation where a system needs to now its location within accuracy of 15cm. Such precision is vital for a car to act safely, correctly predict the behaviors of others and plan actions accordingly. In any case, it’s a good baseline and building blocks of the pipeline to work with Apolloscape dataset that I can develop and improve further. There many things to try next: Use temporal nature of a video. Rely on geometrical features of stereo cameras. Pose graph optimization techniques. Loss based on 3D reprojection errors. Structure from motion methods to build 3D map representation. And what’s more importantly, all above-mentioned methods need no additional information but that we already have in ZPark sample road from Apolloscape dataset. References Kendall, Alex, and Roberto Cipolla. “Geometric loss functions for camera pose regression with deep learning.” (2017). Kendall, Alex, Matthew Grimes, and Roberto Cipolla. “Posenet: A convolutional network for real-time 6-dof camera relocalization.” (2015). Brahmbhatt, Samarth, et al. “Mapnet: Geometry-aware learning of maps for camera localization.” (2017). He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” (2015). Kendall, Alex, Yarin Gal, and Roberto Cipolla. “Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.” (2017). Kendall, Alex, and Yarin Gal. “What uncertainties do we need in bayesian deep learning for computer vision?.” (2017). Clark, Ronald, et al. “VidLoc: A deep spatio-temporal model for 6-dof video-clip relocalization.” (2017). Calafiore, Giuseppe, Luca Carlone, and Frank Dellaert. “Pose graph optimization in the complex domain: Lagrangian duality, conditions for zero duality gap, and optimal solutions.” (2015). Martinec, Daniel, and Tomas Pajdla. “Robust rotation and translation estimation in multiview reconstruction.” (2007). 参考 PoseNet implementation for self-driving car localization using Pytorch on Apolloscape dataset]]></content>
      <categories>
        <category>编程</category>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apolloScape竞赛]]></title>
    <url>%2F2018%2F09%2F09%2FC%2B%2B%E5%9B%9E%E8%B0%83%E6%9C%BA%E5%88%B6%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[引言在 C++ 开发过程中，经常会遇到需要一个类在遇到某种情况下（比如触发了一个行为等等）需要驱使另一类去做某些行为的需求（也就是回调机制）。本文参考如下博文：C++简单实现回调机制进行讲解。 DemoA和B的打招呼程序。 需求 现在有两个人，一位是 A 先生，另一位是 B 先生。现在要求当 A 先生给 B 先生打了招呼之后，B 先生立马回复 A，向 A 问好。 抽象这个过程抽象为三个类进行模拟： 角色 功能 A 向B打招呼 B 当A向自己打招呼时，向A打招呼 main 创造A和B对象和控制A和B的行为 当我们看到上述的分析后，应该想到这么三个内容： A 的作用：A 在这里干了什么呢？他是触发一次回调行为的触发点，也就是因为 A 进行了打招呼的行为，才有了 B 的回复的行为。也就是说：A 是触发方 B 的作用：B 在这里干的就多了。他在得知 A 向自己打招呼了之后，进行了回复。也就是说：B 是驱使方 回复事件：这是我们的回调行为本身，当 A 进行了打招呼的行为，B 进行了回调事件指定的行为 主过程：它掌控着时空，提供上述所有行为的必要条件 那么分析至此，我们已经拥有了写出这个程序的一切条件，接下来让我们一步一步分析一步一步实现吧。 实现：A、B、Event、main首先，我们需要定义一个抽象的事件类，用来定义回复事件： 12345// event classclass CEvent &#123;public: virtual void hiReply() = 0;&#125;; 这个类是这个方法的核心： 这个事件类是一个抽象的类：1.驱使方应该继承这个类并实现其内的具体功能2.触发方应该将此类包含为一个私有变量，当遇到触发事件的时候，通过此指针（父类指针）调用其具体实现的方法（由驱使方实现）3.还需要注意的一点是，为了方便调用，这个类中的方法需要全部声明为公有的，也就是需要在前面加 public 修饰 然后，我们需要实现 A 类，也就是事件触发方： 12345678910111213141516// person A class , call an eventclass A &#123;public: A() : m_pEvent(NULL) &#123;&#125; void sayHi() &#123; cout &lt;&lt; "A: hello B" &lt;&lt; endl; if (nullptr != m_pEvent) &#123; m_pEvent-&gt;hiReply(); &#125; &#125; void setEvent(CEvent *event) &#123; m_pEvent = event; &#125;private: CEvent *m_pEvent;&#125;; 这个类是触发方： 将事件类作为其私有变量：用来接收 B （事件类的子类）的指针，用于实现多态方法调用 实现了 sayHi() 方法：用来触发 B 的回复行为。可以看到在这个函数里，我们使用了事件类对象调用其抽象方法进行了向 B 的事件通知 实现了 setEvent() 方法：用来传递 B 的指针，将其赋值给事件类的指针（父类指针），方便在 sayHi() 方法中多态调用回调方法 接下来，我们需要实现 B 类，也就是驱使方： 12345678910// person B class , implement the event functionclass B : CEvent &#123;public: void sayHi() &#123; cout &lt;&lt; "B: hello A" &lt;&lt; endl; &#125; void hiReply() &#123; cout &lt;&lt; "B: I'm fine, thanks, and you ?" &lt;&lt; endl; &#125;&#125;; B 类中，我们继承了事件类，并且实现了里面的方法。 最后，让我们看看世界的主宰–主过程里干了什么： 123456789int main()&#123; A a; B b; a.setEvent((CEvent*)&amp;b); a.sayHi(); system("pause"); return 0;&#125; 可以看到，主过程里我们干了这几件事： 绑定事件关系：将 b 的指针（事件类子类）传递给了 a 的私有变量 m_pEvent（事件类父类），具有了多态调用的必要条件；这里需要注意的是我们子类向父类的强制转换的写法 (CEvent*)&amp;b，不这么写的话，编译器会报错；想要详细了解 C++ 中父类和子类的转换的可以点击这里 C++中子类和父类之间的相互转化，这篇博客质量还是可以的 触发事件：让 a 向 b 打招呼 让我们运行下这个程序，看看运行结果： 12A: hello BB: I am fine. 总结12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;cstdlib&gt;using std::cout;using std::endl;// event classclass CEvent &#123;public: virtual void hiReply() = 0;&#125;;// person A class , call an eventclass A &#123;public: A() : m_pEvent(NULL) &#123;&#125; void sayHi() &#123; cout &lt;&lt; "A: hello B" &lt;&lt; endl; if (nullptr != m_pEvent) &#123; m_pEvent-&gt;hiReply(); &#125; &#125; void setEvent(CEvent *event) &#123; m_pEvent = event; &#125;private: CEvent *m_pEvent;&#125;;// person B class , implement the event functionclass B : CEvent &#123;public: void hiReply() &#123; cout &lt;&lt; "B: I'm fine, thanks, and you ?" &lt;&lt; endl; &#125;&#125;;int main()&#123; A a; B b; a.setEvent((CEvent*)&amp;b); a.sayHi(); system("pause"); return 0;&#125; 参考 C++简单实现回调机制]]></content>
      <categories>
        <category>编程</category>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xcode配置教程]]></title>
    <url>%2F2018%2F09%2F08%2FXcode%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Xcode配置教程添加删除或剪切整行快捷键]]></content>
      <categories>
        <category>编程</category>
        <category>IOS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>IOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VINS-Mobile版源码阅读]]></title>
    <url>%2F2018%2F09%2F08%2FVIO%2F</url>
    <content type="text"><![CDATA[VIO基础知识IMU是什么IMU中文名叫惯性测量单元，英文名：Inertial measurement unit，简称 IMU。简单理解就是这个东东可以测量两个东西，加速度 a 是沿三个轴\(a_{x}\),\(a_{y}\),\(a_{z}\), 方向的线加速度，而角速度 w 就是这三个方向的角速度 \(w_{x}\),\(w_{y}\),\(w_{z}\), 还有就是IMU的频率比较高一般都在100HZ以上。在IMU内部，除了通常的白噪声，还有个特别的量零偏bias，在这是传感器内部机械、温度等各种物理因素产生的传感器内部误差的综合参数。IMU的加速度计和陀螺仪的每个轴都用彼此相互独立的参数建模，一个角速度测量值和真值之间的连续域上的关系可以写作： $$\tilde{w}=w+b_{w}+n_{w}.$$ 在纯视觉的slam或者vo中，由于图像的运动模糊、遮挡、快速运动、纯旋转、尺度不确定性的一系列问题，导致仅靠一个摄像头很难完成我们实际场景的应用需求，而IMU直接可以得到运动主体自身的角速度、加速度的测量数据，从而对运动有一个约束，或者说与视觉形成互补，可实现快速运动的定位和主体纯旋转的处理，从而进一步提高slam/vio的可靠性。 VIO中IMU与图像之间的时间序列关系如下图所示： 图1 IMU与视觉在时间轴上的信息 如图1所示。在时间轴上，IMU通常以较快的速率采集角速度和加速度的信息，而视觉则是以较慢的频率采集图像。VIO的器件同步，保证了每个时刻采集的数据都是同步的。原理上，我们可以给出每个时刻的位姿估计，然而，现有视觉SLAM，多数是基于关键帧+BA的处理形式。于是，一个重要的问题是，能否将两个视觉关键帧当中的IMU数据，整合在一起，约束它们之间的运动？如果可以的话，又如何来约束？预积分的目的，就在于处理这里的运动关系。为了说清楚这件事，需要介绍一些背景知识。 (1) 坐标系和动力学 图2 VIO器件的坐标系 在图2中，当IMU在世界中运动时，我们有世界坐标系（W），IMU坐标系（B）和像极坐标系（C）。对于IMU，通常估计世界到IMU坐标系的位姿为\(R_{WB}\)和\(P_{WB}\)，其中为了与时间进行区分，这里的平移采用P来表示。图2右侧给出了动力学方程，即位移的微分为速度、速度的微分为加速度、旋转的微分为角速度。 （2）IMU测量数据表示 IMU可测量IMU系（B）的角速度和加速度，测量信号受噪声和零偏的影响，我们得到： B系下的角速度： $$_B\tilde{w}_{WB}(t)=_Bw_{WB}(t)+b^{g}(t)+\eta^{g}(t),$$ 加速度，考虑世界坐标系下的重力： $$_B\tilde{a}(t)=R_{WB}^{T}(t)(_Wa(t)-_Wg)+b^{a}(t)+\eta^{a}(t),$$ IMU的Bias方程： $$\dot{b}^{g}(t)=\eta^{bg},$$ $$\dot{b}^{a}(t)=\eta^{ba},$$ 其中，\(\eta^{g}, \eta^{a}, \eta^{bg}, \eta^{ba}\sim N\)，即服从高斯分布。 为了估计位姿，首先要选择状态变量。在紧耦合（Tightly Coupled）方案中，通常选择位姿、速度、零偏这几个量，作为待估计的状态量，共15维： 于是，可以想见，在带IMU的bundle adjustment中，我们每一个Pose都是这样一个15维的变量。那么，如何通过IMU数据，定义两个状态量之间的运动约束呢？还是回到动力学。 前面给出了微分形式的动力学，当然我们可以把它写成积分形式的。然后，由于是在离散时刻进行采样，所以得到的是离散时刻的动力学方程。再代入IMU的测量，有： 个人整理： 于是，差分方程给出了两个连续的视觉帧之间的IMU约束。进一步，如果把两个关键帧之间的多个视觉帧积分起来，就形成了预积分： 个人整理： IMU 预积分IMU的数据频率一般远高于视觉，在视觉两帧k，k+1之间通常会有&gt;10组IMU数据。IMU的数据通过积分，可以获取当前位姿（p位置，q四元数表达的姿态）、瞬时速度等参数。 在VIO中，如果参考世界坐标系对IMU进行积分，积分项中包含相对于世界坐标系的瞬时旋转矩阵，这样有几个问题： 相对世界坐标系的旋转矩阵有drift，如果一直以其为基准进行积分，必然造成积分误差累积； 在进行优化位姿调整时（通常是调整视觉KeyFrame的pose），相对于世界坐标系的pose会变化，因而优化后的瞬时旋转矩阵和积分时不同，那么积分自然也就存在问题； 一般这个旋转矩阵不知道。。。 因此，一般的预积分的参考坐标系为k帧的IMU参考系，这样可以解决以上问题： 相对k帧的IMU进行积分，不会有累积误差； 即使后面调整了位姿，相对位置不变，因此预积分不存在问题； 这个旋转矩阵为单位矩阵E，后面每出现一个IMU数据，都可以用任何一种数值积分的方法计算；同时可以将重力加速度提取到积分号外面不参加积分，相当于在重力参考系中积分，计算量也会减少。 参考 https://www.cnblogs.com/shang-slam/p/7080113.html https://zhuanlan.zhihu.com/p/38009126 预积分 | Momenta Paper Reading 七期回顾 如何理解IMU以及其预积分 论文一：Visual-Inertial-Aided Navigation for High-Dynamic Motion in Built Environments Without Initial ConditionsTodd Lupton and Salah Sukkarieh The University of Sydney Motivation 位置追踪具有广泛的应用，诸如安全救援、自动驾驶等。 当前VO或SLAM方法均存在一些局限性，如VO没有尺度信息。此外，IMU能够提供尺度信息，但难点在于初始化过程。 本文主要想法及贡献点 This core idea is expanded in this paper with integration into a graphical filter that allows automatic inertial initialization and map management to produce a robust visual-inertial navigation system 本文的核心思想是扩展到一个图形滤波器中，它允许自动惯性初始化和地图管理来产生一个鲁棒的视觉惯性导航系统 The main contribution of this paper is that no explicit initialization stage or large uncertainty priors are required, and the initial conditions are automatically recovered in a linear manner. 贡献点是：无需初始化阶段或大的不确定性先验，并且初始条件自动地以线性方式恢复。]]></content>
      <categories>
        <category>编程</category>
        <category>IOS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>IOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VINS-Mobile版源码阅读]]></title>
    <url>%2F2018%2F09%2F08%2FVINS-Mobile%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[VINS-Mobile版源码阅读入口ViewController.mm参考 VINS（四）estimator_node 数据对齐 imu预积分 vision]]></content>
      <categories>
        <category>编程</category>
        <category>IOS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>IOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CocoaPods简介]]></title>
    <url>%2F2018%2F09%2F08%2FCocoaPods%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[CocoaPods简介CocoaPods是iOS的包管理工具。在开发iOS项目时，经常会使用第三方开源库，手动引入流程复杂，并且库之间还存在依赖关系，更增加了手动管理的难度。开源库如果升级了，你也想用最新版本，还需要重新手动导入，这大大增加了工作量。但用了CocoaPods后，安装和升级都只是一句命令的事情，让你可以专于业务本身。 CocoaPods安装安装RubyCocoaPods基于Ruby语言开发而成，因此安装CocoaPods前需要安装Ruby环境。幸运的是Mac系统默认自带Ruby环境，如果没有请自行查找安装。检测是否安装Ruby： 12$ gem -v2.0.14 安装则会提示当前Ruby版本。gem介绍：gem是一个管理Ruby库和程序的标准包，它通过Ruby Gem（如 http://rubygems.org/ ）源来查找、安装、升级和卸载软件包，非常的便捷。 更换gem源因为国内网络的问题导致gem源间歇性中断，原因你懂的。因此我们需要更换gem源，使用淘宝的gem源https://ruby.taobao.org/。 第一步：移动默认的源: 1gem sources --remove https://rubygems.org/ 第二步：指定淘宝的源: 1gem sources -a https://ruby.taobao.org/ 第三步：查看指定的源是不是淘宝源: 1234$ gem sources -l*** CURRENT SOURCES ***https://ruby.taobao.org/ 如果是https://ruby.taobao.org/，则更换成功。 安装CocoaPods确认改成淘宝源后执行以下命令进行安装： 1sudo gem install cocoapods 稍等片刻即可安装完成，输入以下命令检测是否安装成功： 12$ pod --version0.39.0 成功则会提示CocoaPods版本。 CocoaPods使用实例首先新建一个iOS工程MyDemo，在该工程中演示CocoaPods的使用。 进入工程的根目录，创建Podfile文件. 根据需要，我们可以在Podfile文件中写入我们需要的第三方库，这里以AFNetworking和MJRefresh为例，Podfile内容如下： 1234platform :ios, &apos;7.0&apos;pod &apos;AFNetworking&apos;, &apos;~&gt; 3.0&apos;pod &apos;MJRefresh&apos;,&apos;~&gt; 3.1&apos;# 这段代码的意思是，当前类库支持的iOS最低版本是iOS 7.0, 要下载的两个类库的版本分别为 3.0、3.1。 这时候，你就可以利用CocoPods下载AFNetworking和MJRefresh类库了。在终端中进入工程根目录，运行以下命令： 1234567891011$ cd /Users/myl/Desktop/IOS/MyDemo$ pod installAnalyzing dependenciesDownloading dependenciesInstalling AFNetworking (3.0.4)Installing MJRefresh (3.1.0)Generating Pods projectIntegrating client project[!] Please close any current Xcode sessions and use `MyDemo.xcworkspace` for this project from now on.Sending stats]]></content>
      <categories>
        <category>编程</category>
        <category>IOS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>IOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ARKit教程]]></title>
    <url>%2F2018%2F09%2F08%2FARKit%2F</url>
    <content type="text"><![CDATA[ARKit教程背景介绍苹果公司在WWDC2017 上发布了ARKit,开发者可以使用这套框架在iPhone和iPad上创建属于自己的AR体验。 增强现实技术（Augmented Reality，简称 AR），是一种实时地计算摄影机影像的位置及角度并加上相应图像、视频、3D模型的技术，这种技术的目标是在屏幕上把虚拟世界套在现实世界并进行互动。 ARKit框架提供了两种AR技术，一种是基于3D场景(SceneKit)实现的增强现实，一种是基于2D场景(SpriktKit)实现的增强现实。 需要注意的是，ARKit虽说是iOS11提供的框架，但是并非升级到iOS11都能使用。必须要A9以及以上的处理器才能使用。以下是ARKit 开发环境： 1.Xcode版本：Xcode9及以上 2.iOS系统:iOS11及以上 3.iOS设备：处理器A9及以上（6S机型及以上） 4.MacOS系统：10.12.4及以上（安装Xcode9对Mac系统版本有要求） 根据ARKit的描述，兼容苹果AR功能的iPhone必须搭载iOS 11，同时对手机、平板的整体性能也有更高要求。ARKit支持所有安装了ios 11的iphone与ipad。 判断iPhone 硬件设备型号: iPhone8,1 iPhone6S iPhone8,2 iPhone6S Plus iPhone8,4 iPhoneSE iPhone9,1 /iPhone9,3 iPhone 7 iPhone9,2/iPhone9,4 iPhone 7 Plus iPhone10,1/iPhone10,4 iPhone 8 iPhone10,2/iPhone10,5 iPhone 8 Plus iPhone10,3/iPhone10,6 iPhone X 核心功能（1）追踪 追踪是 ARKit 的核心功能，可以获得设备在物理世界中的方向和位置，此外还可以追踪物体（例如人脸）。 （2）场景理解 场景理解通过学习环境的一些属性来优化追踪，检测水平面（例如地面和桌面）和垂直面以便在场景中放置物体。场景理解还会学习环境中的光照情况，以便在虚拟场景中精确模拟真实的光照，以防物体看起来过亮或过暗。 （3）渲染 渲染就是指用户在设备上实际看到、并且在 AR 场景中交互的内容。ARKit 可以很方便的与你喜欢的渲染引擎进行集成，还为 SceneKit 和 SpriteKit 提供了内置视图。此外 Xcode 还提供了可以快速上手的 Metal 模板。 ARKit工作原理和流程首先需要说明的是，ARKit并不是一个能够独立运行的框架，其依赖SceneKit框架。简单点说，ARKit框架主要提供相机相关的工作，主要负责捕捉和分析现实世界。而展示虚拟物体部分，则是依赖SceneKit提供的能力。如果没有SceneKit，那ARKit只是一个相机而已。（SceneKit是苹果在iOS8中集成的一套3D引擎框架） 下图是ARKit中主要类的关系图： 上图中，ARSCNView可以认为是一个容器，代表的就是看到的现实世界。其作用有两个： 1、如上图右边部分：管理ARSession，在这里简单认为ARSession是管理ARKit世界的上下文，其管理者相机的属性设置，也负责向ARSCNView输出捕捉到的显示世界。 2、如上图左边部分：显示我们添加进去的AR物体，这里可以是一个3D物体，也可以是一个2D物体。这里从继承关系来看，ARSCNView、SCNScene、SCNNode等类是从SceneKit中继承过来的。 从上图可以简单的看出ARKit的工作原理，ARKit框架提供相机能力，在手机上捕捉并构建现实世界。SceneKit提供模型能力，在手机的“现实世界”中添加虚拟物体。 SceneKit 简介坐标系UIKit 使用一个包含有 x 和 y 信息的 CGPoint 来表示一个点的位置，但是在 3D 系统中，需要一个 z 参数来描述物体在空间中的深度，SceneKit 的坐标系可以参考下图： 这个三维坐标系中，表示一个点的位置需要使用(x,y,z)坐标表示。红色方块位于 x 轴，绿色方块位于 y 轴，蓝色方块位于 z 轴，灰色方块位于原点。在 SceneKit 中我们可以这样创建一个三维坐标： 1let position = SCNVector3(x: 0, y: 5, z: 10) SceneKit 中的场景和节点SceneKit 中的场景(SCNScene)想象为一个虚拟的 3D 空间，然后可以将一个个的节点(SCNNode)添加到场景中。SCNScene 中有唯一一个根节点(坐标是(x:0, y:0, z:0))，除了根节点外，所有添加到 SCNScene 中的节点都需要一个父节点。 SCNScene 中的节点加入时可以指定一个三维坐标(默认为(x:0, y:0, z:0))，这个坐标是相对于其父节点的位置。这里说明两个概念： 本地坐标系：以场景中的某节点(非根节点)为原点建立的三维坐标系 世界坐标系：以根节点为原点创建的三维坐标系称为世界坐标系。 SceneKit 中的摄像机有了 SCNScene 和 SCNNode 后，我们还需要一个摄像机(SCNCamera)来决定我们可以看到场景中的哪一块区域(就好比现实世界中有了各种物体，但还需要人的眼睛才能看到物体)。摄像机在 SCNScene 的工作模式如下图： 上图中包含以下几点信息： SceneKit 中 SCNCamera 拍摄的方向始终为 z 轴负方向。 视野(Field of View)是摄像机的可视区域的极限角度。角度越小，视野越窄，反之，角度越大，视野越宽。 视锥体(Viewing Frustum)决定着摄像头可视区域的深度(z 轴表示深度)。任何不在这个区域内的物体将被剪裁掉(离摄像头太近或者太远)，不会显示在最终的画面中。 在 SceneKit 中我们可以使用如下方式创建一个摄像机： 123456let scene = SCNScene()let cameraNode = SCNNode()let camera = SCNCamera()cameraNode.camera = cameracameraNode.position = SCNVector3(x: 0, y: 0, z: 0)scene.rootNode.addChildNode(cameraNode) SCNView最后，我们需要一个 View 来将 SCNScene 中的内容渲染到显示屏幕上，这个工作由 SCNView 完成。这一步其实很简单，只需要创建一个 SCNView 实例，然后将 SCNView 的 scene 属性设置为刚刚创建的 SCNScene，然后将 SCNView 添加到 UIKit 的 view 或 window 上即可。示例代码如下： 1234let scnView = SCNView()scnView.scene = scenevc.view.addSubview(scnView)scnView.frame = vc.view.bounds ARKit API介绍ARKit框架的API其实并不多。下图就是整个ARKit框架提供的类。下面就这几个主要的类（介绍主要的属性）做一个简单的说明。 （1）ARAnchorARAnchor表示一个物体在3D空间的位置和方向（ARAnchor通常称为物体的3D锚点，有点像UIKit框架中CALayer的Anchor）. 注：ARFrame表示的也是物体的位置和方向，但是ARFrame通常表示的是AR相机的位置和方向以及追踪相机的时间，还可以捕捉相机的帧图片,也就是说ARFrame用于捕捉相机的移动，其他虚拟物体用ARAnchor。 （2）ARCameraARCamera是一个相机，它是连接虚拟场景与现实场景之间的枢纽。在ARKit中，它是捕捉现实图像的相机，在SceneKit中它又是3D虚拟世界中的相机。ARCamera 是 ARFrame 中的一个属性，之因为单独拿出来说，是因为这里有必要介绍下相机的一些特性。ARCamera 中与现实世界场景有关的信息有两个： var imageResolution: CGSize该属性表示了相机捕捉到的图像的长度和宽度(以像素为单位)，可以理解成捕捉到的图像的分辨率。 var intrinsics: matrix_float3x3intrinsics 是一个 3x3 矩阵，这个矩阵将我们现实世界中三维坐标系的点映射到相机捕捉的图像中。有兴趣可看下面的详述。 Intrinsic Matrix： Intrinsic Matrix 是相机的一个固有属性，也就是说每个相机都会有 Intrinsic Matrix，因为所有的相机都需要将现实世界中三维空间的点映射到捕捉的图像中二维空间的点。那么这个矩阵是如何工作的呢？我们先来看一个图片： 上图包含如下基本信息： 一个三维坐标系(红色 x 轴，绿色 y 轴，蓝色 z 轴)。 空间中的一个点(蓝色的点 N，坐标为(x’, y’, z’))。 相机的成像平面(紫色的平行四边形) 成像平面与 z 轴的交点(点 M) 成像平面的原点(黄色的点 O)，也就是捕捉的二维图像的二维坐标系的原点。 现在我们需要将三维空间的点(x’, y’, z’)映射到成像平面中的一个点(N’)。 一般我们无需去创建一个相机，因为当我们初始化一个AR视图时，会为我们默认创建一个相机，而且这个相机就是摄像头的位置，同时也是3D世界中的原点所在（x=0,y=0,z=0）。 ARCamera的API一般我们无需关心，因为ARKit会默认帮助我们配置好. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586@interface ARCamera : NSObject &lt;NSCopying&gt;/** 4x4矩阵表示相机位置，同ARAnchor */@property (nonatomic, readonly) matrix_float4x4 transform;/**相机方向（旋转）的矢量欧拉角分别是x/y/z */@property (nonatomic, readonly) vector_float3 eulerAngles;/** 相机追踪状态（在下方会有枚举值介绍） */@property (nonatomic, readonly) ARTrackingState trackingState NS_REFINED_FOR_SWIFT;/**追踪运动类型 */@property (nonatomic, readonly) ARTrackingStateReason trackingStateReason NS_REFINED_FOR_SWIFT;/**相机内置函数3x3矩阵 fx 0 px 0 fy py 0 0 1 fx和fy是像素焦距 px和py是主像素点坐标 原点位于左上像素的中心 */@property (nonatomic, readonly) matrix_float3x3 intrinsics;/**摄像头分辨率 */@property (nonatomic, readonly) CGSize imageResolution;/**投影矩阵*/@property (nonatomic, readonly) matrix_float4x4 projectionMatrix;/**返回一个二维坐标点*/- (CGPoint)projectPoint:(vector_float3)point orientation:(UIInterfaceOrientation)orientation viewportSize:(CGSize)viewportSize;/**创建相机投影矩阵 */- (matrix_float4x4)projectionMatrixWithViewportSize:(CGSize)viewportSize orientation:(UIInterfaceOrientation)orientation zNear:(CGFloat)zNear zFar:(CGFloat)zFar;@end//相机追踪状态枚举typedef NS_ENUM(NSInteger, ARTrackingState) &#123; /** 不被允许 */ ARTrackingStateNotAvailable, /** 受限状态 */ ARTrackingStateLimited, /** 正常. */ ARTrackingStateNormal,&#125; NS_REFINED_FOR_SWIFT;/** 描述相机追踪状态有限的原因 */typedef NS_ENUM(NSInteger, ARTrackingStateReason) &#123; /** 无. */ ARTrackingStateReasonNone, /** 正在初始化，导致跟踪受限 */ ARTrackingStateReasonInitializing, /** 运动过度. */ ARTrackingStateReasonExcessiveMotion, /** 缺少可见的特征. */ ARTrackingStateReasonInsufficientFeatures,&#125; NS_REFINED_FOR_SWIFT; (3)ARErrorARError是一个描述ARKit错误的类，这个错误来源于几个方面，例如设备不支持，或者当相机常驻后台时ARSession会断开等问题. 1234567891011121314151617//作用域，一般会表示是哪一个类出现问题NSString *const ARErrorDomain;//错误码描述 100：不支持会话追踪配置，主线由于A9芯片以下的机型会报错 101：失活状态 102：传感器故障 200：追踪失败typedef NS_ERROR_ENUM(ARErrorDomain, ARErrorCode) &#123; /** Unsupported session configuration. */ ARErrorCodeUnsupportedConfiguration = 100, /** A sensor required to run the session is not available. */ ARErrorCodeSensorUnavailable = 101, /** A sensor failed to provide the required input. */ ARErrorCodeSensorFailed = 102, /** World tracking has encountered a fatal error. */ ARErrorCodeWorldTrackingFailed = 200,&#125;; (4) ARHitTestResultARHitTestResult：点击回调结果，这个类主要用于虚拟增强现实技术（AR技术）中现实世界与3D场景中虚拟物体的交互。 比如我们在相机中移动。拖拽3D虚拟物体，都可以通过这个类来获取ARKit所捕捉的结果. 123456789101112131415161718192021222324252627282930313233343536373839//捕捉类型枚举typedef NS_OPTIONS(NSUInteger, ARHitTestResultType) &#123; /** 点. */ ARHitTestResultTypeFeaturePoint = (1 &lt;&lt; 0), /** 水平面 y为0. */ ARHitTestResultTypeEstimatedHorizontalPlane = (1 &lt;&lt; 1), /** 已结存在的平面. */ ARHitTestResultTypeExistingPlane = (1 &lt;&lt; 3), /** 已结存在的锚点和平面. */ ARHitTestResultTypeExistingPlaneUsingExtent = (1 &lt;&lt; 4),&#125; NS_SWIFT_NAME(ARHitTestResult.ResultType);/**捕捉类型 */@property (nonatomic, readonly) ARHitTestResultType type;/** 3D虚拟物体与相机的距离（单位：米） */@property (nonatomic, readonly) CGFloat distance;/**本地坐标矩阵（世界坐标指的是相机为场景原点的坐标，而每一个3D物体自身有一个场景，本地坐标就是相对于这个场景的坐标）类似于frame和bounds的区别 */@property (nonatomic, readonly) matrix_float4x4 localTransform;/**世界坐标矩阵 */@property (nonatomic, readonly) matrix_float4x4 worldTransform;/** 锚点（3D虚拟物体，在虚拟世界有一个位置，这个位置参数是SceneKit中的SCNVector3：三维矢量），而锚点anchor是这个物体在AR现实场景中的位置，是一个4x4的矩阵 */@property (nonatomic, strong, nullable, readonly) ARAnchor *anchor; (5) ARLightEstimateARLightEstimate是一个灯光效果，它可以让你的AR场景看起来更加的好. 12345678@interface ARLightEstimate : NSObject &lt;NSCopying&gt;/**灯光强度 范围0-2000 默认1000 */@property (nonatomic, readonly) CGFloat ambientIntensity;@end (6) ARFrameARFrame主要是追踪相机当前的状态，这个状态不仅仅只是位置，还有图像帧及时间等参数: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@interface ARFrame : NSObject &lt;NSCopying&gt;/**时间戳. */@property (nonatomic, readonly) NSTimeInterval timestamp;/** 缓冲区图像帧 */@property (nonatomic, readonly) CVPixelBufferRef capturedImage;/**相机（表示这个ARFrame是哪一个相机的，iPhone7plus有两个摄像机） */@property (nonatomic, copy, readonly) ARCamera *camera;/** 返回当前相机捕捉到的锚点数据（当一个3D虚拟模型加入到ARKit中时，锚点值得就是这个模型在AR中的位置） */@property (nonatomic, copy, readonly) NSArray&lt;ARAnchor *&gt; *anchors;/**灯光，详情可见本章节ARLightEstimate类介绍（指的是灯光强度 一般是0-2000，系统默认1000） */@property (nonatomic, copy, nullable, readonly) ARLightEstimate *lightEstimate;/**特征点（应该是捕捉平地或者人脸的，比较苹果有自带的人脸识别功能） */@property (nonatomic, nullable, readonly) ARPointCloud *rawFeaturePoints;/**根据2D坐标点搜索3D模型，这个方法通常用于，当我们在手机屏幕点击某一个点的时候，可以捕捉到这一个点所在的3D模型的位置，至于为什么是一个数组非常好理解。手机屏幕一个是长方形，这是一个二维空间。而相机捕捉到的是一个由这个二维空间射出去的长方体，我们点击屏幕一个点可以理解为在这个长方体的边缘射出一条线，这一条线上可能会有多个3D物体模型point：2D坐标点（手机屏幕某一点）ARHitTestResultType：捕捉类型 点还是面(NSArray&lt;ARHitTestResult *&gt; *)：追踪结果数组 详情见本章节ARHitTestResult类介绍 */- (NSArray&lt;ARHitTestResult *&gt; *)hitTest:(CGPoint)point types:(ARHitTestResultType)types;/**相机窗口的的坐标变换（可用于相机横竖屏的旋转适配） */- (CGAffineTransform)displayTransformWithViewportSize:(CGSize)viewportSize orientation:(UIInterfaceOrientation)orientation;@end (7) ARPlaneAnchorARPlaneAnchor是ARAnchor的子类，笔者称之为平地锚点。ARKit能够自动识别平地，并且会默认添加一个锚点到场景中，当然要想看到真实世界中的平地效果，需要我们自己使用SCNNode来渲染这个锚点,锚点只是一个位置. 12345678910111213141516/**平地类型，目前只有一个，就是水平面 */@property (nonatomic, readonly) ARPlaneAnchorAlignment alignment;/**3轴矢量结构体，表示平地的中心点 x/y/z */@property (nonatomic, readonly) vector_float3 center;/**3轴矢量结构体，表示平地的大小（宽度和高度） x/y/z */@property (nonatomic, readonly) vector_float3 extent;@end (8) ARPointCloudARPointCloud：点状渲染云，主要用于渲染场景. 12345678910111213@interface ARPointCloud : NSObject &lt;NSCopying&gt;/** 点的数量 */@property (nonatomic, readonly) NSUInteger count;/**每一个点的位置的集合（结构体带*表示的是结构体数组） */@property (nonatomic, readonly) const vector_float3 *points;@end (9) ARSCNViewARKit支持3D的AR场景和2D的AR场景，ARSCNView是3D的AR场景视图: 该类非常重要，且API较多 该类是整个ARKit框架中唯一两个有代理的类其中之一 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@interface ARSCNView : SCNView/**代理 */@property (nonatomic, weak, nullable) id&lt;ARSCNViewDelegate&gt; delegate;/**AR会话 */@property (nonatomic, strong) ARSession *session;/**场景 */@property(nonatomic, strong) SCNScene *scene;/**是否自动适应灯光 */@property(nonatomic) BOOL automaticallyUpdatesLighting;/**返回对应节点的锚点，节点是一个3D虚拟物体，它的坐标是虚拟场景中的坐标，而锚点ARAnchor是ARKit中现实世界的坐标。 */- (nullable ARAnchor *)anchorForNode:(SCNNode *)node;/**返回对应锚点的物体 */- (nullable SCNNode *)nodeForAnchor:(ARAnchor *)anchor;/**根据2D坐标点搜索3D模型，这个方法通常用于，当我们在手机屏幕点击某一个点的时候，可以捕捉到这一个点所在的3D模型的位置，至于为什么是一个数组非常好理解。手机屏幕一个是长方形，这是一个二维空间。而相机捕捉到的是一个由这个二维空间射出去的长方体，我们点击屏幕一个点可以理解为在这个长方体的边缘射出一条线，这一条线上可能会有多个3D物体模型point：2D坐标点（手机屏幕某一点）ARHitTestResultType：捕捉类型 点还是面(NSArray&lt;ARHitTestResult *&gt; *)：追踪结果数组 详情见本章节ARHitTestResult类介绍数组的结果排序是由近到远 */- (NSArray&lt;ARHitTestResult *&gt; *)hitTest:(CGPoint)point types:(ARHitTestResultType)types;@end//代理#pragma mark - ARSCNViewDelegate//代理的内部实现了SCNSceneRendererDelegate：scenekit代理 和ARSessionObserver：ARSession监听（KVO机制）@protocol ARSCNViewDelegate &lt;SCNSceneRendererDelegate, ARSessionObserver&gt;@optional/**自定义节点的锚点 */- (nullable SCNNode *)renderer:(id &lt;SCNSceneRenderer&gt;)renderer nodeForAnchor:(ARAnchor *)anchor;/**当添加节点是会调用，我们可以通过这个代理方法得知我们添加一个虚拟物体到AR场景下的锚点（AR现实世界中的坐标） */- (void)renderer:(id &lt;SCNSceneRenderer&gt;)renderer didAddNode:(SCNNode *)node forAnchor:(ARAnchor *)anchor;/**将要刷新节点 */- (void)renderer:(id &lt;SCNSceneRenderer&gt;)renderer willUpdateNode:(SCNNode *)node forAnchor:(ARAnchor *)anchor;/** 已经刷新节点 */- (void)renderer:(id &lt;SCNSceneRenderer&gt;)renderer didUpdateNode:(SCNNode *)node forAnchor:(ARAnchor *)anchor;/** 移除节点 */- (void)renderer:(id &lt;SCNSceneRenderer&gt;)renderer didRemoveNode:(SCNNode *)node forAnchor:(ARAnchor *)anchor;@end (10) ARSessionARSession是一个连接底层与AR视图之间的桥梁，其实ARSCNView内部所有的代理方法都是由ARSession来提供的. ARSession与ARScnView之间的关系看起来是这样的: ARSession获取相机位置数据主要有两种方式: 第一种：push。 实时不断的获取相机位置，由ARSession主动告知用户。通过实现ARSession的代理-(void)session:(ARSession )session didUpdateFrame:(ARFrame )frame来获取. 第二种：pull。 用户想要时，主动去获取。ARSession的属性currentFrame来获取. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111@interface ARSession : NSObject/** 代理 */@property (nonatomic, weak) id &lt;ARSessionDelegate&gt; delegate;/**指定代理执行的线程（主线程不会有延迟，子线程会有延迟），不指定的话默认主线程 */@property (nonatomic, strong, nullable) dispatch_queue_t delegateQueue;/**相机当前的位置（是由会话追踪配置计算出来的） */@property (nonatomic, copy, nullable, readonly) ARFrame *currentFrame;/** 会话追踪配置 */@property (nonatomic, copy, nullable, readonly) ARConfiguration *configuration;/**运行会话（这行代码就是开启AR的关键所在） */- (void)runWithConfiguration:(ARConfiguration *)configuration NS_SWIFT_UNAVAILABLE(&quot;Use run(_:options:)&quot;);/**运行会话，只是多了一个参数ARSessionRunOptions：作用就是会话断开重连时的行为。是一个枚举值如下： ARSessionRunOptionResetTracking：表示重置追踪ARSessionRunOptionRemoveExistingAnchors：移除现有锚点 */- (void)runWithConfiguration:(ARSessionConfiguration *)configuration options:(ARSessionRunOptions)options NS_SWIFT_NAME(run(_:options:));/**暂停会话 */- (void)pause;/**添加锚点 */- (void)addAnchor:(ARAnchor *)anchor NS_SWIFT_NAME(add(anchor:));/**移除锚点 */- (void)removeAnchor:(ARAnchor *)anchor NS_SWIFT_NAME(remove(anchor:));@end//session代理分类两部分，一个是观察者（KVO） 一个是委托者（代理）#pragma mark - ARSessionObserver//session KVO观察者@protocol ARSessionObserver &lt;NSObject&gt;@optional/** session失败 */- (void)session:(ARSession *)session didFailWithError:(NSError *)error;/**相机改变追踪状态 */- (void)session:(ARSession *)session cameraDidChangeTrackingState:(ARCamera *)camera;/** session意外断开（如果开启ARSession之后，APP退到后台就有可能导致会话断开） */- (void)sessionWasInterrupted:(ARSession *)session;/**session会话断开恢复（短时间退到后台再进入APP会自动恢复） */- (void)sessionInterruptionEnded:(ARSession *)session;@end#pragma mark - ARSessionDelegate@protocol ARSessionDelegate &lt;ARSessionObserver&gt;@optional/** 相机当前状态（ARFrame：空间位置，图像帧等）更新 */- (void)session:(ARSession *)session didUpdateFrame:(ARFrame *)frame;/**添加锚点 */- (void)session:(ARSession *)session didAddAnchors:(NSArray&lt;ARAnchor*&gt;*)anchors;/**刷新锚点 */- (void)session:(ARSession *)session didUpdateAnchors:(NSArray&lt;ARAnchor*&gt;*)anchors;/**移除锚点 */- (void)session:(ARSession *)session didRemoveAnchors:(NSArray&lt;ARAnchor*&gt;*)anchors;@end (11) ARConfiguration注意：该类还有一个子类：ARWorldTrackingConfiguration，它们在同一个API文件中. 123456789101112131415161718192021222324252627282930313233343536//会话追踪配置类@interface ARConfiguration : NSObject &lt;NSCopying&gt;/**当前设备是否支持，一般A9芯片以下设备不支持 */@property(class, nonatomic, readonly) BOOL isSupported;/**会话的对齐方式，这里的对齐指的是3D世界的坐标。枚举值见下方默认值ARWorldAlignmentGravity */@property (nonatomic, readwrite) ARWorldAlignment worldAlignment;/**是否需要自适应灯光效果，默认是YES */@property (nonatomic, readwrite, getter=isLightEstimationEnabled) BOOL lightEstimationEnabled;/**是否需要提供音频，默认是NO */@property (nonatomic, readwrite) BOOL providesAudioData;@end//世界会话追踪配置，苹果建议我们使用这个类，这个子类只有一个属性，也就是可以帮助我们追踪相机捕捉到的平地@interface ARWorldTrackingSessionConfiguration : ARSessionConfiguration/**侦查类型。枚举值见下方（默认侦查平地） */@property (nonatomic, readwrite) ARPlaneDetection planeDetection;@end //追踪对其方式，这个决定了会话的参考坐标系（参照物）. 12345678910111213141516171819typedef NS_ENUM(NSInteger, ARWorldAlignment) &#123; /** Aligns the world with gravity that is defined by vector (0, -1, 0). */ ARWorldAlignmentGravity, /** Aligns the world with gravity that is defined by the vector (0, -1, 0) and heading (w.r.t. True North) that is given by the vector (0, 0, -1). */ ARWorldAlignmentGravityAndHeading, /** Aligns the world with the camera’s orientation. */ ARWorldAlignmentCamera&#125;;typedef NS_OPTIONS(NSUInteger, ARPlaneDetection) &#123; /** No plane detection is run. */ ARPlaneDetectionNone = 0, /** Plane detection determines horizontal planes in the scene. */ ARPlaneDetectionHorizontal = (1 &lt;&lt; 0),&#125;; (12) ARSKViewARSKView也是AR视图，只不过他是2D的，由于2D比3D简单很多，并且ARSKView基本与ARSCNView类似，所以这里不做重复介绍。详细内容可参考ARSCNView. 参考 ARKit API介绍 直击苹果 ARKit 技术]]></content>
      <categories>
        <category>编程</category>
        <category>IOS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>IOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读-PDR]]></title>
    <url>%2F2018%2F07%2F13%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PDR%2F</url>
    <content type="text"><![CDATA[论文一、IONet: Learning to Cure the Curse of Drift in Inertial OdometryMotivation现有的基于惯性传感器的位置估计方法 不足： 累计误差无界； SINS的累计误差为指数级增加，PDR的累计误差为线性增加； PDR对于无法检测到步子的场景失效。 现有工作包括： （1） 接连惯性导航方法（Strapdown Inertial Navigation System，SINS） 方法主要思想：双重积分来估计位置和速度。 不足：由于MEMS传感器的成本有限，其测量精度有限，导致位置和速度估计误差呈指数级增长。 （2）PDR （Pedestrian Dead Reckoning） 方法主要思想：计步、步长估计、方向估计 不足：累计误差呈线性增长，对于无法检测步子的场景失效。 本文贡献： （1）建模惯性追踪问题为序列学习问题； （2）本文提出了基于DNN的学习框架。 背景知识(1) 方位姿态更新 公式（1）：$$C_{b}^{n}(t)=C_{b}^{n}(t-1)*\Omega(t),$$ 公式（2）：$$\sigma=w(t)dt,$$ 公式（3）：$$\Omega(t)=C_{b_{t}}^{b_{t-1}}=I+\frac{sin(\sigma)}{\sigma}[\sigma X]+\frac{1-cos(\sigma)}{\sigma^{2}}[\sigma X]^2,$$ (2) 速度更新 $$v(t)=v(t-1)+((C_{b}^{n}(t-1))*a(t)-g_{n})dt,$$ (3) 位置更新 $$L(t)=L(t-1)+v(t-1)dt,$$ 其中，$C_{b}^{n}$为本体坐标系到导航坐标系的旋转矩阵，$\Omega(t)$为时间间隔t内的旋转变化矩阵，a和w分别表示加速度计读数和陀螺仪读数，v和L分别表示世界坐标系下的速度和位置，g是重力值。 问题分析在SINS和PDR中，惯性传感器数据并不是独立的，系统状态都是有迁前一时刻的系统状态和当前时刻的惯性传感器观测值计算得到的。因此，在给定时间窗口内，惯性传感器数据不是独立的。本文假设一个伪独立条件的存在，即给定时间窗口内，导航状态的变化是独立的。因此，本文基于此，提出了序列学习的模型来估计追踪状态。 不可观测到的系统状态包括方向$C_{b}^{n}$，速度v和位置L。在传统模型中，计算公式定义如下： $$[C_{b}^{n}\ v\ L]=f([C_{b}^{n}\ v\ L]_{t-1}, [a\ w]_{t}).$$ (1) 位移L 为了将当前窗口的位移与前一时间窗口的位移分割开来，本文定义了位移变化量$\Delta L$，其是在时间窗口内是独立的，我们得到： $$\Delta L=\int_{t=0}^{n-1}v(t)dt.$$ 继续划分，位移主要由初始速度和加速度计读数计算得到，即： $$\Delta L=nv(0)dt+[(n-1)s_1+(n-2)s_2+\dots+s_{n-1}]dt^2,$$ 其中， $$s(t)=C_{b}^{n}(t-1)a(t)-g.$$ 其中，$s(t)$表示的是世界坐标系下加速度计读数的变化值。那么$\Delta L$就等于第n时刻初始速度对时间的积分加上后续时刻对加速度计的积分。又由公式（1）得： $$\Delta L=nv(0)dt+[(n-1)C_b^n(0)*a_1+(n-2)C_b^n(0)\Omega(1)*a_2+\dots+C_b^n(0)\prod_{i=1}^{n-2}*a_{n-1}]dt^2-\frac{n(n-1)}{2}gdt^2,$$ 进一步得到： $$\Delta L=nv(0)dt+C_b^n(0)Tdt^2-\frac{n(n-1)}{2}gdt^2,$$ 其中， $$T=(n-1)a_1+(n-2)\Omega(1)a_2+\dots+\prod_{i=1}^{n-2}\Omega(i)a_{n-1},$$ 由于本文关注的是水平面上人员的位置追踪问题，因此，竖直轴的变化量假设为0，那么行走位移变化量可表示为： $$\Delta l=||\Delta L||_2=||nv^b(0)dt+Tdt^2-\frac{n(n-1)}{2}g_0^bdt^2||_2.$$ 因此，水平移动距离的变化量可由初始速度、重力值、加速度计读数和加速度读数来计算得到，即： $$\Delta l=f(v^b(0),g_0^b,a_{1:n},w_{1:n}).$$ 此外，方向变化为$\phi$，我们得到： $$(\Delta l, \Delta \phi)=f_{\theta}(v^b(0),g_{0}^{b},\hat{a}_{1:n},\hat{w}_{1:n}),$$ 其中，$\hat{a}$和$\hat{w}$分别为IMU的加速度计读数和陀螺仪读数的原始值。 最后，为了得到全局的位置信息，我们得到如下公式： $$x_n=x_0+\Delta l cos(\phi_0+\Delta \phi)$$ $$y_n=y_0+\Delta l sin(\phi_0+\Delta \phi),$$ 其中，$(x_0,y_0)$为初始位置。 深度神经网络框架系统框架： 学习模型： $$(a, w){200*6}\underrightarrow{f\{\theta}}(\Delta l, \Delta \phi)_{1*2},$$ 其中，时间窗口为200帧，即2秒。 本文采用LSTM结构来训练。损失函数定义为： $$loss=\sum||\Delta \hat{l}-\Delta l||{2}^{2}+k||\Delta \hat{\phi}-\Delta \phi||\{2}^{2}.$$ 实验同一设备不同姿态 不同设备 行走轨迹 小车行走轨迹]]></content>
      <categories>
        <category>学术</category>
        <category>PDR</category>
      </categories>
      <tags>
        <tag>学术</tag>
        <tag>PDR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UNIX教程]]></title>
    <url>%2F2018%2F07%2F01%2FUNIX%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[基础统计文件中字数你可以使用 wc 命令来获取一个文件中的总的行数，字数和字符数。以下是简单的示例来查看有关上面创建的文件的信息: 1$ wc filename 2 19 103 filename 这里是所有四个列的细节: 第一列: 代表文件中的行数。 第二列: 代表文件中的字数。 第三列: 代表文件中的字符数。这是文件的实际大小。 第四列: 代表文件名。]]></content>
      <categories>
        <category>编程</category>
        <category>UNIX</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>UNIX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[毕业关键流程（三方协议、户口、档案、报到证）]]></title>
    <url>%2F2018%2F06%2F25%2F%E6%AF%95%E4%B8%9A%E5%85%B3%E9%94%AE%E6%B5%81%E7%A8%8B%EF%BC%88%E4%B8%89%E6%96%B9%E5%8D%8F%E8%AE%AE%E3%80%81%E6%88%B7%E5%8F%A3%E3%80%81%E6%A1%A3%E6%A1%88%E3%80%81%E6%8A%A5%E5%88%B0%E8%AF%81%EF%BC%89%2F</url>
    <content type="text"><![CDATA[三方协议户口档案报到证报到证改正我们都知道，报到证是办理落户和档案托管、挂靠，以及转正定级的的重要凭证。报到单位和地址是不能出现错误的。从大多数情况来看如果没有涉及到其他方面的问题，只能拿着错误的报到证回原毕业学校办理改签或改派了。 改正报到证抬头1、报到默认打回去生源地后，改派到生源地以外的单位工作 默认打回去生源地一般是XX市人力资源和社会保障局，需要修改报到证抬头单位的话，首先需要该单位同意改出，再到新的接收单位同意改入，才算是完成的改派。否则是无法转干部身份的或者只改入不该出或者只改出不改入都是容易造成档案丢失的。 2、报到证开到XX教育局，要改到XX人力资源和社会保障局 报到证是教育局的，一般都是师范生，档案也是送到教育局的，若需要改派到人事局，先要申请人事主管部门的接收函，必须要人事主管部门的盖章的接收函才有效。拿到接收函和报到证到教育局办理改出手续，再去人事局办理改入手续。 3、报到证抬头是A单位，改为B单位 从A单位那边拿到离职证明或者改派报到证备注，在拿到B单位的接收函，办理改派嵌入单位的手续。到省就业知道中心换取新的报到证。 改正报到证备注1、报到证备注单位有错别字 申请正确单位的接收函，必须要有人事局的盖章，到省就业知道中心改正错别字，但是省就业指导中心一般是不接受个人申请的业务。 2、报到证备注单位从A改成B 首先申请B单位的接收函，需要有B单位所在地人事局的盖章，带上B单位的接收函到A单位申请改派迁出，再到B单位所在城市办理改派迁入手续，需要找到发报到证的相关部门办理换新的报到证。 3、报到证没有备注单位 申请将要添加报到证备注的单位的接收函，带上接收函到发报到证的部门添加报到证备注。但是省就业指导中心一般是布受理个人申请添加备注。 改正报到证报到地址1、报到证报到地址错误 报到证备注地址错误一般是到学校修改，但是学校不受理的话。需要到发报到证的相关部门修改地址。可以直接手写上去，再让相关部门盖章，也可以重新打印新的报到证。 2、报到证报到地址格式不对 到省就业指导中心直接手写修改地址并盖公章或者直接换新的报到证。 注意：报到证修改过来后档案是还没有调走的，因此修改好报到证后还要记得让改入单位出调档函，去原单位调动档案到新的单位。此项工作可以在改派好报到证之后完成。 参考 http://cv.qiaobutang.com/knowledge/articles/56efafc70cf2b5cff66db3a6]]></content>
      <categories>
        <category>其他</category>
        <category>毕业流程</category>
      </categories>
      <tags>
        <tag>其他</tag>
        <tag>毕业流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VIO教程]]></title>
    <url>%2F2018%2F06%2F14%2FVIO%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[VIO背景 基础知识IMU建模误差基本误差：零偏误差、刻度误差。但实际上环境温度会对这两种误差均造成大大小小的影响（ADC温漂）。 （1）所以首先我们需要对传感器进行恒温处理，通常是在IMU的PCB板子上加一两个恒温电阻，利用传感器本身的温度测量功能进行加热闭环控制，我一般把这个目标温度设定在50-60度之间。 （2）实现恒温后，我们再开始校准传感器的两种误差： 陀螺仪。零偏误差可以通过在传感器静止时采集输出数据并取其平均值，刻度误差需要通过高精度转台来对每个轴进行校准； 加速度计。首先建立一个基本的误差方程，g为刻度误差，o为零偏误差，C为重力加速度，u为误差： $$(g_xa_x+o_x)^2+(g_ya_y+o_y)^2+(g_za_z+o_z)^2-C^2=u.$$ 这实际上就是一个椭球的方程，为了求解这个方程，我们需要采集多组传感器数据。这些数据尽量在球面上分布均匀（如常用的六面校准），然后最小二乘法求方程的最优解（使u最小），得到传感器的校准参数。 电子罗盘传感器也可以用这个方法来校准。 图像特征提取算法斑点检测 - LoG与DoH斑点检测 - SIFT斑点检测 - SURF角点检测 - Harris角点角点检测 - FAST角点在实时的视频流处理中，需要对每一帧特征提取，对算法处理速度上有很高的要求，传统的SIFT,Harris等特征点提取很难满足。由此提出Fast（Features from Accelerated Segment Test），由于不涉及尺度，梯度，等复杂运算，Fast检测器速度非常快。它使用一定邻域内像元的灰度值与中心点比较大小去判断是否为一个角点。但它的缺点是不具有方向性,尺度不变性。 Fast角点提取步骤（以Fast-12-16为例）： 1.以固定半径为圆的边上取16个像素点（图中白色框出的位置），与中心点像素值Ip做差。 2.若边上存在连续的12（N&gt;12,若为Fast-9,只需要N&gt;9）个点满足 ( I(x)-I(p) )&gt;threshold 或者 ( I(x)-I(p) ) &lt; -threshold。(其中I(x)表示边上的像素值，I(p)为中心点像素值，threshold为设定的阈值。)则此点作为一个候选角点。如图上的虚线连接的位置。通常为了加速计算，直接比较1,5,9,13位置的差值，超过三个即视为一个候选点（存在连续的12个像元的必要条件），否则直接排除。 3.非极大值抑制，排除不稳定角点。 二进制字符串特征描述子 - BRIEF算法二进制字符串特征描述子 - BRISK算法ORB算法FREAK算法 基于滤波的VIO方法基于松耦合的方法基于紧耦合的方法论文1. MSCKF: High-precision, consistent EKF-based visual-inertial odometry the multi-state-constraint Kalman filter (MSCKF) 基于优化的VIO方法基于松耦合的方法基于紧耦合的方法VINS文献解读测量预处理视觉预处理前端（1）对每张新图像，使用KLT稀疏光流算法跟踪已有特征。 （2）同时检测新的角点特征以维持每张图像100-300特征。其中，新的角点特征利用Good Features to track特征点检测算法，参考文献：《Good Features to track》，opencv，详细讲解参考：Good Features to track特征点检测原理与opencv（python）实现。 （3）检测器通过设置相邻特征最小间隔强制特征均匀分布，outlier去除后，特征投影在单位球面上（unit sphere）。Outlier通过RANSAC基础矩阵测试去除。 （4）关键帧的选取原则有两个：平均视差和跟踪的特征数。当跟踪的特征数小于某个门限或者跟踪特征的平均视差超过某个门限时，插入关键帧。请记住，除了平移，旋转也会导致视差，但是纯旋转时特征无法三角定位，我们在计算视差时用IMU propagation结果补偿旋转。 参考 https://www.zhihu.com/question/53571648 http://baijiahao.baidu.com/s?id=1573826383552981&amp;wfr=spider&amp;for=pc https://blog.csdn.net/luoshixian099/article/details/48294967 https://www.jianshu.com/p/2759593bc92b https://www.jianshu.com/p/387225a1aa60]]></content>
      <categories>
        <category>编程</category>
        <category>VIO</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>VIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim教程]]></title>
    <url>%2F2018%2F06%2F13%2Fvim%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[VIM简单配置VIM高级配置VIM简单命令VIM模式Normal 模式、Insert 模式、 用户刚刚启动 vi/vim，便进入了命令模式。此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。以下是常用的几个命令： 123i 切换到输入模式，以输入字符。x 删除当前光标所在处的字符。: 切换到底线命令模式，以在最底一行输入命令。 输入模式:在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键： 123456789字符按键以及Shift组合，输入字符ENTER，回车键，换行BACK SPACE，退格键，删除光标前一个字符DEL，删除键，删除光标后一个字符方向键，在文本中移动光标HOME/END，移动光标到行首/行尾Page Up/Page Down，上/下翻页Insert，切换光标为输入/替换模式，光标将变成竖线/下划线ESC，退出输入模式，切换到命令模式 底线命令模式 在命令模式下按下:（英文冒号）就进入了底线命令模式。底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。在底线命令模式中，基本的命令有（已经省略了冒号）： 12q 退出程序w 保存文件 按ESC键可随时退出底线命令模式。 使用 vi/vim 进入一般模式: 1vi runoob.txt 直接输入 vi 文件名 就能够进入 vi 的一般模式了。请注意，记得 vi 后面一定要加文件名，不管该文件存在与否！ 第一部份一般模式可用的光标移动、复制粘贴、搜索替换 移动光标的方法 含义 h 或 向左箭头键(←) 光标向左移动一个字符 j 或 向下箭头键(↓) 光标向下移动一个字符 k 或 向上箭头键(↑) 光标向上移动一个字符 l 或 向右箭头键(→) 光标向右移动一个字符 如果你将右手放在键盘上的话，你会发现 hjkl 是排列在一起的，因此可以使用这四个按钮来移动光标。 如果想要进行多次移动的话，例如向下移动 30 行，可以使用 “30j” 或 “30↓” 的组合按键， 亦即加上想要进行的次数(数字)后，按下动作即可！ 移动光标的方法 含义 [Ctrl] + [f] 屏幕『向下』移动一页，相当于 [Page Down]按键 (常用) [Ctrl] + [b] 屏幕『向上』移动一页，相当于 [Page Up] 按键 (常用) [Ctrl] + [d] 屏幕『向下』移动半页 [Ctrl] + [u] 屏幕『向上』移动半页 + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n 那个 n 表示『数字』，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。例如 20 则光标会向后面移动 20 个字符距离。 0 或功能键[Home] 这是数字『 0 』：移动到这一行的最前面字符处 (常用) \$ 或功能键[End] 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n n 为数字。光标向下移动 n 行(常用) 搜索替换 含义 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为『反向』进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示『向上』搜寻 vbird 。 :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！举例来说，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD 则：『:100,200s/vbird/VBIRD/g』。(常用) :1,\$s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,\$s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) 删除、复制与贴上 含义 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 『10x』。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d\$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点『.』就好了！ (常用) 第二部份：一般模式切换到编辑模式的可用的按钮说明 进入输入或取代的编辑模式 含义 i, I 进入输入模式(Insert mode)：i 为『从目前光标所在处输入』， I 为『在目前所在行的第一个非空格符处开始输入』。 (常用) a, A 进入输入模式(Insert mode)：a 为『从目前光标所在的下一个字符处开始输入』， A 为『从光标所在行的最后一个字符处开始输入』。(常用) o, O 进入输入模式(Insert mode)：这是英文字母 o 的大小写。o 为『在目前光标所在的下一行处输入新的一行』； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)：r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用) [Esc] 退出编辑模式，回到一般模式中(常用) 第三部份：一般模式切换到指令行模式的可用的按钮说明 指令行的储存、离开等指令 含义 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为『只读』时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 『filename』 这个档案内容加到游标所在行后面 :n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个档案。 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！例如『:! ls /home』即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ vim 环境的变更 含义 :set nu 显示行号，设定之后，会在每一行的前缀显示该行的行号 :set nonu 与 set nu 相反，为取消行号！ VIM的分屏功能分屏启动Vim使用大写的O参数来垂直分屏: 1vim -On file1 file2 ... 使用小写的o参数来水平分屏: 1vim -on file1 file2 ... 注释: n是数字，表示分成几个屏. 关闭分屏关闭当前窗口: 1Ctrl+W c 关闭当前窗口，如果只剩最后一个了，则退出Vim: 1Ctrl+W q 分屏上下分割当前打开的文件: 1Ctrl+W s 上下分割，并打开一个新的文件: 1:sp filename 左右分割当前打开的文件: 1Ctrl+W v 左右分割，并打开一个新的文件: 1:vsp filename 移动光标Vi中的光标键是h, j, k, l，要在各个屏间切换，只需要先按一下Ctrl+W. 12345Ctrl+W l # 把光标移到右边的屏Ctrl+W h # 把光标移到左边的屏中Ctrl+W k # 把光标移到上边的屏中Ctrl+W j # 把光标移到下边的屏中Ctrl+W w # 把光标移到下一个的屏中 移动分屏这个功能还是使用了Vim的光标键，只不过都是大写。当然了，如果你的分屏很乱很复杂的话，这个功能可能会出现一些非常奇怪的症状。 1234Ctrl+W L # 向右移动Ctrl+W H # 向左移动Ctrl+W K # 向上移动Ctrl+W J # 向下移动 屏幕尺寸下面是改变尺寸的一些操作，主要是高度，对于宽度你可以使用[Ctrl+W &lt;]或是[Ctrl+W &gt;]，但这可能需要最新的版本才支持。 123Ctrl+W = # 让所有的屏都有一样的高度Ctrl+W + # 增加高度Ctrl+W - # 减少高度]]></content>
      <categories>
        <category>编程</category>
        <category>VIM</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>VIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS免费搭建ss教程]]></title>
    <url>%2F2018%2F06%2F11%2FAWS%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BAss%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[注册免费的AWS服务器注册地址：ec2 免费试用全球云服务-亚马逊AWS 填写注册信息、输入信用卡（成功后扣除1美元）、电话语音确认。 上述过程成功后，即可进入EC2创建服务器实例。如下图所示： 创建服务器实例选择Ubuntu16.04 LTS系统： 选择实例类型： 根据自己的需求添加存储，免费用户最大30GB存储： 创建秘钥对： 编辑安全组规则： 登录实例并配置ss环境12$ sudo chmod 400 XXX.pem # 更改秘钥权限$ ssh -i XXX.pem ubuntu@[IP] # 利用公有IP登录实例 安装Shadowsocks: 123456789# 获取root权限sudo -s# 更新apt-getapt-get update# 安装python包管理工具apt-get install python-setuptoolsapt-get install python-pip# 安装shadowsockspip install shadowsocks 创建配置文件: 12mkdir /etc/shadowsocksvim /etc/shadowsocks/ss.json 配置文件内容： 1234567891011&#123; "server":"0.0.0.0", "server_port":443, //ss连接服务器的端口 "local_address":"127.0.0.1", "local_port":1080, "password":"abcd1234", // 设置ss连接时的密码 "timeout":300, "method":"aes-256-cfb", "fast_open":false, "workers": 1&#125; 启动Shadowsocks: 1234启动：sudo /usr/local/bin/ssserver -c /etc/shadowsocks/ss.json -d start停止：sudo /usr/local/bin/ssserver -c /etc/shadowsocks/ss.json -d stop 重启：sudo /usr/local/bin/ssserver -c /etc/shadowsocks/ss.json -d restart 配置自动切换模式配置好 ss 情景模式后虽然可以使用 Chrome 浏览器科学上网了，但是这样的话无论你访问什么网站都会走代理，有时候访问国内的一些网站反而会很慢，这时候自动切换模式就解决了这个问题。下面介绍一下如何配置自动切换模式。 点击左侧的 自动切换，或者自己新建情景模式，类型选择第二个 自动切换模式。然后做如下配置： 导入在线规则列表，类型选择AutoProxy，可以选择导入gfwlist - https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 或者自己自定义的AutoProxy文件。 保存设置并更新情景模式，若更新失败则开启全局代理后更新。 设置规则匹配则使用代理模式，否则直接连接。保存退出。 参考： https://blog.csdn.net/miracleswang/article/details/78959305 https://blog.csdn.net/kntanchao/article/details/79191149]]></content>
      <categories>
        <category>编程</category>
        <category>AWS</category>
        <category>SS</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>AWS</tag>
        <tag>ss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[darknet系列]yolov3训练自己的数据集]]></title>
    <url>%2F2018%2F06%2F09%2Fdarknet%E7%B3%BB%E5%88%97-yolov3%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[环境设置darknet版本： 2018年6月 yolov3 系统配置： 12ubuntu 16.0412GB Titan GPU 数据集：ILSVRC2015 (ILSVRC2015转VOC数据格式详见：http://mapstec.com/2018/04/05/ILSVRC2015%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%ACVOC2007%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F/) darknet配置下载工程: 1git clone https://github.com/pjreddie/darknet 修改Makefile,参考我的makefile文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100GPU=1CUDNN=0OPENCV=1OPENMP=0DEBUG=0ARCH= -gencode arch=compute_30,code=sm_30 \ -gencode arch=compute_35,code=sm_35 \ -gencode arch=compute_50,code=[sm_50,compute_50] \ -gencode arch=compute_52,code=[sm_52,compute_52]# -gencode arch=compute_20,code=[sm_20,sm_21] \ This one is deprecated?# This is what I use, uncomment if you know your arch and want to specify# ARCH= -gencode arch=compute_52,code=compute_52VPATH=./src/:./examplesSLIB=libdarknet.soALIB=libdarknet.aEXEC=darknetOBJDIR=./obj/CC=gccNVCC=/usr/local/cuda-8.0/bin/nvcc AR=arARFLAGS=rcsOPTS=-OfastLDFLAGS= -lm -pthread COMMON= -Iinclude/ -Isrc/CFLAGS=-Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPICifeq ($(OPENMP), 1) CFLAGS+= -fopenmpendififeq ($(DEBUG), 1) OPTS=-O0 -gendifCFLAGS+=$(OPTS)ifeq ($(OPENCV), 1) COMMON+= -DOPENCVCFLAGS+= -DOPENCVLDFLAGS+= `pkg-config --libs opencv` COMMON+= `pkg-config --cflags opencv` endififeq ($(GPU), 1) COMMON+= -DGPU -I/usr/local/cuda/include/CFLAGS+= -DGPULDFLAGS+= -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurandendififeq ($(CUDNN), 1) COMMON+= -DCUDNN CFLAGS+= -DCUDNNLDFLAGS+= -lcudnnendifOBJ=gemm.o utils.o cuda.o deconvolutional_layer.o convolutional_layer.o list.o image.o activations.o im2col.o col2im.o blas.o crop_layer.o dropout_layer.o maxpool_layer.o softmax_layer.o data.o matrix.o network.o connected_layer.o cost_layer.o parser.o option_list.o detection_layer.o route_layer.o upsample_layer.o box.o normalization_layer.o avgpool_layer.o layer.o local_layer.o shortcut_layer.o logistic_layer.o activation_layer.o rnn_layer.o gru_layer.o crnn_layer.o demo.o batchnorm_layer.o region_layer.o reorg_layer.o tree.o lstm_layer.o l2norm_layer.o yolo_layer.oEXECOBJA=captcha.o lsd.o super.o art.o tag.o cifar.o go.o rnn.o segmenter.o regressor.o classifier.o coco.o yolo.o detector.o nightmare.o darknet.oifeq ($(GPU), 1) LDFLAGS+= -lstdc++ OBJ+=convolutional_kernels.o deconvolutional_kernels.o activation_kernels.o im2col_kernels.o col2im_kernels.o blas_kernels.o crop_layer_kernels.o dropout_layer_kernels.o maxpool_layer_kernels.o avgpool_layer_kernels.oendifEXECOBJ = $(addprefix $(OBJDIR), $(EXECOBJA))OBJS = $(addprefix $(OBJDIR), $(OBJ))DEPS = $(wildcard src/*.h) Makefile include/darknet.h#all: obj backup results $(SLIB) $(ALIB) $(EXEC)all: obj results $(SLIB) $(ALIB) $(EXEC)$(EXEC): $(EXECOBJ) $(ALIB) $(CC) $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS) $(ALIB)$(ALIB): $(OBJS) $(AR) $(ARFLAGS) $@ $^$(SLIB): $(OBJS) $(CC) $(CFLAGS) -shared $^ -o $@ $(LDFLAGS)$(OBJDIR)%.o: %.c $(DEPS) $(CC) $(COMMON) $(CFLAGS) -c $&lt; -o $@$(OBJDIR)%.o: %.cu $(DEPS) $(NVCC) $(ARCH) $(COMMON) --compiler-options "$(CFLAGS)" -c $&lt; -o $@obj: mkdir -p objbackup: mkdir -p backupresults: mkdir -p results.PHONY: cleanclean: rm -rf $(OBJS) $(SLIB) $(ALIB) $(EXEC) $(EXECOBJ) $(OBJDIR)/* 编译darknet： 1make 配置完以后可以下载作者的预训练模型测试一下： 1wget https://pjreddie.com/media/files/yolov3.weights 下载之后用图片进行测试： 1./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg 制作自己的数据集本教授使用Imagenet数据集作为训练数据集，详细制作过程参考：http://mapstec.com/2018/04/05/ILSVRC2015%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%ACVOC2007%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F/ 数据处理按darknet的说明编译好后，接下来在darknet-master/scripts文件夹中新建文件夹VOCdevkit，然后将整个VOC2007文件夹都拷到VOCdevkit文件夹下。 然后，需要利用scripts文件夹中的voc_label.py文件生成一系列训练文件和label，具体操作如下： 首先需要修改voc_label.py中的代码，这里主要修改数据集名，以及类别信息，我的是VOC2007，并且所有样本用来训练，没有val或test，有1000类目标，因此按如下设置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import join#sets=[('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val'), ('2007', 'test')]sets = [('2007', 'train')]# classes = ["aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]classes = ["tench","goldfish","great_white_shark","tiger_shark","hammerhead","electric_ray","stingray","cock","hen","ostrich","brambling","goldfinch","house_finch","junco","indigo_bunting","robin","bulbul","jay","magpie","chickadee","water_ouzel","kite","bald_eagle","vulture","great_grey_owl","European_fire_salamander","common_newt","eft","spotted_salamander","axolotl","bullfrog","tree_frog","tailed_frog","loggerhead","leatherback_turtle","mud_turtle","terrapin","box_turtle","banded_gecko","common_iguana","American_chameleon","whiptail","agama","frilled_lizard","alligator_lizard","Gila_monster","green_lizard","African_chameleon","Komodo_dragon","African_crocodile","American_alligator","triceratops","thunder_snake","ringneck_snake","hognose_snake","green_snake","king_snake","garter_snake","water_snake","vine_snake","night_snake","boa_constrictor","rock_python","Indian_cobra","green_mamba","sea_snake","horned_viper","diamondback","sidewinder","trilobite","harvestman","scorpion","black_and_gold_garden_spider","barn_spider","garden_spider","black_widow","tarantula","wolf_spider","tick","centipede","black_grouse","ptarmigan","ruffed_grouse","prairie_chicken","peacock","quail","partridge","African_grey","macaw","sulphur-crested_cockatoo","lorikeet","coucal","bee_eater","hornbill","hummingbird","jacamar","toucan","drake","red-breasted_merganser","goose","black_swan","tusker","echidna","platypus","wallaby","koala","wombat","jellyfish","sea_anemone","brain_coral","flatworm","nematode","conch","snail","slug","sea_slug","chiton","chambered_nautilus","Dungeness_crab","rock_crab","fiddler_crab","king_crab","American_lobster","spiny_lobster","crayfish","hermit_crab","isopod","white_stork","black_stork","spoonbill","flamingo","little_blue_heron","American_egret","bittern","crane","limpkin","European_gallinule","American_coot","bustard","ruddy_turnstone","red-backed_sandpiper","redshank","dowitcher","oystercatcher","pelican","king_penguin","albatross","grey_whale","killer_whale","dugong","sea_lion","Chihuahua","Japanese_spaniel","Maltese_dog","Pekinese","Shih-Tzu","Blenheim_spaniel","papillon","toy_terrier","Rhodesian_ridgeback","Afghan_hound","basset","beagle","bloodhound","bluetick","black-and-tan_coonhound","Walker_hound","English_foxhound","redbone","borzoi","Irish_wolfhound","Italian_greyhound","whippet","Ibizan_hound","Norwegian_elkhound","otterhound","Saluki","Scottish_deerhound","Weimaraner","Staffordshire_bullterrier","American_Staffordshire_terrier","Bedlington_terrier","Border_terrier","Kerry_blue_terrier","Irish_terrier","Norfolk_terrier","Norwich_terrier","Yorkshire_terrier","wire-haired_fox_terrier","Lakeland_terrier","Sealyham_terrier","Airedale","cairn","Australian_terrier","Dandie_Dinmont","Boston_bull","miniature_schnauzer","giant_schnauzer","standard_schnauzer","Scotch_terrier","Tibetan_terrier","silky_terrier","soft-coated_wheaten_terrier","West_Highland_white_terrier","Lhasa","flat-coated_retriever","curly-coated_retriever","golden_retriever","Labrador_retriever","Chesapeake_Bay_retriever","German_short-haired_pointer","vizsla","English_setter","Irish_setter","Gordon_setter","Brittany_spaniel","clumber","English_springer","Welsh_springer_spaniel","cocker_spaniel","Sussex_spaniel","Irish_water_spaniel","kuvasz","schipperke","groenendael","malinois","briard","kelpie","komondor","Old_English_sheepdog","Shetland_sheepdog","collie","Border_collie","Bouvier_des_Flandres","Rottweiler","German_shepherd","Doberman","miniature_pinscher","Greater_Swiss_Mountain_dog","Bernese_mountain_dog","Appenzeller","EntleBucher","boxer","bull_mastiff","Tibetan_mastiff","French_bulldog","Great_Dane","Saint_Bernard","Eskimo_dog","malamute","Siberian_husky","dalmatian","affenpinscher","basenji","pug","Leonberg","Newfoundland","Great_Pyrenees","Samoyed","Pomeranian","chow","keeshond","Brabancon_griffon","Pembroke","Cardigan","toy_poodle","miniature_poodle","standard_poodle","Mexican_hairless","timber_wolf","white_wolf","red_wolf","coyote","dingo","dhole","African_hunting_dog","hyena","red_fox","kit_fox","Arctic_fox","grey_fox","tabby","tiger_cat","Persian_cat","Siamese_cat","Egyptian_cat","cougar","lynx","leopard","snow_leopard","jaguar","lion","tiger","cheetah","brown_bear","American_black_bear","ice_bear","sloth_bear","mongoose","meerkat","tiger_beetle","ladybug","ground_beetle","long-horned_beetle","leaf_beetle","dung_beetle","rhinoceros_beetle","weevil","fly","bee","ant","grasshopper","cricket","walking_stick","cockroach","mantis","cicada","leafhopper","lacewing","dragonfly","damselfly","admiral","ringlet","monarch","cabbage_butterfly","sulphur_butterfly","lycaenid","starfish","sea_urchin","sea_cucumber","wood_rabbit","hare","Angora","hamster","porcupine","fox_squirrel","marmot","beaver","guinea_pig","sorrel","zebra","hog","wild_boar","warthog","hippopotamus","ox","water_buffalo","bison","ram","bighorn","ibex","hartebeest","impala","gazelle","Arabian_camel","llama","weasel","mink","polecat","black-footed_ferret","otter","skunk","badger","armadillo","three-toed_sloth","orangutan","gorilla","chimpanzee","gibbon","siamang","guenon","patas","baboon","macaque","langur","colobus","proboscis_monkey","marmoset","capuchin","howler_monkey","titi","spider_monkey","squirrel_monkey","Madagascar_cat","indri","Indian_elephant","African_elephant","lesser_panda","giant_panda","barracouta","eel","coho","rock_beauty","anemone_fish","sturgeon","gar","lionfish","puffer","abacus","abaya","academic_gown","accordion","acoustic_guitar","aircraft_carrier","airliner","airship","altar","ambulance","amphibian","analog_clock","apiary","apron","ashcan","assault_rifle","backpack","bakery","balance_beam","balloon","ballpoint","Band_Aid","banjo","bannister","barbell","barber_chair","barbershop","barn","barometer","barrel","barrow","baseball","basketball","bassinet","bassoon","bathing_cap","bath_towel","bathtub","beach_wagon","beacon","beaker","bearskin","beer_bottle","beer_glass","bell_cote","bib","bicycle-built-for-two","bikini","binder","binoculars","birdhouse","boathouse","bobsled","bolo_tie","bonnet","bookcase","bookshop","bottlecap","bow","bow_tie","brass","brassiere","breakwater","breastplate","broom","bucket","buckle","bulletproof_vest","bullet_train","butcher_shop","cab","caldron","candle","cannon","canoe","can_opener","cardigan","car_mirror","carousel","carpenters_kit","carton","car_wheel","cash_machine","cassette","cassette_player","castle","catamaran","CD_player","cello","cellular_telephone","chain","chainlink_fence","chain_mail","chain_saw","chest","chiffonier","chime","china_cabinet","Christmas_stocking","church","cinema","cleaver","cliff_dwelling","cloak","clog","cocktail_shaker","coffee_mug","coffeepot","coil","combination_lock","computer_keyboard","confectionery","container_ship","convertible","corkscrew","cornet","cowboy_boot","cowboy_hat","cradle","crane","crash_helmet","crate","crib","Crock_Pot","croquet_ball","crutch","cuirass","dam","desk","desktop_computer","dial_telephone","diaper","digital_clock","digital_watch","dining_table","dishrag","dishwasher","disk_brake","dock","dogsled","dome","doormat","drilling_platform","drum","drumstick","dumbbell","Dutch_oven","electric_fan","electric_guitar","electric_locomotive","entertainment_center","envelope","espresso_maker","face_powder","feather_boa","file","fireboat","fire_engine","fire_screen","flagpole","flute","folding_chair","football_helmet","forklift","fountain","fountain_pen","four-poster","freight_car","French_horn","frying_pan","fur_coat","garbage_truck","gasmask","gas_pump","goblet","go-kart","golf_ball","golfcart","gondola","gong","gown","grand_piano","greenhouse","grille","grocery_store","guillotine","hair_slide","hair_spray","half_track","hammer","hamper","hand_blower","hand-held_computer","handkerchief","hard_disc","harmonica","harp","harvester","hatchet","holster","home_theater","honeycomb","hook","hoopskirt","horizontal_bar","horse_cart","hourglass","iPod","iron","jack-o-lantern","jean","jeep","jersey","jigsaw_puzzle","jinrikisha","joystick","kimono","knee_pad","knot","lab_coat","ladle","lampshade","laptop","lawn_mower","lens_cap","letter_opener","library","lifeboat","lighter","limousine","liner","lipstick","Loafer","lotion","loudspeaker","loupe","lumbermill","magnetic_compass","mailbag","mailbox","maillot","maillot","manhole_cover","maraca","marimba","mask","matchstick","maypole","maze","measuring_cup","medicine_chest","megalith","microphone","microwave","military_uniform","milk_can","minibus","miniskirt","minivan","missile","mitten","mixing_bowl","mobile_home","Model_T","modem","monastery","monitor","moped","mortar","mortarboard","mosque","mosquito_net","motor_scooter","mountain_bike","mountain_tent","mouse","mousetrap","moving_van","muzzle","nail","neck_brace","necklace","nipple","notebook","obelisk","oboe","ocarina","odometer","oil_filter","organ","oscilloscope","overskirt","oxcart","oxygen_mask","packet","paddle","paddlewheel","padlock","paintbrush","pajama","palace","panpipe","paper_towel","parachute","parallel_bars","park_bench","parking_meter","passenger_car","patio","pay-phone","pedestal","pencil_box","pencil_sharpener","perfume","Petri_dish","photocopier","pick","pickelhaube","picket_fence","pickup","pier","piggy_bank","pill_bottle","pillow","ping-pong_ball","pinwheel","pirate","pitcher","plane","planetarium","plastic_bag","plate_rack","plow","plunger","Polaroid_camera","pole","police_van","poncho","pool_table","pop_bottle","pot","potters_wheel","power_drill","prayer_rug","printer","prison","projectile","projector","puck","punching_bag","purse","quill","quilt","racer","racket","radiator","radio","radio_telescope","rain_barrel","recreational_vehicle","reel","reflex_camera","refrigerator","remote_control","restaurant","revolver","rifle","rocking_chair","rotisserie","rubber_eraser","rugby_ball","rule","running_shoe","safe","safety_pin","saltshaker","sandal","sarong","sax","scabbard","scale","school_bus","schooner","scoreboard","screen","screw","screwdriver","seat_belt","sewing_machine","shield","shoe_shop","shoji","shopping_basket","shopping_cart","shovel","shower_cap","shower_curtain","ski","ski_mask","sleeping_bag","slide_rule","sliding_door","slot","snorkel","snowmobile","snowplow","soap_dispenser","soccer_ball","sock","solar_dish","sombrero","soup_bowl","space_bar","space_heater","space_shuttle","spatula","speedboat","spider_web","spindle","sports_car","spotlight","stage","steam_locomotive","steel_arch_bridge","steel_drum","stethoscope","stole","stone_wall","stopwatch","stove","strainer","streetcar","stretcher","studio_couch","stupa","submarine","suit","sundial","sunglass","sunglasses","sunscreen","suspension_bridge","swab","sweatshirt","swimming_trunks","swing","switch","syringe","table_lamp","tank","tape_player","teapot","teddy","television","tennis_ball","thatch","theater_curtain","thimble","thresher","throne","tile_roof","toaster","tobacco_shop","toilet_seat","torch","totem_pole","tow_truck","toyshop","tractor","trailer_truck","tray","trench_coat","tricycle","trimaran","tripod","triumphal_arch","trolleybus","trombone","tub","turnstile","typewriter_keyboard","umbrella","unicycle","upright","vacuum","vase","vault","velvet","vending_machine","vestment","viaduct","violin","volleyball","waffle_iron","wall_clock","wallet","wardrobe","warplane","washbasin","washer","water_bottle","water_jug","water_tower","whiskey_jug","whistle","wig","window_screen","window_shade","Windsor_tie","wine_bottle","wing","wok","wooden_spoon","wool","worm_fence","wreck","yawl","yurt","web_site","comic_book","crossword_puzzle","street_sign","traffic_light","book_jacket","menu","plate","guacamole","consomme","hot_pot","trifle","ice_cream","ice_lolly","French_loaf","bagel","pretzel","cheeseburger","hotdog","mashed_potato","head_cabbage","broccoli","cauliflower","zucchini","spaghetti_squash","acorn_squash","butternut_squash","cucumber","artichoke","bell_pepper","cardoon","mushroom","Granny_Smith","strawberry","orange","lemon","fig","pineapple","banana","jackfruit","custard_apple","pomegranate","hay","carbonara","chocolate_sauce","dough","meat_loaf","pizza","potpie","burrito","red_wine","espresso","cup","eggnog","alp","bubble","cliff","coral_reef","geyser","lakeside","promontory","sandbar","seashore","valley","volcano","ballplayer","groom","scuba_diver","rapeseed","daisy","yellow_ladys_slipper","corn","acorn","hip","buckeye","coral_fungus","agaric","gyromitra","stinkhorn","earthstar","hen-of-the-woods","bolete","ear","toilet_tissue"]def convert(size, box): dw = 1./(size[0]) dh = 1./(size[1]) x = (box[0] + box[1])/2.0 - 1 y = (box[2] + box[3])/2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x*dw w = w*dw y = y*dh h = h*dh return (x,y,w,h)def convert_annotation(year, image_id): in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id)) out_file = open('VOCdevkit/VOC%s/labels/%s.txt'%(year, image_id), 'w') tree=ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult)==1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w,h), b) out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')wd = getcwd()for year, image_set in sets: if not os.path.exists('VOCdevkit/VOC%s/labels/'%(year)): os.makedirs('VOCdevkit/VOC%s/labels/'%(year)) image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split() list_file = open('%s_%s.txt'%(year, image_set), 'w') for image_id in image_ids: list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.JPEG\n'%(wd, year, image_id)) convert_annotation(year, image_id) list_file.close()#os.system("cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt &gt; train.txt")#os.system("cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt &gt; train.all.txt") 修改好后在该目录下运行命令：python voc_label.py，之后则在文件夹scripts\VOCdevkit\VOC2007下生成了文件夹lable。这里包含了类别和对应归一化后的位置（i guess，如有错请指正）。同时在scripts\下应该也生成了train_2007.txt这个文件，里面包含了所有训练样本的绝对路径。 修改.data文件，以cfg/voc.data为例： 123451 classes= 202 train = &lt;path-to-voc&gt;/train.txt3 valid = &lt;path-to-voc&gt;2007_test.txt4 names = data/voc.names5 backup = backup 使用自己的路径替换，例如我的配置如下： 1234classes= 1000train = /home/teng/programmings/semap/darknet/scripts/2007_train.txtnames = /home/teng/programmings/semap/darknet/data/voc.namesbackup = /home/teng/programmings/semap/darknet/backup 修改.names文件，即训练的label： 1234567tenchgoldfishgreat_white_shark...boleteeartoilet_tissue 配置文件修改做好了上述准备，就可以根据不同的网络设置（cfg文件）来训练了。在文件夹cfg中有很多cfg文件，应该跟caffe中的prototxt文件是一个意思。这里以yolo-voc.cfg为例，主要修改参数如下： 123456789101112131415161718192021222324252627282930313233343536[net]# Testing# batch=1# subdivisions=1 #训练时候把上面Testing的参数注释# Trainingbatch=64subdivisions=32 #这个参数根据自己GPU的显存进行修改，显存不够就改大一些... #因为训练时每批的数量 = batch/subdivisions......learning_rate=0.001 #根据自己的需求还有训练速度学习率可以调整一下burn_in=1000max_batches = 30000 #根据自己的需求还有训练速度max_batches可以调整一下policy=stepssteps=10000,20000 #跟着max_batches做相应调整.........[convolutional]size=1stride=1pad=1filters=30 #filters = 3*(classes + 5)activation=linear[yolo]mask = 0,1,2anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326classes=5 #修改类别数num=9jitter=.3ignore_thresh = .5truth_thresh = 1random=1 #显存小的话 =0#这个文件的最下面有3个YOLO层，这里我才放上来了一个，这三个地方的classes做相应修改#每个YOLO层的上一层的convolutional层的filters也要修改 开始训练下载预训练模型（权重）： 1wget https://pjreddie.com/media/files/darknet53.conv.74 训练： 1./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74 训练过程参数详解Region xx: cfg文件中yolo-layer的索引； Avg IOU:当前迭代中，预测的box与标注的box的平均交并比，越大越好，期望数值为1； Class: 标注物体的分类准确率，越大越好，期望数值为1； obj: 越大越好，期望数值为1； No obj: 越小越好； .5R: 以IOU=0.5为阈值时候的recall; recall = 检出的正样本/实际的正样本 0.75R: 以IOU=0.75为阈值时候的recall; count:正样本数目。 参考： https://pjreddie.com/darknet/yolo/ https://blog.csdn.net/lilai619/article/details/79695109 https://zhuanlan.zhihu.com/p/35490655 https://blog.csdn.net/ch_liu23/article/details/53558549]]></content>
      <categories>
        <category>编程</category>
        <category>深度学习</category>
        <category>yolo</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>深度学习</tag>
        <tag>yolo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UJI_LIB_DB数据集介绍]]></title>
    <url>%2F2018%2F06%2F07%2FUJI-LIB-DB%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[UJI_LIB_DB介绍论文出处 Mendoza-Silva, Germán Martín, Richter, Philipp, Torres-Sospedra, Joaquín, Lohan, Elena Simona, &amp; Huerta, Joaquín. (2017). Long-Term Wi-Fi fingerprinting dataset and supporting material (Version V1) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1066041 详细介绍1234567891011时间跨度： 15个月采集点数： 63504采集地点： 大学图书馆（3楼和5楼）AP放置位置： 天花板AP放置高度： 2.65米具体位置： 详见数据集AP数量： 448采集人员数量：1名采集方式： 右手放至胸前，每个采样点采集6个数据采集设备： Samsung Galaxy S3采集软件： 自研 数据库格式定义： $$DB={D_{(m,k,n)}}$$ 其中，$D_{(m,k,n)}$表示在第m月份捕获的第k类型的第n个数据集。每个数据及定义四个集合：RSS值、位置、时间和标识符集合。RSS值集合定义为： $$R_{(p*s)*a}={r_{i,j}},$$ 其中，p表示的是采样点的数量，s表示的是每个采样点采集的数据数量，a表示的是检测到的AP的数量，$r_{i,j}$表示的是第i个指纹第j个AP的RSS值。如果AP没有被检测得到，那么该AP的RSS值为100. 位置集合定义为： $$L_{p*s}*3={(x_i,y_i,f_i)},$$ 其中，$(x_i,y_i)$为二维坐标，$f_i$为第i个指纹被采集的楼层号码。 时间集合被定义为： $$T_{(p*s)*1}={t_i},$$ 其中，$t_i$是第i个指纹被采集的时间戳。 标识符集合被定义为： $$ID_{(P*S)*1}={id_{i}},$$ 其中，$id_{i}$为第i个指纹的标识符。例如： 数据集文件目录： (“trn” for training, “tst” for test, “rss”, “crd”, “tms” and “ids” for RSS values, positions, times and identifiers sets) 采集方法参考 http://www3.uji.es/~jtorres/databases.html https://zenodo.org/record/1066041#.WxjzeVOFPq0]]></content>
      <categories>
        <category>编程</category>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac/Linux-Shell-批量重命名的方法总览]]></title>
    <url>%2F2018%2F06%2F05%2FMac-Linux-Shell-%E6%89%B9%E9%87%8F%E9%87%8D%E5%91%BD%E5%90%8D%E7%9A%84%E6%96%B9%E6%B3%95%E6%80%BB%E8%A7%88%2F</url>
    <content type="text"><![CDATA[删除所有的 .bak 后缀1rename 's/\.bak$//' *.bak 把 .JPEG 文件后缀修改为 .jpg1rename 's/\.JPEG$/\.jpg/' *.JPEG 把所有文件的文件名改为小写1rename 'y/A-Z/a-z/' * 将 abcd.jpg 重命名为 abcd_efg.jpg1for var in *.jpg; do mv "$var" "$&#123;var%.jpg&#125;_efg.jpg"; done 将 abcd_efg.jpg 重命名为 abcd_lmn.jpg1for var in *.jpg; do mv "$var" "$&#123;var%_efg.jpg&#125;_lmn.jpg"; done 把文件名中所有小写字母改为大写字母1for var in `ls`; do mv -f "$var" `echo "$var" |tr a-z A-Z`; done 把格式 *_?.jpg 的文件改为 *_0?.jpg1for var in `ls *_?.jpg`; do mv "$var" `echo "$var" |awk -F '_' '&#123;print $1 "_0" $2&#125;'`; done 把文件名的后四个字母变为 vzomik1for var in `ls`; do mv -f "$var" `echo "$var" |sed 's/....$/vzomik/'`; done 把.txt变成.txt_bak 的后缀123456ls *.txt|xargs -n1 -i&#123;&#125; mv &#123;&#125; &#123;&#125;_bak# xargs -n1 –i&#123;&#125; 类似for循环，-n1意思是一个一个对象的去处理，-i&#123;&#125; 把前面的对象使用&#123;&#125;取代，mv &#123;&#125; &#123;&#125;_bak 相当于 mv 1.txt 1.txt_bak$ find ./*.txt -exec mv &#123;&#125; &#123;&#125;_bak \; # 这个命令中也是把&#123;&#125;作为前面find出来的文件的替代符，后面的”\”为”;”的脱意符，不然shell会把分号作为该行命令的结尾. 批量替换文件名12345678910#!/bin/basholdext="JPG"newext="jpg"dir=$(eval pwd)for file in $(ls $dir | grep .$oldext) do name=$(ls $file | cut -d. -f1) mv $file $&#123;name&#125;.$newext doneecho "change JPG=====&gt;jpg done!" 1.变量oldext和newext分别指定旧的扩展名和新的扩展名。dir指定文件所在目录； 2.“ls $dir | grep .$oldext”用来在指定目录dir中获取扩展名为旧扩展名的所有文件； 3.在循环体内先利用cut命令将文件名中“.”之前的字符串剪切出来，并赋值给name变量；接着将当前的文件名重命名为新的文件名。 通过这个脚本，所有照片的扩展名都成功修改。为了使这个脚本更具有通用型，我们可以增加几条read命令实现脚本和用户之间的交互。改进版的脚本如下： 123456789101112#!/bin/bashread -p "old extension:" oldextread -p "new extension:" newextread -p "The directory:" dircd $dirfor file in $(ls $dir | grep .$oldext) do name=$(ls $file | cut -d. -f1) mv $file $&#123;name&#125;.$newext echo "$name.$oldext ====&gt; $name.$newext" doneecho "all files has been modified." 修改后的脚本可以批量修改任意扩展名。 参考 http://edsionte.com/techblog/archives/category/shell%e7%bc%96%e7%a8%8b https://my.oschina.net/musings/blog/380939]]></content>
      <categories>
        <category>编程</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运筹学书单]]></title>
    <url>%2F2018%2F05%2F23%2F%E8%BF%90%E7%AD%B9%E5%AD%A6%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[什么是运筹学运筹学是20世纪三四十年代发展起来的一门新兴交叉学科。它主要研究人类对各种资源的运用及筹划，在满足一定约束的条件下，以期发挥有限资源的最大效益，达到总体最优的目标－－所谓运筹帷幄。最初由钱学森老先生引入中国，据说最开始的用途是优化航空／军工等领域。 别名：数学规划 (math programming)、优化 (optimization)、最优化理论、决策科学(Decision Science)等。 运筹学的分支 线性规划（Linear Programming）– 最简单和基础的优化问题，如上图，目标函数（max）和约束条件（s.t.）都是线性的，自变量x是实数变量，P问题（多项式时间可解）；或许有些读者没有学过线性代数，更简单的例子： min x1+x2 s.t. 3x1-4x2&gt; 5, x1,x2&gt;=0。 非线性规划 (Nonlinear Programming)–目标函数或约束条件为非线性，例如2次函数； 凸优化 (Convex Optimization)–约束条件形成的可行域（feasible region）是凸的； （混合）整数规划 (Mixed Integer Programming)–自变量有整数变量，NP难问题（指数级算法复杂度）。 半正定规划 (Semi-definite Programming)–每一个自变量x代表一个矩阵； 网络流问题（Network Flow Problems）–一个特殊的混合整数规划问题，满足一个节点流出流量=流入流（或许你听说过最大流最小割定理）； 动态规划(Dynamic Programming)、近似算法(Approximation Algorithms)、启发式算法（Heuristic Algorithms， Meta Heuristics）、遗传算法 (Generic Algorithms)–用来解例如整数规划等NP难优化问题的算法，后俩个通常只能得到局部最优解，最经典的当属最大流最小割定理衍生出来的一些最大流算法（全局最优）。被广泛得用在各个学科和领域，如人工智能； 鲁棒优化（Robust Optimization）–目标函数或约束条件有扰动（不确定性）的情况下，求解其最坏情况下的最优解； 多目标优化 (Muti-objective Optimization)–优化的目标函数是一个向量，通常通过引入权重权衡个目标函数，转化成单目标优化，或者寻找帕累托最优(Pareto Optimality) ； 双层优化（Bilevel Optimization）–和复合函数的概念类似，比如Max-Min Problem,在一个优化问题外嵌套另一个优化问题，通常用迭代法； 随机优化（Stochastic Programming）–加入了不确定的因素（通常以概率形式表现，目标函数变成求期望最大化）； 最优控制（Optimal Control）：严格说属于控制论的范畴，可以简单地把它理解为，优化问题中的变量由x变为f(x)，并且通常f是时间t的函数（或者状态state的函数），约束条件常有偏微分方程。也就是说，控制论期望通过解一个优化问题，得到一个最优的函数f（t），使得这个函数在全局（空间+时间）上是最优的。而运筹学通过解一个优化问题，得到的是最优解x，使得目标函数在一个确定性（deterministic，通常与t无关，或者可以理解为在t的某一时刻）的环境下是最优的。 课程书单基础课程微积分、线性代数、编程语言（C,C++,Python等）、图论（可选） 核心课程线性规划(最核心课程)、非线性规划（机器学习应用广泛）、（凸优化–工程系的必修课）、整数规划（决策问题中变量通常是整数，例如0,1变量）、网络流优化（物流、电网、通讯网络应用）、数学建模（可选）。 Linear Programing简略版，UCLA教授著： LINEAR PROGRAMMING Numerical Optimization，西北大学和美国阿贡实验室著（他引2w次）。 高级课程（Advanced）多目标优化、随机优化、鲁棒优化、启发式算法、近似算法、半正定规划等。 优化： 强烈推荐教材《Introduction to Linear Optimization》by D. Bertsimas and J. Tsitsiklis. 这是MIT两门研究生（博士和硕士）运筹学课程，15.081（6.251）和15.093，的教材。两位作者都是MIT运筹学中心ORC的教授，其中D.Bersimas是目前ORC的co-director之一，美国工程院院士（运筹优化领域）。这本书讲解非常详细，读起来比较愉快。 实战演练最后,应用或工程学科的朋友应更关心用运筹解决实际问题的流程，增加项目经验： 拿到实际问题和数据，先搜索已有的经典模型（例如网络流模型），看看能不能往经典模型上靠。 没有已有模型，就尝试自己建立新的优化模型，建模的思路函数是尽量线性化、少整数变量、big M尽量小（把二次函数线性化的trick）、低次函数（Polynomial）。 把优化问题编程到优化软件或编程调用优化Library 根据求解速度和最优解，不断优化算法和模型，以提高求解效率。 其他Augue运筹学里面其实更重要的是建模。换言之，就是看现实问题和数学语言是怎么对应的。这个因为考试的原因，太容易被忽略了其重要性。 建模这事情说难不难说易不易。易在好像就是定义几个变量，定义一下变量之间的关系和目标函数。难在１. 对现实问题要看透：什么才是问题里面的最重要的因素，抓住重点 ２. 找到最合适的数学语言和它对应， ３. 模型要尽量容易解。 第一点是因问题而异的，没法聊。第二点是可以通过了解各种模型适用于刻画具有什么结构的问题来达到。运筹学里面有很多模型。举几个例子：１. 线性规划能表示所有有线性结构的问题，例如做采购，我们知道了每家供应商的固定价格和最大供应量，我们希望最小化成本，那总成本＝单价×数量，这个就是这个问题里面的线性关系。２. 整数规划能处理一些线性规划处理不了的问题。例如还是采购，假如选了某家供应商，每选定一个供应商，还要增加一个固定成本，于是我们就要多设一个变量来代表是不是选了这个供应商，这时候就需要整数限制。不然那个变量解出来等于０.５，我们只选半个它？３. 当现实问题涉及多个参与者，每个参与者都有自己优化的东西，这时候就涉及互动，就可以将博弈论派上用场了。４. 如果见到一个系统是随时间变化的，就可以考虑用最优控制。等等等等。懂了对自己身边的具体问题建模，再拿个软件解一下模型，对大部分人就够了。所以要看书或者看视频自学的话，第一步是，每一章只看前面讲建模（modeling或者formulation）那一节。够用了。 参考 https://zhuanlan.zhihu.com/p/25579864?refer=operations-research]]></content>
      <categories>
        <category>理论</category>
        <category>运筹学</category>
      </categories>
      <tags>
        <tag>理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[colmap-posenet教程]]></title>
    <url>%2F2018%2F05%2F17%2Fcolmap-posenet%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Colmap 教程Posenet 教程转换colmap数据至文本文件1$ ./colmap-path/build/src/exe/model_converter --input_path ./ --output_path ./ --ouput_type TXT 根据TXT文件生成posenet的训练数据集和测试数据集1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!/usr/bin/python#coding=utf-8'''Created on 2017年11月26日@author: teng'''import osfrom numpy import*import numpy as npimport randomdef ReadLog(log_file): lines = [] with open(log_file, 'r') as fin: lines=fin.readlines() return linesif __name__ == '__main__': imagesFile = [] q = [] loc = [] count = 0 count_data = 0 lines = ReadLog('images.txt') for line in lines[4:]: if count % 2 == 0: data = line.strip().split() imagesFile.append(str(data[9])) q.append([float(data[1]), float(data[2]), float(data[3]), float(data[4])]) loc.append([float(data[5]), float(data[6]), float(data[7])]) count_data += 1 count += 1 randomTest = [] randomVal = [] for i in range(int(count_data/10)): randomTest.append(random.randint(0, count_data-1)) randomVal.append(random.randint(0, count_data-1)) output = open('train.txt', 'w') output1 = open('test.txt', 'w') output2 = open('val.txt', 'w') for i in range(len(loc)): if i in randomTest: output1.write("images/" + str(imagesFile[i]) + " " + str(loc[i][0]) + " " + \ str(loc[i][1]) + " " + str(loc[i][2]) + " " + \ str(q[i][0]) + " " + str(q[i][1]) + " " + \ str(q[i][2]) + " " + str(q[i][3]) + "\n") elif i in randomVal: output2.write("images/" + str(imagesFile[i]) + " " + str(loc[i][0]) + " " + \ str(loc[i][1]) + " " + str(loc[i][2]) + " " + \ str(q[i][0]) + " " + str(q[i][1]) + " " + \ str(q[i][2]) + " " + str(q[i][3]) + "\n") else: output.write("images/" + str(imagesFile[i]) + " " + str(loc[i][0]) + " " + \ str(loc[i][1]) + " " + str(loc[i][2]) + " " + \ str(q[i][0]) + " " + str(q[i][1]) + " " + \ str(q[i][2]) + " " + str(q[i][3]) + "\n") output.close() output1.close() output2.close() posenet 教程生成lmdb文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556caffe_root = '/home/teng/programmings/semap/caffe-posenet/' # Change to your directory to caffe-posenetimport syssys.path.insert(0, caffe_root + 'python')import numpy as npimport lmdbimport caffeimport randomimport cv2directory = '/home/teng/programmings/semap/caffe-posenet/posenet/models/611/'dataset = 'train.txt'poses = []images = []with open(directory+dataset) as f: for line in f: fname, p0,p1,p2,p3,p4,p5,p6 = line.split() p0 = float(p0) p1 = float(p1) p2 = float(p2) p3 = float(p3) p4 = float(p4) p5 = float(p5) p6 = float(p6) poses.append((p0,p1,p2,p3,p4,p5,p6)) images.append(directory+fname)r = list(range(len(images)))print rprint 'r = \n'random.shuffle(r)print rprint 'Creating PoseNet Dataset.'env = lmdb.open('train_lmdb', map_size=int(1e12))count = 0for i in r: print('i = &#123;&#125;, r = &#123;&#125;').format(i, len(r)) if (count+1) % 100 == 0: print 'Saving image: ', count+1 X = cv2.imread(images[i]) print('images[i] = \n X = &#123;&#125;').format(images[i], X) X = cv2.resize(X, (455,256)) # to reproduce PoseNet results, please resize the images so that the shortest side is 256 pixels X = np.transpose(X,(2,0,1)) im_dat = caffe.io.array_to_datum(np.array(X).astype(np.uint8)) im_dat.float_data.extend(poses[i]) str_id = '&#123;:0&gt;10d&#125;'.format(count) with env.begin(write=True) as txn: txn.put(str_id, im_dat.SerializeToString()) count = count+1env.close() test文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556caffe_root = '/home/teng/programmings/semap/caffe-posenet/' # Change to your directory to caffe-posenetimport syssys.path.insert(0, caffe_root + 'python')import numpy as npimport lmdbimport caffeimport randomimport cv2directory = '/home/teng/programmings/semap/caffe-posenet/posenet/models/611/'dataset = 'test.txt'poses = []images = []with open(directory+dataset) as f: for line in f: fname, p0,p1,p2,p3,p4,p5,p6 = line.split() p0 = float(p0) p1 = float(p1) p2 = float(p2) p3 = float(p3) p4 = float(p4) p5 = float(p5) p6 = float(p6) poses.append((p0,p1,p2,p3,p4,p5,p6)) images.append(directory+fname)r = list(range(len(images)))print rprint 'r = \n'random.shuffle(r)print rprint 'Creating PoseNet Dataset.'env = lmdb.open('test_lmdb', map_size=int(1e12))count = 0for i in r: print('i = &#123;&#125;, r = &#123;&#125;').format(i, len(r)) if (count+1) % 100 == 0: print 'Saving image: ', count+1 X = cv2.imread(images[i]) print('images[i] = \n X = &#123;&#125;').format(images[i], X) X = cv2.resize(X, (455,256)) # to reproduce PoseNet results, please resize the images so that the shortest side is 256 pixels X = np.transpose(X,(2,0,1)) im_dat = caffe.io.array_to_datum(np.array(X).astype(np.uint8)) im_dat.float_data.extend(poses[i]) str_id = '&#123;:0&gt;10d&#125;'.format(count) with env.begin(write=True) as txn: txn.put(str_id, im_dat.SerializeToString()) count = count+1env.close() 计算均值文件12$ ./caffe-posenet-path/build/tools/compute_image_mean train_lmdb/ train.binaryproto$ ./caffe-posenet-path/biuld/tools/compute_image_mean test_lmdb/ test.binaryproto 修改训练配置文件修改 train_bayesian_posenet.prototxt12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152name: "GoogLeNet"layers &#123; top: "data" top: "label" name: "data" type: DATA data_param &#123; source: "/home/teng/programmings/semap/caffe-posenet/posenet/models/611/lmdb/train_lmdb/" # revised batch_size: 64 backend: LMDB &#125; include &#123; phase: TRAIN &#125; transform_param &#123; mirror: false crop_size: 224 mean_file: "/home/teng/programmings/semap/caffe-posenet/posenet/models/611/lmdb/train.binaryproto" # revised &#125;&#125;layers &#123; top: "data" top: "label" name: "data" type: DATA data_param &#123; source: "/home/teng/programmings/semap/caffe-posenet/posenet/models/611/lmdb/test_lmdb" # revised batch_size: 32 backend: LMDB &#125; include &#123; phase: TEST &#125; transform_param &#123; mirror: false crop_size: 224 mean_file: "/home/teng/programmings/semap/caffe-posenet/posenet/models/611/lmdb/test.binaryproto" # revised &#125;&#125;layers &#123; name: "slice_label" type: SLICE bottom: "label" top: "label_xyz" top: "label_wpqr" slice_param &#123; slice_dim: 1 slice_point: 3 &#125;&#125;...... 修改 solver_bayesian_posenet.prototxt12345678910111213141516net: "/home/teng/programmings/semap/caffe-posenet/posenet/models/611/train/ train_bayesian_posenet.prototxt" #训练或者测试配置文件test_initialization: falsetest_iter: 7 #完成一次测试需要的迭代次数test_interval: 36 #测试间隔base_lr: 0.00001 #基础学习率lr_policy: "step" #学习率变化规律gamma: 0.9 #学习率变化指数stepsize: 20000 #学习率变化频率momentum: 0.9 #动量display: 20 #屏幕显示间隔max_iter: 120000 #最大迭代次数solver_type: SGDweight_decay: 0.005 #权重衰减snapshot: 5000 #保存模型间隔snapshot_prefix: "/home/teng/programmings/semap/caffe-posenet/posenet/models/611/train/models/posenet" #权重衰减solver_mode: GPU #是否使用GPU 解释： 训练样本 总共:2280个 batch_szie:64 将所有样本处理完一次（称为一代，即epoch)需要：2280/64=36 次迭代才能完成 所以这里将test_interval设置为36，即处理完一次所有的训练数据后，才去进行测试。所以这个数要大于等于36.如果想训练100代，则最大迭代次数为3600； 测试样本 同理，如果有212个测试样本，batch_size为32，那么需要7次才能完整的测试一次。 所以test_iter为7；这个数要大于等于7. 学习率 学习率变化规律我们设置为随着迭代次数的增加，慢慢变低。总共迭代120000次，我们将变化6次，所以stepsize设置为120000/6=20000，即每迭代20000次，我们就降低一次学习率。 训练1234567$ ./caffe-posenet-path/build/tools/caffe train -solver=examples/mnist/lenet_solver.prototxt 2&gt;&amp;1 | tee log# 从中断点的 snapshot 继续训练$ ./caffe-posenet-path/build/tools/caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate# 观察各个阶段的运行时间可以使用$ ./caffe-posenet-path/build/tools/caffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10# 使用已有模型提取特征$ ./caffe-posenet-path/build/tools/extract_features.bin models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel examples/feature_extraction/train_val.prototxt fc7 examples/temp_features 10 lmdb 1） fc7表示提取全连接第七层特征，conv5表示提取第五个卷积层的特征， examples/temp_features表示存放结果的目录（目录不需要提前构建） 2.）10：输入的包的数量，我们test时的batchsize是50，这里输入10，表示会提取50*10=500张图片的特征 3.）imageNet网络有很多层（data conv1 conv2 conv3 conv4 conv5 fc6 fc7 fc8 prob），我们可以选取任意一层；fc7是最后一层特征，fc8输出的就是softmax的输出了，所以提取fc7层 4.）lmdb：输出的数据格式是lmdb，还可以是leveldb]]></content>
      <categories>
        <category>编程</category>
        <category>colmap-posenet</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>colmap-posenet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-深度学习环境配置-Centos]]></title>
    <url>%2F2018%2F04%2F08%2Flinux-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-Centos%2F</url>
    <content type="text"><![CDATA[Centos 7 环境安装下载Centos 7安装镜像，制作启动优盘。 Install CentOS 7 安装CentOS 7。 第一步，配置日期、语言和键盘。 第二步，选择-系统-安装位置，进入磁盘分区界面。选择-其它存储选项-分区-我要配置分区，点左上角的“完成”，进入下面的界面: 1234# 分区前先规划好:# swap #交换分区，一般设置为内存的2倍# / #剩余所有空间# 挂载点：swap, 期望容量：2048 点左上角的“完成”，接受更改。 第三步，在这步中，你可以通过选择列表中安全配置来设置你的系统“安全策略Security Policy”，点击选择配置按钮来选择你想要的安全配置并点击“应用安全策略Apply security policy”按钮到 On。点击“完成Done”按钮后继续安装流程。 第四步，点击“软件选择Software Selection”按钮来配置你的基础机器环境。左边的列表是你可以选择安装桌面环境（Gnome、KDE Plasma 或者创意工作站）或者安装一个服务器环境（Web 服务器、计算节点、虚拟化主机、基础设施服务器、带图形界面的服务器或者文件及打印服务器）或者执行一个最小化的安装。为了随后能自定义你的系统，选择最小化安装并附加兼容库，点击“完成Done”按钮继续。对于完整的 Gnome 或者 KDE 桌面环境。选择： GNOM Applications Internet Applications Compatibility Libries Compatibility Libries 第五步，设置你的主机名并启用网络服务。点击“网络和主机名Network &amp; Hostname”，在主机名中输入你的 FQDN（完整限定网域名称），如果你在局域网中有一个 DHCP 服务器，将以太网按钮从 OFF 切换到 ON 来激活网络接口。为了静态配置你的网络接口，点击“配置Configure”按钮，添加 IP 设置，并点击“保存Save”按钮来应用更改。完成后，点击“完成Done”按钮来回到主安装菜单。 第六步，最后检查下所有到目前为止的配置，如果一切没问题，点击“开始安装Begin Installation”按钮开始安装 基础配置Centos 7 更换阿里源备份 1mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 下载新的CentOS-Base.repo 到/etc/yum.repos.d/ 12345678# CentOS 5wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo# CentOS 6wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo# CentOS 7wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 之后运行yum makecache生成缓存. 安装常用工具1yum -y install nano vim wget curl net-tools lsof gcc gcc-c++ll 等待安装完成即可。如果提示有错可以执行： 1yum makecache 重建缓存即可。 NVIDIA显卡驱动安装检查是否安装了GPU1lspci | grep -i nvidia 安装kernel-devel和kernel-headers12yum install kernel-devel yum install kernel-headers 修改/etc/modprobe.d/blacklist.conf 文件，以阻止 nouveau 模块的加载方法： 添加blacklist nouveau，注释掉blacklist nvidiafb（如果存在）blacklist.conf不存在时，执行下面的脚本 1# echo -e "blacklist nouveau\noptions nouveau modeset=0" &gt; /etc/modprobe.d/blacklist.conf 重新建立initramfs image文件12# mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak# dracut /boot/initramfs-$(uname -r).img $(uname -r) 安装驱动123456Ctrl + Alt +F2 #纯文本命令模式 登陆----获取root权限 init 3 切换至安装包文件夹 ./NVIDIA-Linux-x86_64-375.66.run #根据提示安装 cuda_8.0.61_375.26_linux.run #根据提示安装 安装cuda123$ sudo rpm -i cuda-repo-rhel7-8-0-local-ga2-8.0.61-1.x86_64.rpm$ sudo yum clean all$ sudo yum install cuda 报错了： 1234Error: Package: 1:nvidia-kmod-375.26-2.el7.x86_64 (cuda-8-0-local-ga2) Requires: dkms You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest 缺少2个包，装第一个： 1sudo vim /etc/yum.repos.d/linuxtech.testing.repo 输入： 123456[linuxtech-testing]name=LinuxTECH Testingbaseurl=http://pkgrepo.linuxtech.net/el6/testing/enabled=0gpgcheck=1gpgkey=http://pkgrepo.linuxtech.net/el6/release/RPM-GPG-KEY-LinuxTECH.NET 1sudo yum --enablerepo=linuxtech-testing install libvdpau 第二个： 12yum -y install epel-releaseyum -y install --enablerepo=epel dkms 配置环境变量12345678gedit ~/.bashrc #写入bashrc文件保存 #gpu driver export CUDA_HOME=/usr/local/cuda-8.0 export PATH=/usr/local/cuda-8.0/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH export LD_LIBRARY_PATH="/usr/local/cuda-8.0/lib:$&#123;LD_LIBRARY_PATH&#125;" source ~/.bashrc 测试1nvidia-smi 参考 https://my.oschina.net/u/2449787/blog/778145]]></content>
      <categories>
        <category>编程</category>
        <category>Linux</category>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Linux</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git教程]]></title>
    <url>%2F2018%2F04%2F06%2Fgit%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Github常用命令总结远程仓库的使用本小节将远程仓库变为自己可推送数据的仓库。 1234567$ git clone git://github.com/schacon/ticgit.git$ cd grit$ git remote -v$ git remote add pb git://github.com/paulboone/ticgit.git$ git fetch pb$ git push origin master$ git remote show origin Git命令总结 git init 命令把这个目录变成Git可以管理的仓库 git add 文件添加到仓库 git commit 文件提交到仓库 git log 历史记录 git log –pretty=oneline 历史记录 HEAD 当前版本 HEAD^ 上一个版本 HEAD^^ 上上一个版本 HEAD~100 往上100个版本 git reset –hard HEAD^ 回退到上一个版本 git reflog 记录你的每一次命令 git diff HEAD – readme.txt 查看工作区和版本库里面最新版本的区别 git status 版本的状态 git checkout – file 丢弃工作区的修改 git reset HEAD file 暂存区的修改撤销掉（unstage），重新放回工作区 git reset 既可以回退版本，也可以把暂存区的修改回退到工作区 git rm 从版本库中删除该文件 git remote add origin git@github.com:michaelliao/learngit.git 关联远程库 git push -u origin master 本地库的所有内容推送到远程库上 git push 本地库的内容推送到远程 git push origin master 推送最新修改 git clone git@github.com:michaelliao/gitskills.git 克隆一个本地库 git checkout -b dev 创建dev分支，然后切换到dev分支 git branch 查看当前分支 git checkout master 切换回master分支 git merge dev 把dev分支的工作成果合并到master分支上 git merge 合并指定分支到当前分支 git branch -d dev 删除dev分支 git branch 创建分支 git checkout 切换分支 git log –grap 分支合并图 git merge –no-ff -m “merge with no-ff” dev 强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息 git stash 把当前工作现场“储藏”起来，等以后恢复现场后继续工作 git stash list 工作现场 git stash apply 恢复stash内容并不删除 git stash drop 删除工作现场 git stash pop 恢复的同时把stash内容也删了 git branch -D 丢弃一个没有被合并过的分支 git remote 查看远程库的信息 git remote -v 显示更详细的信息 git pull 最新的提交从origin/dev抓下来 git branch –set-upstream dev origin/dev git pull也失败了，原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接，建立本地分支和远程分支的关联 git checkout -b branch-name origin/branch-name 在本地创建和远程分支对应的分支，本地和远程分支的名称最好一致 git tag 打一个新标签 git tag 查看所有标签 git log –pretty=oneline –abbrev-commit 找到历史提交的commit id git show 查看标签信息 git tag -a -m “blablabla…” 指定标签信息 git tag -s -m “blablabla…” 用PGP签名标签 git push origin 推送某个标签到远程 git push origin –tags 一次性推送全部尚未推送到远程的本地标签 git tag -d 删除一个本地标签 git push origin :refs/tags/ 删除一个远程标签 git config –global alias.st status 告诉Git，以后st就表示status 安装Git Linux上安装Git $ sudo apt-get install git Mac OS X上安装Git $ brew install git Windows上安装Git 在Windows上使用Git，可以从Git官网直接下载安装程序，（网速慢的同学请移步国内镜像），然后按默认选项安装即可。 安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！ 安装完成后，还需要最后一步设置，在命令行输入： $ git config –global user.name “Your Name” $ git config –global user.email “email@example.com“ 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。 注意git config命令的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 创建版本库版本库又名仓库，英文名repository。首先，选择一个合适的地方，创建一个空目录： $ mkdir learngit $ cd learngit $ pwd /Users/michael/learngit 第二步，通过git init命令把这个目录变成Git可以管理的仓库： $ git init Initialized empty Git repository in /Users/michael/learngit/.git/ 瞬间Git就把仓库建好了，而且告诉你是一个空的仓库（empty Git repository），细心的读者可以发现当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 如果你没有看到.git目录，那是因为这个目录默认是隐藏的，用ls -ah命令就可以看见。 把文件添加到版本库,现在我们编写一个readme.txt文件，内容如下： Git is a version control system. Git is free software. 用命令git add告诉Git，把文件添加到仓库： $ git add readme.txt 执行上面的命令，没有任何显示，这就对了，Unix的哲学是“没有消息就是好消息”，说明添加成功。 用命令git commit告诉Git，把文件提交到仓库： $ git commit -m &quot;wrote a readme file&quot; [master (root-commit) cb926e7] wrote a readme file 1 file changed, 2 insertions(+) create mode 100644 readme.txt 简单解释一下git commit命令，-m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。 git commit命令执行成功后会告诉你，1个文件被改动（我们新添加的readme.txt文件），插入了两行内容（readme.txt有两行内容）。 为什么Git添加文件需要add，commit一共两步呢？因为commit可以一次提交很多文件，所以你可以多次add不同的文件，比如： $ git add file1.txt $ git add file2.txt file3.txt $ git commit -m &quot;add 3 files.&quot; 版本回退在实际工作中，我们脑子里怎么可能记得一个几千行的文件每次都改了什么内容，不然要版本控制系统干什么。版本控制系统肯定有某个命令可以告诉我们历史记录，在Git中，我们用git log命令查看 $ git log commit 3628164fb26d48395383f8f31179f24e0882e1e0 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 15:11:49 2013 +0800 append GPL commit ea34578d5496d7dd233c827ed32a8cd576c5ee85 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file git log命令显示从最近到最远的提交日志，我们可以看到3次提交，最近的一次是append GPL，上一次是add distributed，最早的一次是wrote a readme file。如果嫌输出信息太多，看得眼花缭乱的，可以试试加上–pretty=oneline参数： $ git log --pretty=oneline 3628164fb26d48395383f8f31179f24e0882e1e0 append GPL ea34578d5496d7dd233c827ed32a8cd576c5ee85 add distributed cb926e7ea50ad11b8f9e909c05226233bf755030 wrote a readme file 需要友情提示的是，你看到的一大串类似3628164…882e1e0的是commit id（版本号），和SVN不一样，Git的commit id不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示，而且你看到的commit id和我的肯定不一样，以你自己的为准。为什么commit id需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。 好了，现在我们启动时光穿梭机，准备把readme.txt回退到上一个版本，也就是“add distributed”的那个版本，怎么做呢？ 首先，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交3628164…882e1e0（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令： $ git reset --hard HEAD^ HEAD is now at ea34578 add distributed 看看readme.txt的内容是不是版本add distributed： $ cat readme.txt Git is a distributed version control system. Git is free software. 现在，你回退到了某个版本，关掉了电脑，第二天早上就后悔了，想恢复到新版本怎么办？找不到新版本的commit id怎么办？ 在Git中，总是有后悔药可以吃的。当你用$ git reset –hard HEAD^回退到add distributed版本时，再想恢复到append GPL，就必须找到append GPL的commit id。Git提供了一个命令git reflog用来记录你的每一次命令： $ git reflog ea34578 HEAD@{0}: reset: moving to HEAD^ 3628164 HEAD@{1}: commit: append GPL ea34578 HEAD@{2}: commit: add distributed cb926e7 HEAD@{3}: commit (initial): wrote a readme file 撤销修改 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout – file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。 删除文件一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用rm命令删了 这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： $ git rm test.txt rm &apos;test.txt&apos; $ git commit -m &quot;remove test.txt&quot; [master d17efd8] remove test.txt 1 file changed, 1 deletion(-) delete mode 100644 test.txt 现在，文件就从版本库中被删除了。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： $ git checkout -- test.txt git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 添加远程库在Repository name填入learngit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库 目前，在GitHub上的这个learngit仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。 现在，我们根据GitHub的提示，在本地的learngit仓库下运行命令： $ git remote add origin git@github.com:michaelliao/learngit.git 添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。 下一步，就可以把本地库的所有内容推送到远程库上： $ git push -u origin master Counting objects: 19, done. Delta compression using up to 4 threads. Compressing objects: 100% (19/19), done. Writing objects: 100% (19/19), 13.73 KiB, done. Total 23 (delta 6), reused 0 (delta 0) To git@github.com:michaelliao/learngit.git * [new branch] master -&gt; master Branch master set up to track remote branch master from origin. 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 从现在起，只要本地作了提交，就可以通过命令： $ git push origin master 把本地master分支的最新修改推送至GitHub 创建与合并分支 原理： 在版本回退里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 一开始的时候，master分支是一条线，Git用master指向最新的提交，再用HEAD指向master，就能确定当前分支，以及当前分支的提交点，每次提交，master分支都会向前移动一步，这样，随着你不断提交，master分支的线也越来越长 当我们创建新的分支，例如dev时，Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev，你看，Git创建一个分支很快，因为除了增加一个dev指针，改改HEAD的指向，工作区的文件都没有任何变化！不过，从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变 假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并，所以Git合并分支也很快！就改改指针，工作区内容也不变！合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支： 实战 首先，我们创建dev分支，然后切换到dev分支： $ git checkout -b dev Switched to a new branch &apos;dev&apos; git checkout命令加上-b参数表示创建并切换，相当于以下两条命令： $ git branch dev $ git checkout dev Switched to branch &apos;dev&apos; 然后，用git branch命令查看当前分支 $ git branch * dev master git branch命令会列出所有分支，当前分支前面会标一个*号。 然后，我们就可以在dev分支上正常提交 $ git add readme.txt $ git commit -m &quot;branch test&quot; [dev fec145a] branch test 1 file changed, 1 insertion(+) 现在，dev分支的工作完成，我们就可以切换回master分支 $ git checkout master Switched to branch &apos;master&apos; 切换回master分支后，再查看一个readme.txt文件，刚才添加的内容不见了！因为那个提交是在dev分支上，而master分支此刻的提交点并没有变 现在，我们把dev分支的工作成果合并到master分支上： $ git merge dev Updating d17efd8..fec145a Fast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) git merge命令用于合并指定分支到当前分支。合并后，再查看readme.txt的内容，就可以看到，和dev分支的最新提交是完全一样的。 注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。 当然，也不是每次合并都能Fast-forward，我们后面会讲其他方式的合并。 合并完成后，就可以放心地删除dev分支了： $ git branch -d dev Deleted branch dev (was fec145a). 删除后，查看branch，就只剩下master分支了： $ git branch * master 因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。 小结 Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch 切换分支：git checkout 创建+切换分支：git checkout -b 合并某分支到当前分支：git merge 删除分支：git branch -d 解决冲突当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 用git log –graph命令可以看到分支合并图。 分支管理策略通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。 如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。 下面我们实战一下–no-ff方式的git merge： 首先，仍然创建并切换dev分支： $ git checkout -b dev Switched to a new branch &apos;dev&apos; 修改readme.txt文件，并提交一个新的commit： $ git add readme.txt $ git commit -m &quot;add merge&quot; [dev 6224937] add merge 1 file changed, 1 insertion(+) 现在，我们切换回master： $ git checkout master Switched to branch ‘master’准备合并dev分支，请注意–no-ff参数，表示禁用Fast forward： $ git merge --no-ff -m &quot;merge with no-ff&quot; dev Merge made by the &apos;recursive&apos; strategy. readme.txt | 1 + 1 file changed, 1 insertion(+) 因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。 合并后，我们用git log看看分支历史： $ git log --graph --pretty=oneline --abbrev-commit * 7825a50 merge with no-ff |\ | * 6224937 add merge |/ * 59bc1cb conflict fixed ... 可以看到，不使用Fast forward模式，merge后就像这样： 分支策略在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活； 那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本； 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。 Bug分支软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交，并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？ 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作： $ git stash Saved working directory and index state WIP on dev: 6224937 add merge HEAD is now at 6224937 add merge 现在，用git status查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。 首先确定要在哪个分支上修复bug，假定需要在master分支上修复，就从master创建临时分支： $ git checkout master Switched to branch &apos;master&apos; Your branch is ahead of &apos;origin/master&apos; by 6 commits. $ git checkout -b issue-101 Switched to a new branch &apos;issue-101&apos; 现在修复bug，需要把“Git is free software …”改为“Git is a free software …”，然后提交： $ git add readme.txt $ git commit -m &quot;fix bug 101&quot; [issue-101 cc17032] fix bug 101 1 file changed, 1 insertion(+), 1 deletion(-) 修复完成后，切换到master分支，并完成合并，最后删除issue-101分支： $ git checkout master Switched to branch &apos;master&apos; Your branch is ahead of &apos;origin/master&apos; by 2 commits. $ git merge --no-ff -m &quot;merged bug fix 101&quot; issue-101 Merge made by the &apos;recursive&apos; strategy. readme.txt | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) $ git branch -d issue-101 Deleted branch issue-101 (was cc17032). 太棒了，原计划两个小时的bug修复只花了5分钟！现在，是时候接着回到dev分支干活了！ $ git checkout dev Switched to branch &apos;dev&apos; $ git status # On branch dev nothing to commit (working directory clean) 工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看： $ git stash list stash@{0}: WIP on dev: 6224937 add merge 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法： 一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； 另一种方式是用git stash pop，恢复的同时把stash内容也删了： $ git stash pop # On branch dev # Changes to be committed: # (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) # # new file: hello.py # # Changes not staged for commit: # (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) # (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) # # modified: readme.txt # Dropped refs/stash@{0} (f624f8e5f082f2df2bed8a4e09c12fd2943bdd40) 再用git stash list查看，就看不到任何stash内容了： $ git stash list 你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令： $ git stash apply stash@{0} Feature分支软件开发中，总有无穷无尽的新的功能要不断添加进来。 添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。 现在，你终于接到了一个新任务：开发代号为Vulcan的新功能，该功能计划用于下一代星际飞船。 于是准备开发： $ git checkout -b feature-vulcan Switched to a new branch &apos;feature-vulcan&apos; 5分钟后，开发完毕： $ git add vulcan.py $ git status # On branch feature-vulcan # Changes to be committed: # (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) # # new file: vulcan.py # $ git commit -m &quot;add feature vulcan&quot; [feature-vulcan 756d4af] add feature vulcan 1 file changed, 2 insertions(+) create mode 100644 vulcan.py 切回dev，准备合并： $ git checkout dev 一切顺利的话，feature分支和bug分支是类似的，合并，然后删除。 但是，就在此时，接到上级命令，因经费不足，新功能必须取消！虽然白干了，但是这个分支还是必须就地销毁： $ git branch -d feature-vulcan error: The branch &apos;feature-vulcan&apos; is not fully merged. If you are sure you want to delete it, run &apos;git branch -D feature-vulcan&apos;. 销毁失败。Git友情提醒，feature-vulcan分支还没有被合并，如果删除，将丢失掉修改，如果要强行删除，需要使用命令git branch -D feature-vulcan。 现在我们强行删除： $ git branch -D feature-vulcan Deleted branch feature-vulcan (was 756d4af). 多人协作当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认名称是origin。 要查看远程库的信息，用git remote： $ git remote origin 或者，用git remote -v显示更详细的信息 推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上： $ git push origin master 如果要推送其他分支，比如dev，就改成： $ git push origin dev 当你的小伙伴从远程库clone时，默认情况下，你的小伙伴只能看到本地的master分支。不信可以用git branch命令看看： $ git branch * master * 现在，你的小伙伴要在dev分支上开发，就必须创建远程origin的dev分支到本地，于是他用这个命令创建本地dev分支： $ git checkout -b dev origin/dev 现在，他就可以在dev上继续修改，然后，时不时地把dev分支push到远程 你的小伙伴已经向origin/dev分支推送了他的提交，而碰巧你也对同样的文件作了修改，并试图推送： $ git add hello.py $ git commit -m &quot;add coding: utf-8&quot; [dev bd6ae48] add coding: utf-8 1 file changed, 1 insertion(+) $ git push origin dev To git@github.com:michaelliao/learngit.git ! [rejected] dev -&gt; dev (non-fast-forward) error: failed to push some refs to &apos;git@github.com:michaelliao/learngit.git&apos; hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Merge the remote changes (e.g. &apos;git pull&apos;) hint: before pushing again. hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 推送失败，因为你的小伙伴的最新提交和你试图推送的提交有冲突，解决办法也很简单，Git已经提示我们，先用git pull把最新的提交从origin/dev抓下来，然后，在本地合并，解决冲突，再推送： $ git pull remote: Counting objects: 5, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 3 (delta 0) Unpacking objects: 100% (3/3), done. From github.com:michaelliao/learngit fc38031..291bea8 dev -&gt; origin/dev There is no tracking information for the current branch. Please specify which branch you want to merge with. See git-pull(1) for details git pull &lt;remote&gt; &lt;branch&gt; If you wish to set tracking information for this branch you can do so with: git branch --set-upstream dev origin/&lt;branch&gt; git pull也失败了，原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接： $ git branch --set-upstream dev origin/dev Branch dev set up to track remote branch dev from origin. 再pull： $ git pull Auto-merging hello.py CONFLICT (content): Merge conflict in hello.py Automatic merge failed; fix conflicts and then commit the result. 这回git pull成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的解决冲突完全一样。解决后，提交，再push： $ git commit -m &quot;merge &amp; fix hello.py&quot; [dev adca45d] merge &amp; fix hello.py $ git push origin dev Counting objects: 10, done. Delta compression using up to 4 threads. Compressing objects: 100% (5/5), done. Writing objects: 100% (6/6), 747 bytes, done. Total 6 (delta 0), reused 0 (delta 0) To git@github.com:michaelliao/learngit.git 291bea8..adca45d dev -&gt; dev 因此，多人协作的工作模式通常是这样： 首先，可以试图用git push origin branch-name推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch –set-upstream branch-name origin/branch-name。 这就是多人协作的工作模式，一旦熟悉了，就非常简单。 小结 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 参考 https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000 https://git-scm.com]]></content>
      <categories>
        <category>编程</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ILSVRC2015数据集转VOC2007数据集格式]]></title>
    <url>%2F2018%2F04%2F05%2FILSVRC2015%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%ACVOC2007%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[下载imageNet2015数据集image-net官网下载：image-net.org VOC2007数据格式介绍12345678910111213├── Annotations│ ├── 000001.xml│ └── 000002.xml| |__ ...├── ImageSets│ └── Main│ ├── test.txt│ ├── train.txt│ └── val.txt└── JPEGImages ├── 000001.jpg └── 000002.jpg |__ ... JPEGImages文件夹中包含了PASCAL VOC所提供的所有的图片信息，包括了训练图片和测试图片。 Annotations文件夹中存放的是xml格式的标签文件，每一个xml文件都对应于JPEGImages文件夹中的一张图片。 xml文件的具体格式如下：（对于2007_000392.jpg）: 12345678910111213141516171819202122232425262728293031323334353637383940&lt;annotation&gt; &lt;folder&gt;VOC2012&lt;/folder&gt; &lt;filename&gt;2007_000392.jpg&lt;/filename&gt; //文件名 &lt;source&gt; //图像来源（不重要） &lt;database&gt;The VOC2007 Database&lt;/database&gt; &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt; &lt;image&gt;flickr&lt;/image&gt; &lt;/source&gt; &lt;size&gt; //图像尺寸（长宽以及通道数） &lt;width&gt;500&lt;/width&gt; &lt;height&gt;332&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;1&lt;/segmented&gt; //是否用于分割（在图像物体识别中01无所谓） &lt;object&gt; //检测到的物体 &lt;name&gt;horse&lt;/name&gt; //物体类别 &lt;pose&gt;Right&lt;/pose&gt; //拍摄角度 &lt;truncated&gt;0&lt;/truncated&gt; //是否被截断（0表示完整） &lt;difficult&gt;0&lt;/difficult&gt; //目标是否难以识别（0表示容易识别） &lt;bndbox&gt; //bounding-box（包含左下角和右上角xy坐标） &lt;xmin&gt;100&lt;/xmin&gt; &lt;ymin&gt;96&lt;/ymin&gt; &lt;xmax&gt;355&lt;/xmax&gt; &lt;ymax&gt;324&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; //检测到多个物体 &lt;name&gt;person&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;198&lt;/xmin&gt; &lt;ymin&gt;58&lt;/ymin&gt; &lt;xmax&gt;286&lt;/xmax&gt; &lt;ymax&gt;197&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; ImageSets存放的是每一种类型的challenge对应的图像数据。在ImageSets下有四个文件夹： 其中Action下存放的是人的动作（例如running、jumping等等，这也是VOC challenge的一部分） Layout下存放的是具有人体部位的数据（人的head、hand、feet等等，这也是VOC challenge的一部分） Main下存放的是图像物体识别的数据，总共分为20类。 Segmentation下存放的是可用于分割的数据。 在这里主要考察Main文件夹。 Main文件夹下包含了20个分类的***_train.txt、***_val.txt和***_trainval.txt。 这些txt中的内容都差不多如下： 123456000012000017000023000026000032... 表示图像的name。 ILSVRC2015数据集介绍123456789101112131415161718192021222324252627282930313233343536373839├── Annotations│ └── CLS-LOC│ └── train│ ├── n01440764│ │ ├── n01440764_18.xml│ │ └── n01440764_37.xml| | |__ ...│ └── n01443537│ ├── n01443537_16.xml│ └── n01443537_2.xml| |__ ...├── Data│ └── CLS-LOC│ ├── test│ ├── train│ │ ├── n01440764│ │ │ ├── n01440764_36.JPEG│ │ │ └── n01440764_37.JPEG| | |__ ...│ │ └── n01443537│ │ ├── n01443537_16.JPEG│ │ └── n01443537_2.JPEG| | |__ ...│ └── val├── ImageSets│ └── CLS-LOC│ ├── test.txt│ ├── train_cls.txt│ ├── train_loc.txt│ └── val.txt└── devkit ├── COPYING ├── data │ └── map_clsloc.txt | |__ ... ├── evaluation │ └── VOCreadxml.m | |__ ... └── readme.txt ILSVRC2015_devkit\devkit\data\map_clsloc.txt文件描述了类别对应关系。 转换方法图片JPEG转jpg格式VOC2007中的原图片的数据格式为.jpg格式，而ILSVRC2015数据集中的原始图片为.JPEG格式。考虑到有些深度神经网络只支持.jpg格式的图片。因此，首先将.JPEG格式图片转换为.jpg格式的图片。转换代码可参考如下： 1234567891011121314151617181920212223242526clc;clear all; maindir = '/Users/xiaoqiangteng/Downloads/imageset/JPEGImages1/';subdir = dir( maindir ); % 遍历所有子文件夹 for i = 1 : length( subdir ) if( isequal( subdir( i ).name, '.' ) || ... isequal( subdir( i ).name, '..' ) || ... ~subdir( i ).isdir ) % 过滤空文件夹 continue; end subdirpath = fullfile( maindir, subdir( i ).name, '*.JPEG' ); %subdir( i ).name = 'n00007846'; subdirpath = S:\ImageNet\JPEGImages1\n00007846\*.JPEG; images = dir( subdirpath ); for j = 1 : length( images ) imagepath = fullfile(maindir, subdir( i ).name, images( j ).name); imgdata = imread( imagepath); subdirpath1 = strcat(maindir, subdir( i ).name); subdirpath = strcat(subdirpath1, '/'); jpgPath = [subdirpath, images( j ).name(1:end-5), '.jpg']; imwrite(imgdata, jpgPath, 'mode','lossless'); delete(imagepath, images( j ).name); endend 待解决的问题： 函数imwrite(imgdata, jpgPath);默认参数会改变图片大小。 imwrite(imgdata, jpgPath, ‘mode’,’lossless’);加入特定模式后，图片损坏。 转换XML文件参考这篇http://blog.csdn.net/samylee/article/details/51201744，他是将每个图片的数据写成了一个txt文件，然后用txt转化为xml文件。我模仿这种方法，那么我得先获得txt文件，所以现在的第一步是要将我的imageNet的annotation，即xml文件转化为参考博客所提到的txt文件。 XML文件转txt文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384%createtxt.mclc;clear all; RootPath = '/Users/xiaoqiangteng/Downloads/imageset/';[dir_name, count, label] = importDataFiles(RootPath);path_image = '/Users/xiaoqiangteng/Downloads/imageset/JPEGImages2/';%原始图片文件夹path_xml = '/Users/xiaoqiangteng/Downloads/imageset/Annotations1/';path_label = '/Users/xiaoqiangteng/Downloads/imageset/labels/';%生成的txt文件夹subdir = dir(path_xml);for i = 3 : length( subdir ) if( isequal( subdir( i ).name, '.' ) || ... isequal( subdir( i ).name, '..' ) || ... ~subdir( i ).isdir ) % 过滤空文件夹 continue; end subdirpath = fullfile(path_xml, subdir( i ).name, '*.xml'); xml_files1 = dir( subdirpath ); %XML文件排序 xml_files2 = []; int_xml_files = []; int_xml = []; xml_files = []; length_xml = length(xml_files1); for k = 1:length(xml_files1) xml_files2(k).name = xml_files1(k).name(11:end-4); int_xml_files(k) = str2num(xml_files2(k).name); end int_xml = sort(int_xml_files); for ii = 1:length(xml_files1) xml_files3 = strcat(subdir( i ).name, '_'); xml_files4 = strcat(xml_files3, num2str(int_xml(ii))); xml_files(ii).name = strcat(xml_files4, '.xml'); end mkdir(path_label, subdir( i ).name); pathtxt1 = strcat(path_label, subdir( i ).name); pathtxt2 = strcat(pathtxt1, '/'); % 遍历XML文件 for j = 1 : length(xml_files) disp(j); try pathtxt = [pathtxt2 xml_files( j ).name(1:end-4) '.txt']; subdir_xml = fullfile(path_xml, subdir( i ).name, xml_files( j ).name); str = fileread(subdir_xml); v = xml_parse( str ); xmin = v.object.bndbox.xmin; ymin = v.object.bndbox.ymin; xmax = v.object.bndbox.xmax; ymax = v.object.bndbox.ymax; filename = v.filename; fid = fopen(pathtxt,'wt'); fprintf(fid,'%s%s',filename,'.JPEG'); fprintf(fid,'%c',' '); fprintf(fid,'%s', label&#123;i-2&#125;); fprintf(fid,'%c',' '); fprintf(fid,'%c',xmin); fprintf(fid,'%c',' '); fprintf(fid,'%c',ymin); fprintf(fid,'%c',' '); fprintf(fid,'%c',xmax); fprintf(fid,'%c',' '); fprintf(fid,'%c',ymax); fclose(fid); catch% delete_image1 = strcat(path_image, subdir( i ).name);% delete_image2 = strcat(delete_image1, '/');% delete_image = [delete_image2, xml_files( j ).name(1:end-4), '.JPEG'];% delete(delete_image); delete_xml1 = strcat(path_xml, subdir( i ).name); delete_xml2 = strcat(delete_xml1, '/'); delete_xml = [delete_xml2, xml_files( j ).name(1:end-4), '.xml']; delete(delete_xml); disp('Wrong'); end endend importDataFiles(RootPath)函数： 1234567891011121314function [dir_name, count, label]=importDataFiles(RootPath)DirOutput = dir(fullfile(RootPath));SimpleName = &#123;DirOutput(3:end).name&#125;';LenSimFile = length(SimpleName);for i=1:LenSimFile fileName = fullfile(RootPath,SimpleName&#123;i&#125;); switch SimpleName&#123;i&#125; case 'map_clsloc.txt' [dir_name, count, label] = textread(fileName,'%s%d%s'); end endend 改代码通过遍历xml文件来生成txt文件。原因在于原始图片文件夹内的图片多余对应的xml文件。 可能存在的问题： （1）以上matlab代码通过使用XML 函数来解析XML文件，即xml_parse()函数。需要先下载该函数的工具包，下载地址：https://cn.mathworks.com/matlabcentral/fileexchange/4278-xml-toolbox?focused=5055046&amp;tab=function 但是该工具包在高版本的matlab已不支持，请尝试低版本的Matlab。楼主使用matlab 2014a版本，可运行。 （2）待解决的问题 以上代码仅仅支持XML中存在一个object对象。若存在多个object对象，即会报错，运行catch语句块，将不能够读取的xml文件从原文件夹中删除，以此来保证xml文件的数量同txt文件的数量相同。但是，该问题应该很好能够解决。 TXT转XML接下来就可以进行将txt转化为pascal voc格式的xml文件了，在当前目录下创建一个Annotations的文件夹，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859%writeanno.mclc;clear all; path_image = '/home/teng/programmings/datasets/imagenet/imagenet/JPEGImages/';path_label = '/home/teng/programmings/datasets/imagenet/imagenet/labels/';path_xml = '/home/teng/programmings/datasets/imagenet/imagenet/Annotations/';subdir = dir(path_label);for i = 3 : length( subdir ) if( isequal( subdir( i ).name, '.' ) || ... isequal( subdir( i ).name, '..' ) || ... ~subdir( i ).isdir ) continue; end subdirpath = fullfile(path_label, subdir( i ).name, '*.txt'); txt_files = dir( subdirpath ); mkdir(path_xml, subdir( i ).name); for j = 1:length(txt_files) disp(i, j) path_label_dir1 = strcat(path_label, subdir( i ).name); path_label_dir = strcat(path_label_dir1, '/'); msg = textread(strcat(path_label_dir, txt_files(j).name(1:end-4),'.txt'),'%s'); clear rec; path_xml_subdir1 = strcat(path_xml, subdir( i ).name); path_xml_subdir = strcat(path_xml_subdir1, '/'); path = [path_xml_subdir txt_files(j).name(1:end-4) '.xml']; fid=fopen(path,'w'); rec.annotation.folder = 'VOC2007';%数据集名 rec.annotation.filename = strcat(txt_files(j).name(1:end-4), '.JPEG'); rec.annotation.source.database = 'The VOC2007 Database'; rec.annotation.source.annotation = 'PASCAL VOC2007'; rec.annotation.source.image = 'flickr'; rec.annotation.source.flickrid = '0'; rec.annotation.owner.flickrid = 'I do not know'; rec.annotation.owner.name = 'I do not know'; path_image_subdir1 = strcat(path_image, subdir( i ).name); path_image_subdir = strcat(path_image_subdir1, '/'); img = imread([path_image_subdir txt_files(j).name(1:end-4) '.JPEG']); rec.annotation.size.width = int2str(size(img,2)); rec.annotation.size.height = int2str(size(img,1)); rec.annotation.size.depth = int2str(size(img,3)); rec.annotation.segmented = '0'; rec.annotation.object.name = msg&#123;2&#125;; rec.annotation.object.pose = 'Left'; rec.annotation.object.truncated = '1'; rec.annotation.object.difficult = '0'; rec.annotation.object.bndbox.xmin = msg&#123;3&#125;;%坐标x1 rec.annotation.object.bndbox.ymin = msg&#123;4&#125;;%坐标y1 rec.annotation.object.bndbox.xmax = msg&#123;5&#125;;%坐标x2 rec.annotation.object.bndbox.ymax = msg&#123;6&#125;;%坐标y2 writexml(fid,rec,0); fclose(fid); end end writexml函数： 1234567891011121314151617181920212223242526272829303132333435%writexml.mfunction xml = writexml(fid,rec,depth)fn=fieldnames(rec);for i=1:length(fn) f=rec.(fn&#123;i&#125;); if ~isempty(f) if isstruct(f) for j=1:length(f) fprintf(fid,'%s',repmat(char(9),1,depth)); a=repmat(char(9),1,depth); fprintf(fid,'&lt;%s&gt;\n',fn&#123;i&#125;); writexml(fid,rec.(fn&#123;i&#125;)(j),depth+1); fprintf(fid,'%s',repmat(char(9),1,depth)); fprintf(fid,'&lt;/%s&gt;\n',fn&#123;i&#125;); end else if ~iscell(f) f=&#123;f&#125;; end for j=1:length(f) fprintf(fid,'%s',repmat(char(9),1,depth)); fprintf(fid,'&lt;%s&gt;',fn&#123;i&#125;); if ischar(f&#123;j&#125;) fprintf(fid,'%s',f&#123;j&#125;); elseif isnumeric(f&#123;j&#125;)&amp;&amp;numel(f&#123;j&#125;)==1 fprintf(fid,'%s',num2str(f&#123;j&#125;)); else error('unsupported type'); end fprintf(fid,'&lt;/%s&gt;\n',fn&#123;i&#125;); end end endend 移动目录内所有文件夹内的文件至上层目录12345678910111213141516171819202122232425262728#! /bin/bashfunction read_dir()&#123;for file in `ls $1` #注意此处这是两个反引号，表示运行系统命令doif [ -d $1"/"$file ] #注意此处之间一定要加上空格，否则会报错thenecho $1$fileread_dir $1"/"$fileelsemv $1"/"$file /home/teng/programmings/datasets/imagenet/imagenet/Annotationsfidone&#125; function delete_dir()&#123;for file in `ls $1`doif [ -d $1"/"$file ]thenecho $1$"/"filerm -rf $1"/"$filefidone&#125; #读取第一个参数read_dir $1delete_dir $1 这一步的目的在于将Annotations文件夹内的所有xml文件置于一个目录下。 imageSets文件夹代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445%createimagesets.mclc;clear all; file = dir('/home/teng/programmings/datasets/imagenet/imagenet/Annotations/');len = length(file)-2;num_trainval=sort(randperm(len, floor(9*len/10)));%trainval集占所有数据的9/10，可以根据需要设置num_train=sort(num_trainval(randperm(length(num_trainval), floor(5*length(num_trainval)/6))));%train集占trainval集的5/6，可以根据需要设置num_val=setdiff(num_trainval,num_train);%trainval集剩下的作为val集num_test=setdiff(1:len,num_trainval);%所有数据中剩下的作为test集path = '/home/teng/programmings/datasets/imagenet/imagenet/ImageSets/Main/';fid=fopen(strcat(path, 'trainval.txt'),'a+');for i=1:length(num_trainval) s = sprintf('%s',file(num_trainval(i)+2).name); fprintf(fid,[s(1:length(s)-4) '\r\n']);endfclose(fid);fid=fopen(strcat(path, 'train.txt'),'a+');for i=1:length(num_train) s = sprintf('%s',file(num_train(i)+2).name); fprintf(fid,[s(1:length(s)-4) '\r\n']);endfclose(fid);fid=fopen(strcat(path, 'val.txt'),'a+');for i=1:length(num_val) s = sprintf('%s',file(num_val(i)+2).name); fprintf(fid,[s(1:length(s)-4) '\r\n']);endfclose(fid);fid=fopen(strcat(path, 'test.txt'),'a+');for i=1:length(num_test) s = sprintf('%s',file(num_test(i)+2).name); fprintf(fid,[s(1:length(s)-4) '\r\n']);endfclose(fid); 这样所需的文件夹我们都已备齐，将imageSets，Annotations和JPEGiImage文件夹分别放入voc数据集的对应位置，在这之前先将其原来的文件夹删除。 参考： https://blog.csdn.net/xbcReal/article/details/51259558 https://blog.csdn.net/samylee/article/details/51201744 https://blog.csdn.net/sinat_30071459/article/details/50723212]]></content>
      <categories>
        <category>编程</category>
        <category>深度学习</category>
        <category>数据集制作</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++教程-cmake]]></title>
    <url>%2F2018%2F04%2F03%2FC%2B%2B%E6%95%99%E7%A8%8B-cmake%2F</url>
    <content type="text"><![CDATA[Cmake入门CMake编译原理CMake是一种跨平台编译工具，比make更为高级，使用起来要方便得多。CMake主要是编写CMakeLists.txt文件，然后用cmake命令将CMakeLists.txt文件转化为make所需要的makefile文件，最后用make命令编译源码生成可执行程序或共享库（so(shared object)）。因此CMake的编译基本就两个步骤： 12cmake ..make cmake 指向CMakeLists.txt所在的目录，例如cmake .. 表示CMakeLists.txt在当前目录的上一级目录。cmake后会生成很多编译的中间文件以及makefile文件，所以一般建议新建一个新的目录，专门用来编译，例如: 1234mkdir buildcd buildcmake ..make make根据生成makefile文件，编译程序。 使用Cmake编译程序源码文件介绍本文以一个例子入手介绍，即开平方。 12345├── CMakeLists.txt└── src ├── main.cpp ├── test_math.cpp └── test_math.h 其中，src目录存放所有的源代码，即test_math.cpp、test_math.h和main.cpp。每个源代码文件内容如下： test_math.h: 12345678#ifndef TEST4_TEST_MATH_H#define TEST4_TEST_MATH_H#include &lt;math.h&gt;double cal_sqrt(double value);#endif //TEST4_TEST_MATH_H test_math.cpp: 123456#include "test_math.h"double cal_sqrt(double value)&#123; return sqrt(value);&#125; main.cpp: 12345678910#include &lt;iostream&gt;#include "test_math.h"int main() &#123; double a = 49.0; double b = 0.0; b = cal_sqrt(a); printf("sqrt result:%f\n",b); return 0;&#125; 编写CMakeLists.txtCMakeLists.txt文件，如下所示： 12345678910111213141516171819202122#1.cmake verson，指定cmake版本cmake_minimum_required(VERSION 3.8)#2.project name，指定项目的名称，一般和项目的文件夹名称对应project(test4)#3.head file path，头文件目录#INCLUDE_DIRECTORIES(include)#4.source directory，源文件目录AUX_SOURCE_DIRECTORY(src DIR_SRCS)#5.set environment variable，设置环境变量，编译用到的源文件全部都要放到这里，否则编译能够通过，但是执行的时候会出现各种问题，比如"symbol lookup error xxxxx , undefined symbol"SET(TEST_MATH $&#123;DIR_SRCS&#125;)#7.add link library，添加可执行文件所需要的库，比如我们用到了libm.so（命名规则：lib+name+.so），就添加该库的名称#TARGET_LINK_LIBRARIES($&#123;PROJECT_NAME&#125; m)#6.add executable file，添加要编译的可执行文件ADD_EXECUTABLE($&#123;PROJECT_NAME&#125; $&#123;TEST_MATH&#125;)set(CMAKE_CXX_STANDARD 11) 编译和运行程序由于编译中出现许多中间的文件，因此最好新建一个独立的目录build，在该目录下进行编译，编译步骤如下所示： 1234mkdir buildcd buildcmake ..make build下生成的目录结构如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152├── CMakeLists.txt├── cmake-build-debug│ ├── CMakeCache.txt│ ├── CMakeFiles│ │ ├── 3.8.2│ │ │ ├── CMakeCCompiler.cmake│ │ │ ├── CMakeCXXCompiler.cmake│ │ │ ├── CMakeDetermineCompilerABI_C.bin│ │ │ ├── CMakeDetermineCompilerABI_CXX.bin│ │ │ ├── CMakeSystem.cmake│ │ │ ├── CompilerIdC│ │ │ │ ├── CMakeCCompilerId.c│ │ │ │ ├── a.out│ │ │ │ └── tmp│ │ │ └── CompilerIdCXX│ │ │ ├── CMakeCXXCompilerId.cpp│ │ │ ├── a.out│ │ │ └── tmp│ │ ├── CMakeDirectoryInformation.cmake│ │ ├── CMakeOutput.log│ │ ├── CMakeTmp│ │ ├── Makefile.cmake│ │ ├── Makefile2│ │ ├── TargetDirectories.txt│ │ ├── clion-environment.txt│ │ ├── clion-log.txt│ │ ├── cmake.check_cache│ │ ├── feature_tests.bin│ │ ├── feature_tests.c│ │ ├── feature_tests.cxx│ │ ├── progress.marks│ │ └── test4.dir│ │ ├── CXX.includecache│ │ ├── DependInfo.cmake│ │ ├── build.make│ │ ├── cmake_clean.cmake│ │ ├── depend.internal│ │ ├── depend.make│ │ ├── flags.make│ │ ├── link.txt│ │ ├── progress.make│ │ └── src│ │ ├── main.cpp.o│ │ └── test_math.cpp.o│ ├── Makefile│ ├── cmake_install.cmake│ ├── test4│ └── test4.cbp└── src ├── main.cpp ├── test_math.cpp └── test_math.h 参考 http://www.cnblogs.com/cv-pr/p/6206921.html]]></content>
      <categories>
        <category>编程</category>
        <category>C++</category>
        <category>cmake</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[colmap源码解读]]></title>
    <url>%2F2018%2F04%2F03%2Fcolmap%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[feature_extractor.cc#include 1 auto_ptr C++的auto_ptr所做的事情，就是动态分配对象以及当对象不再需要时自动执行清理。 使用std::auto_ptr，要#include 。 double *p = new double;//为指针分配内存 std::auto_ptr autop(p); //继承性指针，必须依赖上面的指针p //创建智能指针管理指针p指向的内存，可以自动释放内存，不用delete就可以自动删除 //搭配原生指针p使用，不用担心多delete或者少delete //auto_ptr更多用于管理类和对象的内存 2 unique_ptr unique_ptr是一种定义在中的智能指针(smart pointer)。它持有对对象的独有权——两个unique_ptr不能指向一个对象，不能进行复制操作只能进行移动操作。unique_ptr在超出作用域，即以下情况时它指向的对象会被摧毁： unique_ptr指向的对象被破坏 对象通过operator=（）或reset（）被指定到另一个指针） unique_ptr还可能没有对象，这种情况被称为empty。 //C++11新指针 //std::unique_ptr&lt;指针指向的变量数据类型&gt;指针变量名(new 指针指向的变量数据类型); std::unique_ptrpdb(new double);]]></content>
      <categories>
        <category>编程</category>
        <category>colmap</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>colmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++教程-glog功能介绍]]></title>
    <url>%2F2018%2F04%2F03%2FC%2B%2B%E6%95%99%E7%A8%8B-glog%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[概述Google glog是一个基于程序级记录日志信息的c++库，编程使用方式与c++的stream操作类似，例： LOG(INFO) &lt;&lt; &quot;Found &quot; &lt;&lt; num_cookies &lt;&lt; &quot; cookies&quot;; “LOG”宏为日志输出关键字，“INFO”为严重性程度。 主要支持功能： 参数设置，以命令行参数的方式设置标志参数来控制日志记录行为； 严重性分级，根据日志严重性分级记录日志； 可有条件地记录日志信息； 条件中止程序。丰富的条件判定宏，可预设程序终止条件； 异常信号处理。程序异常情况，可自定义异常处理过程； 支持debug功能。可只用于debug模式； 自定义日志信息； 线程安全日志记录方式； 系统级日志记录； google perror风格日志信息； 精简日志字符串信息。 安装最新版本：0.3.1 http://code.google.com/p/google-glog/ 安装只需： 123./configuremakemake install 简单示例: 12345678910#include &lt;iostream&gt;#include "glog/logging.h" // glog 头文件using namespace std;int main(int argc, char** argv) &#123; google::InitGoogleLogging(argv[0]); // 初始化 // FLAGS_log_dir="."; LOG(INFO) &lt;&lt; "hello glog"; // 打印log：“hello glog. 类似于C++ stream。&#125; Makefile: LIB=$(HOME)/install/glog/lib #glog 安装路径 INCLUDE=$(HOME)/install/glog/include test_glog : main.o g++ -o $@ $^ -L$(LIB) -lglog –lpthread #-lpthread 因为glog在多线程中需要一些锁机制。 main.o: main.cpp g++ -c -o $@ $^ -I$(INCLUDE) 说明： glog 默认对log分为4级： INFO, WARNING, ERROR, FATAL. 打印log语句类似于C++中的stream，实际上LOG(INFO) 宏返回的是一个继承自std::ostrstream类的对象。 编译运行上述demo， glog默认会在/tmp/目录下生成log日志文件：test_glog.search-x2.username.log.INFO.20111003-161341.2083 文件名各字段对应含义为： ...log.... 其中： 1）， 其实对应google::InitGoogleLogging(argv[0])；中的argv[0]，即通过改变google::InitGoogleLogging的参数可以修改日志文件的名称。 2），每个级别的日志会输出到不同的文件中。并且高级别日志文件会同样输入到低级别的日志文件中。 即：FATAL的信息会同时记录在INFO，WARNING，ERROR，FATAL日志文件中。默认情况下，glog还会将会将FATAL的日志发送到stderr中。 现在的问题是：log总不能都打印到/tmp/目录下吧。 参数设置： 不同于log4系列的日志系统通过配置文件的方式， glog采用命令的方式来来配置参数。在glog的官方文档里，提到如下两种方式来配置参数（以修改日志目录为例：） 1），gflags： ./your_application –log_dir=. （gflags 我还没有使用过） 2），export 修改环境变量，如下所示：修改GLOG_log_dir为上层目录 3）以上两种方法都需要使用命令行，除此之外，还可以直接在程序中指定（官方文档中没有提到， glog源代码中也不鼓励这么用，但确实是可行的）： 在glog/logging.h 头文件287—350行，有诸如“GLOG_log_dir”等变量的宏定义， 则其GLOG_log_dir实际为FLAGS_log_dir, 因此只需要在程序中设置FLAGS_log_dir的值即可。其他变量类似。取消main.cpp中的注释行“// FLAGS_log_dir=”.”; ” 试试吧 参考 http://blog.51cto.com/mengjh/546766 http://www.cnblogs.com/foreveryl/archive/2011/10/14/2212265.html]]></content>
      <categories>
        <category>编程</category>
        <category>C++</category>
        <category>glog</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-深度学习环境配置-Ubuntu]]></title>
    <url>%2F2018%2F03%2F31%2Flinux-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-Ubuntu%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04 cuda8.0 GPU驱动配置 ubuntu 16.04 64bit 显卡：NVIDIA Tesla k40m + 集成显卡 更换阿里源更换之前要先备份之前的源： 1sudo cp /etc/apt/source.list /etc/apt/source.list.bak 编辑源列表文件: 1sudo vim/etc/apt/sources.list 原来的列表删除，替换： 123456789101112131415161718# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricteddeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 更新： 1sudo apt-get update Ubuntu16.04挂载新硬盘并格式化硬盘查看硬盘: 1sudo fdisk -l 新建分区: 1$ sudo fdisk /dev/sdb 之后进入command状态，大概是这么操作的： 输入 m 查看帮助 输入 p 查看 /dev/sdb 分区的状态 输入 n 创建sdb这块硬盘的分区 选 p primary =&gt;输入 p Partition number =&gt;分一个区所以输入 1 其他的默认回车即可 最后输入 w 保存并退出 Command 状态。 操作示例: 1234567891011121314151617181920212223242526272829303132Command (m for help): n# n创建分区Partition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions)Select (default p): p# p(primary主分区） e(extended拓展分区)Partition number (1-4, default 1): 1# 分区号First sector (2048-83886079, default 2048): # 默认Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-83886079, default 83886079): # 大小，可自定义，保持默认Created a new partition 1 of type 'Linux' and of size 40 GiB.Command (m for help): p# 查看分区情况Disk /dev/sdb: 40 GiB, 42949672960 bytes, 83886080 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xbb6c1792Device Boot Start End Sectors Size Id Type/dev/sdb1 2048 83886079 83884032 40G 83 LinuxCommand (m for help): w# 保存The partition table has been altered.Calling ioctl() to re-read partition table.Syncing disks. 在通过查看命令即可查看，新增的硬盘. 格式化: 1$ sudo mkfs.ext4 /dev/sdb1 # ext4为分区格式 挂载: 12sudo mkdir /home/datasudo mount /dev/sdb1 /home/data 开机自动挂载: 1$ sudo blkid 添加UUID到/etc/fstab 添加UUID=63295b70-daec-4253-b659-821f51200be9 /home/data ext4 defaults,errors=remount-ro 0 1到/etc/fstab 其中UUID后面跟sdb1的UUID 重启。 安装必要的软件1sudo apt-get install vim git openssh-server 检查是否正确识别显卡1lspci | grep -i nvidia 查看是否已有安装的NVIDIA驱动1lsmod | grep nvidia 查看集显驱动1lsmod | grep nouveau 禁用nouveau驱动和相关的驱动包用编辑器打开blacklist.conf配置文件 1sudo gedit /etc/modprobe.d/blacklist.conf 在文件的最后一行加入下面的命令，屏蔽有影响的驱动包（这里有的博客添加了blacklist amd76x_edac，但是经测试后不加也是可以安装成功的） 12345678blacklist rivafbblacklist vga16fbblacklist nouveaublacklist nvidiafbblacklist rivatvoptions nouveau modeset=0alias nouveau offalias lbm-nouveau off 禁用 nouveau 内核模块: 12$echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf$sudo update-initramfs -u 卸载所有安装的nvidia驱动1sudo apt-get remove –purge nvidia* 重启1sudo reboot GPU驱动配置根据GPU型号从相应网站下载驱动，例如使用NVIDIA Tesla M60，从NVIDIA网站选择对应的型号和操作系统，CUDA Toolkit版本，下载驱动文件，如NVIDIA-Linux-x86_64-375.66.run，运行驱动文件，根据提示安装： 安装驱动安装驱动可能需要的依赖(可选): 123456$sudo apt-get update$sudo apt-get install dkms build-essential linux-headers-generic$sudo gedit ~/.bashrc export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH$source ~/.bashrc 进入命令行界面 12Ctrl-Alt+F1 sudo /etc/init.d/lightdm stop #关闭当前图形环境令 关闭桌面服务 1sudo service lightdm stop 给驱动run文件赋予执行权限 1sudo chmod a+x NVIDIA-Linux-x86_64-384.66.run 安装: 注意下面参数 1234sudo ./NVIDIA-Linux-x86_64-384.66.run –no-x-check –no-nouveau-check –no-opengl-files # –no-x-check安装驱动时关闭X服务 # –no-nouveau-check 安装驱动时禁用nouveau # –no-opengl-files 只安装驱动文件，不安装OpenGL文件 注意：安装CUDA时一定使用runfile文件，这样可以进行选择。不再选择安装驱动，以及在弹出xorg.conf时选择NO.不要使用ubuntu设置中附加驱动中驱动 报错： （1）ERROR: Unable to load the kernel module ‘nvidia.ko’. This happens most frequently when this kernel module was built against the wrong or improperly configured kernel sources, with a version of gcc that differs from the one used to build the target kernel, or if a driver such as rivafb, nvidiafb, or nouveau is present and prevents the NVIDIA kernel module from obtaining ownership of the NVIDIA graphics device(s), or no NVIDIA GPU installed in this system is supported by this NVIDIA Linux graphics driver release. 解决方法： 禁用nouveau驱动和相关的驱动 首先，Ctrl+Alt+F1进入命令提示符界面 然后，输入对应的username和passwd进入命令行. 最后，使用指令sudo service lightdm stop 关闭图形界面，再利用cd指令进入下载好的驱动目录 12345678sudo chmod 755 NVIDIA-Linux-x86_64-384.111.run #修改权限（否则没有访问权限，无法进行指令安装）sudo ./NVIDIA-Linux-x86_64-384.111.run –no-x-check –no-nouveau-check –no-opengl-files #安装驱动#–no-x-check 关闭X服务#–no-nouveau-check 禁用nouveau#–no-opengl-files 不安装OpenGL文件#...安装完成后sudo update-initramfs -usudo reboot (2) WARNING: Unable to find a suitable destination to install 32-bit compatibility libraries. Your system may not be set up for 32-bit compatibility. 32-bit compatibility files will not be installed; if you wish to install them, re-run the installation and set a valid directory with the –compat32-libdir option. 解决方法： 运行命令： 1sudo aptitude install ia32-libs 安装cuda注意这里下载的是cuda8.0的runfile（local）文件。 这里是nvidia给出的官方安装指南（遇到问题时可以查阅) 下载完cuda8.0后，执行如下语句，运行runfile文件： 1sudo sh cuda_8.0.27_linux.run 因为驱动之前已经安装，这里就不要选择安装驱动。其余的都直接默认或者选择是即可。 使用： 1sudo gedit /etc/profile 打开“profile”文件，在末尾处添加（注意不要有空格，不然会报错): 12export PATH=/usr/local/cuda-8.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$LD_LIBRARY_PATH 重启电脑： 1sudo reboot 测试cuda的Samples: 123cd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuerysudo make./deviceQuery OpenCV3.1配置安装依赖库12345678910sudo apt-get install build-essential# 必须的，gcc编译环境sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev# 必须的,包括cmake等工具sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev# 可选的，看个人需要，总共5M左右sudo apt-get install libv4l-dev 下载 源码OpenCV 或着用git clone： 1234cd ~/opencv310 # opencv310为自己建的，源码将放在这里git clone https://github.com/Itseez/opencv.gitgit clone https://github.com/Itseez/opencv_contrib.git CMake Opencv源码建立一个编译目录（例如：/build）把cmake后的文件都放在这里边。 123cd ~/opencvmkdir build //建立一个build目录，把cmake的文件都放着里边cd build //进入build目录 cmake时ippicv_linux_20151201.tgz总是不能成功下载，故cmake之前将./downloads/linux-808b791a6eac9ed78d32a7666804320e 文件拷贝至./opencv-3.1.0/3rdparty/ippicv/ 路径下(先执行一次cmake 命令生成文件路径，在将ippicv_linux_20151201.tgz复制进去) 首先，手动下载ippicv 然后开始cmake，这里需要注意几个cmake的参数，比较重要。 1sudo cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local WITH_LIBV4L=ON .. 切记最后’..’两个点之前要加空格！！ 把代码编译成可执行文件这里官方推荐使用多进程编译，推荐七个进程： 123456789101112# 报错：# modules/cudalegacy/src/graphcuts.cpp:120:54: error: # ‘NppiGraphcutState’ has not been declared# typedef NppStatus (*init_func_t)(NppiSize oSize, # NppiGraphcutState** ppState, Npp8u* pDeviceMem);# 这是因为opecv3.0与cuda8.0不兼容导致的。解决办法： # 修改 ～/opencv/modules/cudalegacy/src/graphcuts.cpp文件内容# 将 # #if !defined (HAVE_CUDA) || defined (CUDA_DISABLER) # 改为 # #if !defined (HAVE_CUDA) || defined (CUDA_DISABLER) || (CUDART_VERSION &gt;= 8000) make -j7 # 并行运行七个jobs，这一步也在build目录中进行 安装1234sudo make install如果你要在python下运行opencv库的情况下，那就必须安装安装python-opencvsudo apt-get install python-opencv 配置库文件路径1234/bin/bash -c 'echo "/usr/local/lib" &gt; /etc/ld.so.conf.d/opencv.conf'#或者直接打开/etc/ld.so.conf.d/opencv.conf，添加/usr/local/lib#使配置生效sudo ldconfig(重要) 设置环境变量12345678sudo vim/etc/bash.bashrc #在最后加入以下两行代码PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig export PKG_CONFIG_PATH #使配置生效sudo source /etc/bash.bashrc （该步骤可能会报错找不到命令，原因是source为root命令su（进入root权限） 查看12pkg-config --modversion opencvpkg-config --cflags opencv 运行测试我是用python+opencv的，我这里直接运行opencv自带的python的例子程序，TX1自带摄像头不能用，需要使用外接USB摄像头，插入USB接口即可，无需安装驱动，也无需改动测试代码。 1234cd &lt;opencv3.1.0_dir&gt;/samples/python/python video.pypython edge.pypython facedetect.py colmap配置clone colmap源码到本地： 1git clone https://github.com/colmap/colmap 安装依赖： 1sudo apt-get install cmake build-essential libboost-all-dev libeigen3-dev libsuitesparse-dev libfreeimage-dev libgoogle-glog-dev libgflags-dev libglew-dev qtbase5-dev libqt5opengl5-dev 配置Ceres Solver: 123456789sudo apt-get install libatlas-base-dev libsuitesparse-devgit clone https://ceres-solver.googlesource.com/ceres-solvercd ceres-solvermkdir buildcd buildcmake .. -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFFmakesudo make install# 注：如果该安装包无法下载，请离线下载安装，需翻墙 配置和编译colmap: 123456cd path/to/colmapmkdir buildcd buildcmake ..make -j8sudo make install 运行colmap: 12colmap -hcolmap gui Caffe配置安装依赖： 12345sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libopenblas-dev liblapack-dev libatlas-base-devsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-devsudo apt-get install git cmake build-essential 有一定几率安装失败而导致后续步骤出现问题，所以要确保以上依赖包都已安装成功，验证方法就是重新运行安装命令，如验证 git cmake build-essential是否安装成功共则再次运行以下命令： 1sudo apt-get install git cmake build-essential 安装的路径下 clone ： 1git clone https://github.com/BVLC/caffe.git 进入 caffe ，将 Makefile.config.example 文件复制一份并更名为 Makefile.config ，也可以在 caffe 目录下直接调用以下命令完成复制操作 ： 1sudo cp Makefile.config.example Makefile.config 复制一份的原因是编译 caffe 时需要的是 Makefile.config 文件，而Makefile.config.example 只是caffe 给出的配置文件例子，不能用来编译 caffe。 参考我的Makefile.config： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).# USE_CUDNN := 1# CPU-only switch (uncomment to build without GPU support).# CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you're using OpenCV 3OPENCV_VERSION := 3# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# "sudo apt-get install nvidia-cuda-toolkit" then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 lines for compatibility.CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \ -gencode arch=compute_20,code=sm_21 \ -gencode arch=compute_30,code=sm_30 \ -gencode arch=compute_35,code=sm_35 \ -gencode arch=compute_50,code=sm_50 \ -gencode arch=compute_50,code=compute_50# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlasBLAS := open# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /path/to/your/blas# BLAS_LIB := /path/to/your/blas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.# MATLAB_DIR := /usr/local# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.PYTHON_INCLUDE := /usr/include/python2.7 \ /usr/lib64/python2.7/site-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it's in root.# ANACONDA_HOME := $(HOME)/anaconda# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \ # $(ANACONDA_HOME)/include/python2.7 \ # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \# We need to be able to find libpythonX.X.so or .dylib.PYTHON_LIB := /usr/lib# PYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs)WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)# USE_PKG_CONFIG := 1BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that 'make runtest' will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ 可以开始编译了，在 caffe 目录下执行 ： 123make all -j8sudo make runtest -j8sudo make pycaffe -j8 # 安装 pycaffe notebook 接口环境 darknet配置环境: Ubuntu16.04 + Titan X + Cuda8.0 + OpenCV3.1 + Python2.7 请参考前文。 12345678git clone https://github.com/pjreddie/darknetcd darknet# 配置Makefile# GPU=1# CUDNN=0# OPENCV=1# DEBUG=0make 可能出现的报错： （1）error:/usr/bin/ld: 找不到 -lippicvcollect2: error: ld returned 1 exit status Makefile:82: recipe for target ‘libdarknet.so’ failed 解决方法：找到-lippicv对应的库（libippicv.a），该库位于 安装目录./opencv-3.1.0/3rdparty/ippicv/unpack/ippicv_lnx/lib/intel64文件夹下 ，进入该文件夹下执行 1234567891011121314151617sudo cp sudo cp libippicv.a /usr/local/lib/``` 继续执行make 即可。（2）找不到nvcc解决方法：修改darknet下的Makefile文件，将其中的NVCC=nvcc改为/usr/local/cuda-*/bin/nvcc即安装的cuda版本信息保存 继续执行make 即可。下载权重测试:```bashwget http://pjreddie.com/media/files/yolo.weights ./darknet yolo test cfg/yolo.cfg yolo.weights data/dog.jpg ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg YOLOv2训练自己的数据集（VOC格式） YOLOv2目标检测单目标训练自己数据全过程（自用） 参考 https://www.liaohuqiu.net/cn/posts/ssh-public-key-auto-login/ https://blog.csdn.net/asukasmallriver/article/details/72927860 https://blog.csdn.net/u011440558/article/details/78358447 https://www.mtyun.com/library/how-to-install-caffe-on-centos7 https://blog.csdn.net/qq_28413479/article/details/76377184 https://blog.csdn.net/yhaolpz/article/details/71375762 https://www.jianshu.com/p/10ed332caf07]]></content>
      <categories>
        <category>编程</category>
        <category>Linux</category>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Linux</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-TensorFlow教程]]></title>
    <url>%2F2018%2F03%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-TensorFlow%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[TensorFlow简介TensorFlow是Google开发的一款神经网络的Python外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.TensorFlow 让我们可以先绘制计算结构图, 也可以称是一系列可人机交互的计算操作, 然后把编辑好的Python文件 转换成 更高效的C++, 并在后端进行计算. TensorFlow安装Docker安装Dock安装Docker安装请参考实验室GPU服务器部署教程 Docker 需要用户具有 sudo 权限，为了避免每次命令都输入sudo，可以把用户加入 Docker 用户组，参考：docker docs 1sudo usermod -aG docker $USER 安装NVIDIA-Docker安装完成docker并检查安装正确（能跑出来hello-world）后，如果需要docker容器中有gpu支持，需要再安装NVIDIA-Docker，同样找到并打开该项目的主页： 12NVIDIA/nvidia-docker: Build and run Docker containers leveraging NVIDIA GPUshttps://github.com/NVIDIA/nvidia-docker 可以看到在Quick start小节，根据系统版本执行命令： Ubuntu 14.04/16.04/18.04, Debian Jessie/Stretch： 123456789101112131415161718# If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containersdocker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -fsudo apt-get purge -y nvidia-docker# Add the package repositoriescurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \ sudo apt-key add -distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \ sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get update# Install nvidia-docker2 and reload the Docker daemon configurationsudo apt-get install -y nvidia-docker2sudo pkill -SIGHUP dockerd# Test nvidia-smi with the latest official CUDA imagedocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi CentOS 7 (docker-ce), RHEL 7.4/7.5 (docker-ce), Amazon Linux 1/2: If you are not using the official docker-ce package on CentOS/RHEL, use the next section. 123456789101112131415# If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containersdocker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -fsudo yum remove nvidia-docker# Add the package repositoriesdistribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \ sudo tee /etc/yum.repos.d/nvidia-docker.repo# Install nvidia-docker2 and reload the Docker daemon configurationsudo yum install -y nvidia-docker2sudo pkill -SIGHUP dockerd# Test nvidia-smi with the latest official CUDA imagedocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi If yum reports a conflict on /etc/docker/daemon.json with the docker package, you need to use the next section instead. CentOS 7 (docker), RHEL 7.4/7.5 (docker): 12345678910111213141516171819# If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containersdocker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -fsudo yum remove nvidia-docker# Add the package repositoriesdistribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \ sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo# Install the nvidia runtime hooksudo yum install -y nvidia-container-runtime-hooksudo mkdir -p /usr/libexec/oci/hooks.decho -e '#!/bin/sh\nPATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" exec nvidia-container-runtime-hook "$@"' | \ sudo tee /usr/libexec/oci/hooks.d/nvidiasudo chmod +x /usr/libexec/oci/hooks.d/nvidia# Test nvidia-smi with the latest official CUDA image# You can't use `--runtime=nvidia` with this setup.docker run --rm nvidia/cuda nvidia-smi 上面最后一条命令是检查是否安装成功，安装成功，则会显示关于GPU的信息。 然后在执行下面这句，默认用nvdia-docker替代docker命令： 12echo 'alias docker=nvidia-docker' &gt;&gt; ~/.bashrcbash 下载使用TensorFlow镜像打开dockerhub关于tensorflow的页面： 12tensorflow/tensorflow – Docker Hubhttps://hub.docker.com/r/tensorflow/tensorflow/ 根据需要的版本下载tensorflow镜像并开启tensorflow容器： CPU版本 1docker run -it -p 8888:8888 tensorflow/tensorflow GPU版本 1nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu 如何使用,执行以上命令的结果类似如下： 1234567891011121314$ nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu[I 02:51:21.230 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret[W 02:51:21.242 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.[I 02:51:21.249 NotebookApp] Serving notebooks from local directory: /notebooks[I 02:51:21.249 NotebookApp] 0 active kernels [I 02:51:21.249 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/?token=8f90cc7b9ad6ccc4f36f53f347c7a314220bbcb82dd416ea[I 02:51:21.249 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 02:51:21.249 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=8f90cc7b9ad6ccc4f36f53f347c7a314220bbcb82dd416ea[I 02:51:31.832 NotebookApp] 302 GET / (172.17.0.1) 0.74ms[I 02:51:31.943 NotebookApp] 302 GET /tree? (172.17.0.1) 1.44ms 其中看到有个网址： 1http://localhost:8888/?token=8f90cc7b9ad6ccc4f36f53f347c7a314220bbcb82dd416ea 每个人的网址在token=后面的内容是不一样的，现在我们打开浏览器，输入网址： 1http://localhost:8888/ 输入刚刚token后面的值后,点击第一个1_hello_tensorflow.ipynb，然后可以选择执行所有代码. 常用命令： 1docker image pull library/hello-world 上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。 1docker image ls 抓取成功以后，就可以在本机看到这个 image 文件了。运行这个 image 文件: 1docker container run hello-world docker container run命令会从 image 文件，生成一个正在运行的容器实例。 注意，docker container run命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的docker image pull命令并不是必需的步骤。 有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。 1docker container run -it ubuntu bash 对于那些不会自动终止的容器，必须使用docker container kill 命令手动终止。 1docker container kill [containID] image 文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。 12345# 列出本机正在运行的容器$ docker container ls# 列出本机所有容器，包括终止运行的容器$ docker container ls --all 上面命令的输出结果之中，包括容器的 ID。很多地方都需要提供这个 ID，比如上一节终止容器运行的docker container kill命令。 终止运行的容器文件，依然会占据硬盘空间，可以使用docker container rm命令删除。 1docker container rm [containerID] 运行上面的命令之后，再使用docker container ls –all命令，就会发现被删除的容器文件已经消失了。 创建tensorflow docker容器： 12345docker container run --name [name] -it -p 8888:8888 tensorflow/tensorflow:latest-gpu /bin/bash# [name]-- 容器的名字# -it -- 保留命令行运行# -p 8888:8888 —— 将本地的8888端口和http://localhost:8888/映射# tensorflow/tensorflow:latest-gpu ：默认是tensorflow/tensorflow:latest,指定使用的镜像 启动docker： 1docker start [name] 进入docker: 1docker attach [name] 重命名docker: 1docker rename old_name new_name 如何进入正在执行的 docker containerdocker attach这个是官方提供的一种方法。 测试，首先启动一个container: 12$ docker run -i -t ubuntu bashroot@4556f5ad6067:/# 不要退出，打开另一个终端： 123456$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4556f5ad6067 ubuntu:14.04 "bash" 45 seconds ago Up 43 seconds jolly_ardinghelli$ docker attach 4556f5ad6067root@4556f5ad6067:/# 这样就连接进去了。这时候如果我们输入一些命令，就能看到在两个终端都有显示和输出。这种方式有比较大的局限性，如果知道了entrypoint或者有程序正在执行，通过docker attach进入之后是不能执行操作的，一个终端退出之后整个container就终止了。不推荐使用这种方式。 lxc-attach如果使用这种方式，首先要保证docker是以lxc方式启动的，具体可以这样做： 修改/etc/default/docker增加DOCKER_OPTS=”-e lxc” 重启docker服务sudo service docker restart 启动container的方式和之前一样： 12$ docker run -i -t ubuntu bashroot@e7f01f0ff598:/# 进入container可以这样： 1234567891011$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe7f01f0ff598 ubuntu:14.04 "bash" 17 seconds ago Up 15 seconds grave_jones$ ps aux | grep e7f01f0ff598root 23691 0.0 0.0 43140 1876 pts/9 Ss 21:47 0:00 lxc-start -n e7f01f0ff598c80d70a996135c98fbaeddc6daa61436bbbfa735233e8b6f8ebe -f /var/lib/docker/containers/e7f01f0ff598c80d70a996135c98fbaeddc6daa61436bbbfa735233e8b6f8ebe/config.lxc -- /.dockerinit -g 172.17.42.1 -i 172.17.0.3/16 -mtu 1500 -- bashma6174 23756 0.0 0.0 13428 928 pts/12 S+ 21:47 0:00 grep --color=auto e7f01f0ff598$ sudo lxc-attach -n e7f01f0ff598c80d70a996135c98fbaeddc6daa61436bbbfa735233e8b6f8eberoot@e7f01f0ff598:/# 这种方式还是很方便的。前提是需要重启docker服务以lxc的方式执行，进入container之后会有一个终端可以执行命令，不影响正在执行的程序。 nsenter如果docker不是以lxc方式启动的，这时候还想进入一个正在执行的container的话，可以考虑使用nsenter 这个程序的安装方式很独特，使用docker进行安装： 1$ docker run --rm -v /usr/local/bin:/target jpetazzo/nsenter 使用方法也很简单，首先你要进入的container的PID： 1$ PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; &lt;container_name_or_ID&gt;) 然后就可以用这个命令进入container了： 1$ nsenter --target $PID --mount --uts --ipc --net --pid 为了使用方便可以写一个脚本自动完成： 123$ cat /bin/docker_enter#!/bin/bashsudo nsenter --target `docker inspect --format &#123;&#123;.State.Pid&#125;&#125; $1` --mount --uts --ipc --net --pid bash 这样每次要进入某个container只需要执行docker_enter &lt;container_name_or_ID&gt;就可以了。 ssh这个原理也很简单，在container里面启动ssh服务，然后通过ssh的方式去登陆到container里面，不推荐这种方式，主要是配置ssh登陆比较繁琐，开启ssh服务也会耗费资源，完全没有必要。 TensorFlow安装方式一 下载镜像 1docker pull tensorflow/tensorflow 创建Tensorflow容器 123456docker run --name my-tensorflow -it -p 8888:8888 -v ~/tensorflow:/test/data tensorflow/tensorflow# --name：创建的容器名，即my-tensorflow# -it：保留命令行运行# p 8888:8888：将本地的8888端口和http://localhost:8888/映射# -v ~/tensorflow:/test/data:将本地的~/tensorflow挂载到容器内的/# test/data下# tensorflow/tensorflow ：默认是tensorflow/tensorflow:latest,指定使用的镜像 拷贝带token的URL在浏览器打开 1http://[all ip addresses on your system]:8888/?token=649d7cab1734e01db75b6c2b476ea87aa0b24dde56662a27 显示Jupyter Notebook，Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本。示例中已经显示了Tensorflow的入门教程，点开一个可以看见。 关闭容器 1docker stop my-tensortflow 再次打开 1docker start my-tensortflow TensorFlow安装方式二 下载镜像 1docker pull tensorflow/tensorflow 创建Tensorflow容器 12docker run -it --name bash_tensorflow tensorflow/tensorflow /bin/bash# 这样我们就创建了名为bash_tensorflow的容器 start命令启动容器 1docker start bash_tensorflow 再连接上容器 12docker attach bash_tensorflow# 可以看到我们用终端连接上了容器，和操作Linux一样了。 Pip安装Linux 和 MacOS123456# Ubuntu/Linux 64-位 系统的执行代码:$ sudo apt-get install python-pip python-dev# Mac OS X 系统的执行代码:$ sudo easy_install --upgrade pip$ sudo easy_install --upgrade six CPU 版 12345# python 2+ 的用户:$ pip install tensorflow# python 3+ 的用户:$ pip3 install tensorflow GPU 版 1234567$ sudo apt-get install libcupti-dev$ sudo apt-get install python-pip python-dev # for Python 2.7$ sudo apt-get install python3-pip python3-dev # for Python 3.n$ pip install tensorflow # Python 2.7; CPU support (no GPU support)$ pip3 install tensorflow # Python 3.n; CPU support (no GPU support)$ pip install tensorflow-gpu # Python 2.7; GPU support$ pip3 install tensorflow-gpu # Python 3.n; GPU support 测试 1import tensorflow TensorFlow 教程Session 会话控制参考： https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-3-session/ Session 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分. 例子讲解：建立两个 matrix ,输出两个 matrix 矩阵相乘的结果。 12345678import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2) 因为 product 不是直接计算的步骤, 所以我们会要使用 Session 来激活 product 并得到计算结果. 有两种形式使用会话控制 Session 。 123456789101112# method 1sess = tf.Session()result = sess.run(product)print(result)sess.close()# [[12]]# method 2with tf.Session() as sess: result2 = sess.run(product) print(result2)# [[12]] Variable 变量参考： https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-4-variable/ 在 Tensorflow 中，定义了某字符串是变量，它才是变量。定义语法： state = tf.Variable() 123456789101112import tensorflow as tfstate = tf.Variable(0, name='counter')# 定义常量 oneone = tf.constant(1)# 定义加法步骤 (注: 此步并没有直接计算)new_value = tf.add(state, one)# 将 State 更新成 new_valueupdate = tf.assign(state, new_value) 如果你在 Tensorflow 中设定了变量，那么初始化变量是最重要的！！所以定义了变量以后, 一定要定义 init = tf.initialize_all_variables() . 到这里变量还是没有被激活，需要再在 sess 里, sess.run(init) , 激活 init 这一步. 12345678910# 如果定义 Variable, 就一定要 initialize# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 # 使用 Sessionwith tf.Session() as sess: sess.run(init) for _ in range(3): sess.run(update) print(sess.run(state)) 注意：直接 print(state) 不起作用！！ 一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！ Placeholder 传入值参考： https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-5-placeholde/ placeholder 是 Tensorflow 中的占位符，暂时储存变量. Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(*, feed_dict={input: }). 示例： 12345678import tensorflow as tf#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# mul = multiply 是将input1和input2 做乘法运算，并输出为 output ouput = tf.multiply(input1, input2) 接下来, 传值的工作交给了 sess.run() , 需要传入的值放在了feed_dict={} 并一一对应每一个 input. placeholder 与 feed_dict={} 是绑定在一起出现的。 123with tf.Session() as sess: print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))# [ 14.] 建造神经网络参考： https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-2-create-NN/ add_layer 功能1234567891011import tensorflow as tfimport numpy as npdef add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 导入数据构建所需的数据。 这里的x_data和y_data并不是严格的一元二次函数的关系，因为我们多加了一个noise,这样看起来会更像真实情况。 123x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)y_data = np.square(x_data) - 0.5 + noise 利用占位符定义我们所需的神经网络的输入。 tf.placeholder()就是代表占位符，这里的None代表无论输入有多少都可以，因为输入只有一个特征，所以这里是1。 12xs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1]) 接下来，我们就可以开始定义神经层了。 通常神经层都包括输入层、隐藏层和输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有10个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 所以，我们构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。 搭建网络利用之前的add_layer()函数，这里使用 Tensorflow 自带的激励函数tf.nn.relu。 1l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) 接着，定义输出层。此时的输入就是隐藏层的输出——l1，输入有10层（隐藏层的输出层），输出有1层。 1prediction = add_layer(l1, 10, 1, activation_function=None) 计算预测值prediction和真实值的误差，对二者差的平方求和再取平均。 12loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) 接下来，是很关键的一步，如何让机器学习提升它的准确率。tf.train.GradientDescentOptimizer()中的值通常都小于1，这里取的是0.1，代表以0.1的效率来最小化误差loss。 1train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) 使用变量时，都要对它进行初始化，这是必不可少的。 12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 定义Session，并用 Session 来执行 init 初始化步骤。 （注意：在tensorflow中，只有session.run()才会执行我们定义的运算。） 12sess = tf.Session()sess.run(init) 训练机器学习的内容是train_step, 用 Session 来 run 每一次 training 的数据，逐步提升神经网络的预测准确性。 (注意：当运算要用到placeholder时，就需要feed_dict这个字典来指定输入。) 123for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) 每50步我们输出一下机器学习的误差。 123if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;)) 12345678910111213141516171819200.0212046190.0099806760.0071747210.0066330120.006229750.0058940370.0056211460.00538017370.005199970.0050501110.0049220690.00480957050.00471409270.00462343170.00453349580.00445049630.0043783090.00432568460.00428021560.0042369063]]></content>
      <categories>
        <category>编程</category>
        <category>深度学习</category>
        <category>TF</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>深度学习</tag>
        <tag>TF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验室GPU服务器部署教程]]></title>
    <url>%2F2018%2F03%2F29%2F%E5%AE%9E%E9%AA%8C%E5%AE%A4GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Docker 安装参考：https://docs.docker.com/install/linux/docker-ce/ubuntu/#set-up-the-repository Prerequisites 12345To install Docker CE, you need the 64-bit version of one of these Ubuntu versions:Artful 17.10 (Docker CE 17.11 Edge and higher only)Xenial 16.04 (LTS)Trusty 14.04 (LTS) Uninstall old versions 1sudo apt-get remove docker docker-engine docker.io Install Docker CE 1234567891011121314151617181920$ sudo apt-get update# Install packages to allow apt to use a repository over HTTPS$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common# Add Docker’s official GPG key:$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo apt-key fingerprint 0EBFCD88# Use the following command to set up the stable repository.$ sudo add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable"$ sudo apt-get update# Install the latest version of Docker CE$ sudo apt-get install docker-ce# Verify that Docker CE is installed correctly by running the hello-world image.$ sudo docker run hello-world UPGRADE DOCKER CE 1sudo apt-get update Uninstall Docker CE 12$ sudo apt-get purge docker-ce$ sudo rm -rf /var/lib/docker Docker 备份、恢复和迁移备份首先，为了备份Docker中的容器，我们会想看看我们想要备份的容器列表。要达成该目的，我们需要在我们运行着Docker引擎，并已创建了容器的Linux机器中运行 docker ps 命令。 1# docker ps 在此之后，我们要选择我们想要备份的容器，然后去创建该容器的快照。我们可以使用 docker commit 命令来创建快照。 1# docker commit -p 30b8f18f20b4 container-backup 该命令会生成一个作为Docker镜像的容器快照，我们可以通过运行 docker images 命令来查看Docker镜像，如下。 1# docker images 正如我们所看见的，上面做的快照已经作为Docker镜像保存了。现在，为了备份该快照，我们有两个选择，一个是我们可以登录进Docker注册中心，并推送该镜像；另一个是我们可以将Docker镜像打包成tar包备份，以供今后使用。 如果我们想要在Docker注册中心上传或备份镜像，我们只需要运行 docker login 命令来登录进Docker注册中心，然后推送所需的镜像即可。 123# docker login# docker tag a25ddfec4d2a arunpyasi/container-backup:test# docker push arunpyasi/container-backup 如果我们不想备份到docker注册中心，而是想要将此镜像保存在本地机器中，以供日后使用，那么我们可以将其作为tar包备份。要完成该操作，我们需要运行以下 docker save 命令。 1# docker save -o ~/container-backup.tar container-backup 要验证tar包是否已经生成，我们只需要在保存tar包的目录中运行 ls 命令即可。 恢复容器接下来，在我们成功备份了我们的Docker容器后，我们现在来恢复这些制作了Docker镜像快照的容器。如果我们已经在注册中心推送了这些Docker镜像，那么我们仅仅需要把那个Docker镜像拖回并直接运行即可。 1# docker pull arunpyasi/container-backup:test 但是，如果我们将这些Docker镜像作为tar包文件备份到了本地，那么我们只要使用 docker load 命令，后面加上tar包的备份路径，就可以加载该Docker镜像了。 1# docker load -i ~/container-backup.tar 现在，为了确保这些Docker镜像已经加载成功，我们来运行 docker images 命令。 1# docker images 在镜像被加载后，我们将用加载的镜像去运行Docker容器。 1# docker run -d -p 80:80 container-backup 迁移Docker容器迁移容器同时涉及到了上面两个操作，备份和恢复。我们可以将任何一个Docker容器从一台机器迁移到另一台机器。在迁移过程中，首先我们将把容器备份为Docker镜像快照。然后，该Docker镜像或者是被推送到了Docker注册中心，或者被作为tar包文件保存到了本地。如果我们将镜像推送到了Docker注册中心，我们简单地从任何我们想要的机器上使用 docker run 命令来恢复并运行该容器。但是，如果我们将镜像打包成tar包备份到了本地，我们只需要拷贝或移动该镜像到我们想要的机器上，加载该镜像并运行需要的容器即可。 Docker SSH 访问假设我们已经pull了一个docker 镜像，如下图所示的tensorflow/tensorflow。 启动容器123456docker run --name my-tensorflow -it -p 8888:8888 -v ~/tensorflow:/test/data tensorflow/tensorflow# --name：创建的容器名，即my-tensorflow# -it：保留命令行运行# p 8888:8888：将本地的8888端口和http://localhost:8888/映射# -v ~/tensorflow:/test/data:将本地的~/tensorflow挂载到容器内的/# test/data下# tensorflow/tensorflow ：默认是tensorflow/tensorflow:latest,指定使用的镜像 如： 1234docker run -it --name tf tensorflow/tensorflow /bin/bash# 这样我们就创建了名为tf的容器docker start tfdocker attach tf 修改容器的root密码1234apt-get install vim -yapt-get install openssh-server -yapt-get install passwdpasswd root 修改ssh配置123vim /etc/ssh/sshd_config# 修改PermitRootLogin yes UsePAM no 启动ssh服务1service ssh start 退出容器1exit 提交容器成为新的镜像1例如叫做ubuntu-ssh，输入docker commit 容器ID ubuntu-ssh 启动这个镜像的容器，并映射本地的一个闲置的端口1docker run -it -p 50001:22 tf-ssh /bin/bash ssh登录1ssh root@127.0.0.1 -p 50001 Docker后台运行阿里云加速器设置由于官方Docker Hub网络速度较慢，这里使用阿里云提供的Docker Hub. 需要配置阿里云加速器，官方说明如下： 针对Docker客户端版本大于1.10的用户： 123456789# 您可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器$ sudo mkdir -p /etc/docker$ sudo tee /etc/docker/daemon.json &lt;&lt;-‘EOF’ &#123; “registry-mirrors”: [“https://fird1mfg.mirror.aliyuncs.com“] &#125; EOF$ sudo systemctl daemon-reload$ sudo systemctl restart docker 针对Docker客户的版本小于等于1.10的用户或者想配置启动参数，可以使用下面的命令将配置添加到docker daemon的启动参数中. 123456789101112# Ubuntu 12.04 14.04的用户:$ echo “DOCKER_OPTS=/”$DOCKER_OPTS –registry-mirror=https://fird1mfg.mirror.aliyuncs.com/”” | sudo tee -a /etc/default/docker$ sudo service docker restart# Ubuntu 15.04 16.04的用户$ sudo mkdir -p /etc/systemd/system/docker.service.d$ sudo tee /etc/systemd/system/docker.service.d/mirror.conf &lt;&lt;-‘EOF’ [Service] ExecStart=/usr/bin/docker daemon -H fd:// –registry-mirror=https://fird1mfg.mirror.aliyuncs.com EOF$ sudo systemctl daemon-reload$ sudo systemctl restart docker NVIDIA-Docker安装 Prerequisties 1234GNU/Linux x86_64 with kernel version &gt; 3.10 Docker &gt;= 1.9 (official docker-engine, docker-ce or docker-ee only) NVIDIA GPU with Architecture &gt; Fermi (2.1) NVIDIA drivers &gt;= 340.29 with binary nvidia-modprobe (驱动版本与CUDA计算能力相关) CUDA与NVIDIA driver安装 cuda 12处理NVIDIA-Docker依赖项 NVIDIA drivers &gt;= 340.29 with binary nvidia-modprobe 要求. 根据显卡，下载对应版本的CUDA并进行安装. NVIDIA-Docker安装 1234567#Install nvidia-docker and nvidia-docker-pluginwget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.debsudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb#Test nvidia-smisudo nvidia-docker run –rm nvidia/cuda nvidia-smi 默认用nvdia-docker替代docker命令： 12echo 'alias docker=nvidia-docker' &gt;&gt; ~/.bashrcbash 参考 https://jingyan.baidu.com/article/a3aad71aa180e7b1fa009676.html https://github.com/ufoym/deepo#Installation https://hub.docker.com/r/ufoym/deepo/ https://github.com/fatedier/frp/blob/master/README_zh.md#frp-%E7%9A%84%E4%BD%9C%E7%94%A8 https://ranpox.github.io/2018/01/14/notification-of-gpu-server/]]></content>
      <categories>
        <category>编程</category>
        <category>深度学习</category>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>环境配置</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多线程教程]]></title>
    <url>%2F2018%2F03%2F26%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么多线程多线程是加速程序计算的有效方式 添加线程 Thread 导入模块 1import threading 获取已激活的线程数 1threading.active_count() 查看所有线程信息 1threading.enumerate() 查看现在正在运行的线程 1threading.current_thread() 添加线程 threading.Thread()接收参数target代表这个线程要完成的任务，需自行定义 123456789def thread_job(): print('This is a thread of %s' % threading.current_thread())def main(): thread = threading.Thread(target=thread_job,) # 定义线程 thread.start() # 让线程开始工作 if __name__ == '__main__': main() join 功能123456789101112import threadingimport timedef thread_job(): print("T1 start\n") for i in range(10): time.sleep(0.1) # 任务间隔0.1s print("T1 finish\n")added_thread = threading.Thread(target=thread_job, name='T1')added_thread.start()print("all done\n") 12345678910111213thread_1.start() # start T1thread_2.start() # start T2thread_2.join() # join for T2thread_1.join() # join for T1print("all done\n")"""T1 startT2 startT2 finishT1 finishall done""" 储存进程结果 Queue 导入线程,队列的标准模块 123import threadingimport timefrom queue import Queue 定义一个被多线程调用的函数 1234def job(l,q): for i in range (len(l)): l[i] = l[i]**2 q.put(l) #多线程调用的函数不能用return返回值 完整的代码: 123456789101112131415161718192021222324252627import threadingimport timefrom queue import Queuedef job(l,q): for i in range (len(l)): l[i] = l[i]**2 q.put(l)def multithreading(): q =Queue() threads = [] data = [[1,2,3],[3,4,5],[4,4,4],[5,5,5]] for i in range(4): t = threading.Thread(target=job,args=(data[i],q)) t.start() threads.append(t) for thread in threads: thread.join() results = [] for _ in range(4): results.append(q.get()) print(results)if __name___=='__main__': multithreading() 线程锁 Lock 不使用 Lock 的情况 1234567891011121314151617181920212223import threadingdef job1(): global A for i in range(10): A+=1 print('job1',A)def job2(): global A for i in range(10): A+=10 print('job2',A)if __name__== '__main__': lock=threading.Lock() A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 运行结果: 1234567891011121314151617181920job1job2 11job2 21job2 31job2 41job2 51job2 61job2 71job2 81job2 91job2 101 1job1 102job1 103job1 104job1 105job1 106job1 107job1 108job1 109job1 110 使用 Lock 的情况 123456789101112131415161718192021222324252627import threadingdef job1(): global A,lock lock.acquire() for i in range(10): A+=1 print('job1',A) lock.release()def job2(): global A,lock lock.acquire() for i in range(10): A+=10 print('job2',A) lock.release()if __name__== '__main__': lock=threading.Lock() A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 运行结果: 1234567891011121314151617181920job1 1job1 2job1 3job1 4job1 5job1 6job1 7job1 8job1 9job1 10job2 20job2 30job2 40job2 50job2 60job2 70job2 80job2 90job2 100job2 110 参考 https://morvanzhou.github.io/tutorials/python-basic/threading/]]></content>
      <categories>
        <category>编程</category>
        <category>Python</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[semantic_mapping_论文阅读]]></title>
    <url>%2F2018%2F03%2F26%2Fsemantic-mapping-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[Semantic Mapping 文献综述 1，CNN-SLAM为今年CVPR的文章，是比较完整的pipeline，将LSD-SLAM里的深度估计和图像匹配都替换成基于CNN的方法，取得了更为robust的结果，并可以融合语义信息。见ProjectCNNSLAM.类似的工作还有UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning。 问题在于准确度非常低。做过benchmark，基于单帧彩色照片进行距离信息预测，在室内每个像素的平均误差约50cm，在室外平均误差则高达７米以上。一篇投ICRA的工作结合少量的距离信息和彩色信息进行距离图像预测，效果比单纯用彩色照片准确得多且鲁棒性强。这个方法可以帮助传统SLAM从稀疏点云快速生成密集的点云，也可以用在激光雷达的超分辨率上。代码已开源sparse-to-dense,论文：Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image VINet是AAAI2017的文章，利用CNN和RNN构建了一个VIO，即输入image和IMU信息，直接输出估计的pose。 Unsupervised learning of depth and ego-motion from video是Google CVPR 2017的oral文章，利用CNN学习一个无监督的深度估计和pose估计网络，代码见SfMLearner.SfM-Net利用监督学习也干了类似的工作。 重定位PoseNet和Delving deeper into convolutional neural networks for camera relocalization 语义地图 Semi-Dense 3D Semantic Mapping from Monocular SLAM 哪怕在传统基于特征点的SLAM已经能做到非常稳定高效的这个情况下，适合结合deep learning的科研方向还是有很多的。这其中包括 提高特征点稳定性（减少outlier）和自动提取不同层级的特征点（点、线、面、物体）， 快速生成密集的地图（而非稀疏的三维点云） 结合语义信息和图像分割 生成动态地图（可以实时更新、表达动态物体） 降低SLAM调参的难度 7.跳出SLAM，说点题外话，利用深度强化学习来进行端对端的机器人导航，已经有了不错的结果。人类在环境中导航，不也是直接输入image，输出action吗？有兴趣的可以看看这两篇文章：Cognitive Mapping and Planning for Visual Navigation和Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning。navigation 是更适合 DL 的一个场景。人在移动的时候，并不会建立精确的环境地图，无法具体说出障碍物距离自己多少厘米。所以，我一直有一个直觉：「navigation 应该不需要精确地图信息与定位信息」，而 DL 似乎有可能实现这一全新的方法。 navigation 是更适合 DL 的一个场景。人在移动的时候，并不会建立精确的环境地图，无法具体说出障碍物距离自己多少厘米。所以，一直有一个直觉：「navigation 应该不需要精确地图信息与定位信息」，而 DL 似乎有可能实现这一全新的方法。但是，目前所有这些工作都存在一个问题：只是训练出一个 local planner，无法实现全局的路径规划。 语义分割和SLAM的结合还很粗糙.最简单的方式，就是跑一个pixel-wise的图像语义分割，再跑一个dense或者semi-dense的SLAM，把前者的结果map到后者的地图上去，每个像素（或者surfel）上做recursive Bayesian update，其实也就是概率累乘。参见Andrew Davison组的SemanticFusion (ICRA’17)，代码已开源。国内学者也有类似的工作，用LSD-SLAM + DeepLab v2，这个是单目的（SemanticFusion是RGB-D）,即Semi-Dense 3D Semantic Mapping from Monocular SLAM。这种结合方式，按某些学者的意见都不能称为semantic SLAM，只能叫semantic mapping，因为localization部分跟semantics没关系嘛。 真正的semantic SLAM，语义信息是要能够帮助定位的，比如这篇：Probabilistic Data Association for Semantic SLAM(ICRA’17)。用object detection的结果作为SLAM前端的输入，跟ORB之类的特征互补提高定位鲁棒性。优点很明显，这下SLAM不会因为你把床收拾了一下就啥都不认识了（视觉特征都变了，但床还是床）。难点是detection结果的data association最好能跟定位联合优化，但前者是个离散问题。这篇文章用EM算法，E步考虑所有可能的association，比较粗暴，但识别物体较少的时候还不错（论文实验里只识别椅子）。上面这篇文章没有语义分割。。。map里只有稀稀拉拉的几个物体（位置和类别）。 另外，SLAM也能提升语义理解水平。前面提到的SemanticFusion和类似的工作里，融合了多个视角语义理解结果的3D地图，其中的语义标签准确率高于单帧图像得到的结果，这很容易理解。另外，通过在3D空间引入一些先验信息，比如用CRF对地图做一下diffusion，能进一步提升准确率。但CRF毕竟还是简单粗暴，如果设计更精细的滤波算法，尤其是能从真实数据中学习一些先验的话，应该效果还会更好。这方面的工作还没有。 再提一个，融合优化之后的结果如果反馈给图像语义理解算法做一下fine-tuning，那就是self-supervised learning了。这方面的工作也还没有。 接下来是语义地图怎么用的问题。对上层应用有价值的语义地图，应该包含一个个物体及其模型，而不仅是一堆标记了类别的voxel。一个比较好的例子来自IROS’17：Meaningful Maps With Object-Oriented Semantic Mapping不过这篇文章里的语义信息来自SSD和非神经网络的分割，还没有用端到端的语义分割网络。 另外，针对动态场景，怎样处理物体移位，怎样区别长效地图和短效地图，怎么“脑补”同类物体，这里面一堆问题可以研究。更别说地图构建出来之后如何做体现空间智能的自然语言交互和任务规划，如何reasoning了，这方面研究目前连影子都没有。 一篇很好的综述，里面也有很多关于语义SLAM的介绍：Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age 语义slam开源： DA-RNN_Semantic Mapping with Data Associated SemanticFusion Pop-up SLAM: Semantic Monocular Plane SLAM.场景理解用于改善状态估计，尤其是在低纹理区域，是目前极少的开源语义SLAM方案之一 参考： 当前深度学习和slam结合有哪些比较好的论文，有没有一些开源的代码? 当前语义分割帮助视觉SLAM提高定位准确度，建立语义地图的研究现状如何？ 研读论文一：CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction Tateno, K., Tombari, F., Laina, I., &amp; Navab, N. (2017). CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction. arXiv preprint arXiv:1704.03489.CVPR 2017 Motivation 本文提出了一种基于深度神经网络方法能够从单目图像中预测深度信息，用途可为弹幕图像的重建。尤其是对于纯单目重建是小的区域，如纹理不丰富的区域，效果较好。此外，相比较于单目SLAM，该深度SLAM还可提供scale。此外，该方法还融合了语义标记，来重建语义信息的场景。 传统的基于深度摄像头的SLAM有如下缺点： 有效工作距离较短 太阳光的影响等 普适性 双目摄像头缺点：特征丰富程度敏感 鉴于卷积神经网络（CNN）深度预测的最新进展，本文研究了深度神经网络的预测深度图，可以部署用于精确和密集的单目重建。我们提出了一种方法，其中CNN预测的稠密深度图与通过直接单目SLAM获得的深度测量自然地融合在一起。我们的融合方案在图像定位中优于单目SLAM方法，例如沿低纹理区域，反之亦然。我们展示了使用深度预测来估计重建的绝对尺度，从而克服了单眼SLAM的主要局限性之一。最后，我们提出一个框架，从单个帧获得的语义标签有效地融合了密集的SLAM，从单个视图产生语义相干的场景重构。两个基准数据集的评估结果显示了我们的方法的鲁棒性和准确性]]></content>
      <categories>
        <category>学术</category>
        <category>论文阅读</category>
        <category>Semantic Mapping</category>
      </categories>
      <tags>
        <tag>学术</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy和pandas教程]]></title>
    <url>%2F2018%2F03%2F26%2Fnumpy%E5%92%8Cpandas%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[安装 numpy和pandasnumpy 和 pandas 12sudo pip install numpysudo pip install pandas Numpy教程Numpy 属性 ndim：维度 shape：行数和列数 size：元素个数 Numpy 的创建 array array：创建数组 dtype：指定数据类型 zeros：创建数据全为0 ones：创建数据全为1 empty：创建数据接近0 arrange：按指定范围创建数据 linspace：创建线段 Numpy 基础运算123import numpy as npa=np.array([10,20,30,40]) # array([10, 20, 30, 40])b=np.arange(4) # array([0, 1, 2, 3]) 减法 1c=a-b # array([10, 19, 28, 37]) 加法 1c=a+b # array([10, 21, 32, 43]) 乘法 1c=a*b # array([ 0, 20, 60, 120]) 有所不同的是，在Numpy中，想要求出矩阵中各个元素的乘方需要依赖双星符号 ** 乘法 1c=b**2 # array([0, 1, 4, 9]) 三角函数 12c=10*np.sin(a) # cos tan# array([-5.44021111, 9.12945251, -9.88031624, 7.4511316 ]) 逻辑判断 12print(b&lt;3) # array([ True, True, True, False], dtype=bool) 上述运算均是建立在一维矩阵，即只有一行的矩阵上面的计算，如果我们想要对多行多维度的矩阵进行操作，需要对开始的脚本进行一些修改： 12345678910a=np.array([[1,1],[0,1]])b=np.arange(4).reshape((2,2))print(a)# array([[1, 1],# [0, 1]])print(b)# array([[0, 1],# [2, 3]]) 此时构造出来的矩阵a和b便是2行2列的，其中 reshape 操作是对矩阵的形状进行重构， 其重构的形状便是括号中给出的数字。 矩阵乘法运算 123456c_dot = np.dot(a,b)# array([[2, 4],# [2, 3]])c_dot_2 = a.dot(b)# array([[2, 4],# [2, 3]]) 特定运算 求和：np.sum(A) 最小值：np.min(A) 最大值：np.max(A) 最小值索引：np.argmin(A) 最大值索引：np.argmax(A) 平均值：np.mean(A) 中位数：np.medium(A) 累加：np.cumsum(A) 累差：np.diff(A) 非零数：np.nonzero(A) 排序：np.sort(A) 矩阵反向（转置）：np.transpose(A)或A.T 截断：np.clip(A,5,9) # 小于5为5，大于9为9 Numpy 索引一维索引123456import numpy as npA = np.arange(3,15)# array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) print(A[3]) # 6 二维索引1print(A[1][1]) # 8 这一脚本中的flatten是一个展开性质的函数，将多维的矩阵进行展开成1行的数列。而flat是一个迭代器，本身是一个object属性。 12345678910111213import numpy as npA = np.arange(3,15).reshape((3,4)) print(A.flatten()) # array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])for item in A.flat: print(item) # 3# 4……# 14 Numpy array 合并 上下合并 - np.vstack() 123456789import numpy as npA = np.array([1,1,1])B = np.array([2,2,2]) print(np.vstack((A,B))) # vertical stack"""[[1,1,1] [2,2,2]]""" 左右合并 - np.hstack() 1234567D = np.hstack((A,B)) # horizontal stackprint(D)# [1,1,1,2,2,2]print(A.shape,D.shape)# (3,) (6,) 转置操作 - np.newaxis() 123456789101112131415print(A[np.newaxis,:])# [[1 1 1]]print(A[np.newaxis,:].shape)# (1,3)print(A[:,np.newaxis])"""[[1][1][1]]"""print(A[:,np.newaxis].shape)# (3,1) 此时我们便将具有3个元素的array转换为了1行3列以及3行1列的矩阵了。 12345678910111213141516import numpy as npA = np.array([1,1,1])[:,np.newaxis]B = np.array([2,2,2])[:,np.newaxis] C = np.vstack((A,B)) # vertical stackD = np.hstack((A,B)) # horizontal stackprint(D)"""[[1 2][1 2][1 2]]"""print(A.shape,D.shape)# (3,1) (3,2) 合并操作需要针对多个矩阵或序列 - np.concatenate() 当你的合并操作需要针对多个矩阵或序列时，借助concatenate函数可能会让你使用起来比前述的函数更加方便： 1234567891011121314151617181920212223242526C = np.concatenate((A,B,B,A),axis=0)print(C)"""array([[1], [1], [1], [2], [2], [2], [2], [2], [2], [1], [1], [1]])"""D = np.concatenate((A,B,B,A),axis=1)print(D)"""array([[1, 2, 2, 1], [1, 2, 2, 1], [1, 2, 2, 1]])""" axis参数很好的控制了矩阵的纵向或是横向打印，相比较vstack和hstack函数显得更加方便。 Numpy array 分割1234567A = np.arange(12).reshape((3, 4))print(A)"""array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])""" 纵向分割 12345678print(np.split(A, 2, axis=1))"""[array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])]""" 横向分割 123print(np.split(A, 3, axis=0))# [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] 错误的分割 范例的Array只有4列，只能等量对分，因此输入以上程序代码后Python就会报错。 123print(np.split(A, 3, axis=1))# ValueError: array split does not result in an equal division 不等量的分割 12345678910print(np.array_split(A, 3, axis=1))"""[array([[0, 1], [4, 5], [8, 9]]), array([[ 2], [ 6], [10]]), array([[ 3], [ 7], [11]])]""" 其他的分割方式 在Numpy里还有np.vsplit()与横np.hsplit()方式可用。 12345678910111213print(np.vsplit(A, 3)) #等于 print(np.split(A, 3, axis=0))# [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])]print(np.hsplit(A, 2)) #等于 print(np.split(A, 2, axis=1))"""[array([[0, 1], [4, 5], [8, 9]]), array([[ 2, 3], [ 6, 7], [10, 11]])]""" Numpy copy &amp; deep copy copy() 的赋值方式没有关联性 12345b = a.copy() # deep copyprint(b) # array([11, 22, 33, 3])a[3] = 44print(a) # array([11, 22, 33, 44])print(b) # array([11, 22, 33, 3]) Pandas 教程Numpy 和 Pandas 有什么不同如果用 python 的列表和字典来作比较, 那么可以说 Numpy 是列表形式的，没有数值标签，而 Pandas 就是字典形式。Pandas是基于Numpy构建的，让Numpy为中心的应用变得更加简单。 Series 1234567891011121314import pandas as pdimport numpy as nps = pd.Series([1,3,6,np.nan,44,1])print(s)"""0 1.01 3.02 6.03 NaN4 44.05 1.0dtype: float64""" Series的字符串表现形式为：索引在左边，值在右边。由于我们没有为数据指定索引。于是会自动创建一个0到N-1（N为长度）的整数型索引。 DataFrame 12345678910111213dates = pd.date_range('20160101',periods=6)df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=['a','b','c','d'])print(df)""" a b c d2016-01-01 -0.253065 -2.071051 -0.640515 0.6136632016-01-02 -1.147178 1.532470 0.989255 -0.4997612016-01-03 1.221656 -2.390171 1.862914 0.7780702016-01-04 1.473877 -0.046419 0.610046 0.2046722016-01-05 -1.584752 -0.700592 1.487264 -1.7782932016-01-06 0.633675 -1.414157 -0.277066 -0.442545""" DataFrame是一个表格型的数据结构，它包含有一组有序的列，每列可以是不同的值类型（数值，字符串，布尔值等）。DataFrame既有行索引也有列索引， 它可以被看做由Series组成的大字典。 DataFrame 的一些简单运用 1234567891011print(df['b'])"""2016-01-01 -2.0710512016-01-02 1.5324702016-01-03 -2.3901712016-01-04 -0.0464192016-01-05 -0.7005922016-01-06 -1.414157Freq: D, Name: b, dtype: float64""" 我们在创建一组没有给定行标签和列标签的数据 df1: 123456789df1 = pd.DataFrame(np.arange(12).reshape((3,4)))print(df1)""" 0 1 2 30 0 1 2 31 4 5 6 72 8 9 10 11""" 默认的从0开始 index. 还有一种生成 df 的方法, 如下 df2: 12345678910111213141516df2 = pd.DataFrame(&#123;'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical(["test","train","test","train"]), 'F' : 'foo'&#125;) print(df2)""" A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo""" 这种方法能对每一列的数据进行特殊对待. 如果想要查看数据中的类型, 我们可以用 dtype 这个属性: 123456789101112print(df2.dtypes)"""df2.dtypesA float64B datetime64[ns]C float32D int32E categoryF objectdtype: object""" 如果想看对列的序号: 123print(df2.index)# Int64Index([0, 1, 2, 3], dtype='int64') 同样, 每种数据的名称也能看到: 123print(df2.columns)# Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object') 果只想看所有df2的值: 12345678print(df2.values)"""array([[1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'], [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo']], dtype=object)""" 想知道数据的总结, 可以用 describe(): 12345678910111213df2.describe()""" A C Dcount 4.0 4.0 4.0mean 1.0 1.0 3.0std 0.0 0.0 0.0min 1.0 1.0 3.025% 1.0 1.0 3.050% 1.0 1.0 3.075% 1.0 1.0 3.0max 1.0 1.0 3.0""" 如果想翻转数据, transpose: 1234567891011121314151617181920print(df2.T)""" 0 1 2 \A 1 1 1 B 2013-01-02 00:00:00 2013-01-02 00:00:00 2013-01-02 00:00:00 C 1 1 1 D 3 3 3 E test train test F foo foo foo 3 A 1 B 2013-01-02 00:00:00 C 1 D 3 E train F foo """ 如果想对数据的 index 进行排序并输出: 12345678910111213141516171819202122print(df2.sort_index(axis=1, ascending=False))""" F E D C B A0 foo test 3 1.0 2013-01-02 1.01 foo train 3 1.0 2013-01-02 1.02 foo test 3 1.0 2013-01-02 1.03 foo train 3 1.0 2013-01-02 1.0"""如果是对数据 值 排序输出:```pythonprint(df2.sort_values(by='B'))""" A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo""" 参考： Pandas 基本介绍]]></content>
      <categories>
        <category>编程</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux入门教程]]></title>
    <url>%2F2018%2F03%2F25%2Flinux%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Linux入门教程SSH 远程连接 LinuxLinux端配置1sudo apt-get install openssh-server 查看IP地址：1ifconfig Mac端配置1ssh 用户名@IP地址 #输入密码确认 免密登录Mac端： 12ssh-keygen -t rsa -f test -C "test-key" # 一直回车cat test.pub # 查看公钥内容 配置公钥到服务器：将公钥内容添加到服务器的~/.ssh/authorized_keys 文件中. 例子： 1scp /home/yourname/.ssh/authorized_keys yourname@192.168.38.58:/home/yourname/.ssh/ alias 实现命令快速登陆：做好配置之后，通过ssh可以直接登录了。对经常登录的服务器，可以将ssh登录命令的alias加到 ~/.bash_profile文件中。 12$ cat ~/.bash_profile | grep 101alias to-101='ssh huqiu@192.168.154.101' 登录的时候: 1$ to-101 无法登录一般的原因： 客户端的私钥和公钥文件位置必须位于 ~/.ssh 下。 确保双方 ~/.ssh 目录，父目录，公钥私钥，authorized_keys 文件的权限对当前用户至少要有执行权限，对其他用户最多只能有执行权限。 注意git登录，要求对公钥和私钥以及config文件，其他用户不能有任何权限。 服务器端 ~/.ssh/authorized_keys 文件名确保没错 :). ssh-copy-id：ssh-copy-id 是一个小脚本，你可以用这个小脚本完成以上工作。这个脚本在linux系统上一般都有。 ssh-keygen 基本用法 1) 使用 ssh-keygen 时，请先进入到 ~/.ssh 目录，不存在的话，请先创建。并且保证 ~/.ssh 以及所有父目录的权限不能大于 711 2) 使用 ssh-kengen 会在~/.ssh/目录下生成两个文件，不指定文件名和密钥类型的时候，默认生成的两个文件是： id_rsa id_rsa.pub 第一个是私钥文件，第二个是公钥文件。 生成ssh key的时候，可以通过 -f 选项指定生成文件的文件名，如下: 1ssh-keygen -f test -C "test key" # test - 文件名，"test key" - 备注 SSH config1234567Host example # 关键词HostName example.com # 主机地址User root # 用户名IdentityFile ~/.ssh/id_ecdsa # 认证文件Port 22 # 指定端口ControlMaster autoControlPath /tmp/%r@%h:%p OpenCV3.1配置安装依赖库12345678910sudo apt-get install build-essential# 必须的，gcc编译环境sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev# 必须的,包括cmake等工具sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev# 可选的，看个人需要，总共5M左右sudo apt-get install libv4l-dev 下载 源码OpenCV 或着用git clone： 1234cd ~/opencv310 # opencv310为自己建的，源码将放在这里git clone https://github.com/Itseez/opencv.gitgit clone https://github.com/Itseez/opencv_contrib.git CMake Opencv源码建立一个编译目录（例如：/build）把cmake后的文件都放在这里边。 123cd ~/opencvmkdir build //建立一个build目录，把cmake的文件都放着里边cd build //进入build目录 cmake时ippicv_linux_20151201.tgz总是不能成功下载，故cmake之前将./downloads/linux-808b791a6eac9ed78d32a7666804320e 文件拷贝至./opencv-3.1.0/3rdparty/ippicv/ 路径下(先执行一次cmake 命令生成文件路径，在将ippicv_linux_20151201.tgz复制进去) ippicv_linux_20151201.tgz下载链接:链接: https://pan.baidu.com/s/1jBBPxXX_NqCodS5bAln4-g 密码: x4sn 然后开始cmake，这里需要注意几个cmake的参数，比较重要。 1sudo cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local WITH_LIBV4L=ON .. 切记最后’..’两个点之前要加空格！！ 把代码编译成可执行文件这里官方推荐使用多进程编译，推荐七个进程： 1make -j7 # 并行运行七个jobs，这一步也在build目录中进行 安装1234sudo make install如果你要在python下运行opencv库的情况下，那就必须安装安装python-opencvsudo apt-get install python-opencv 配置库文件路径1234/bin/bash -c 'echo "/usr/local/lib" &gt; /etc/ld.so.conf.d/opencv.conf'#或者直接打开/etc/ld.so.conf.d/opencv.conf，添加/usr/local/lib#使配置生效sudo ldconfig(重要) 设置环境变量12345678sudo vim/etc/bash.bashrc #在最后加入以下两行代码PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig export PKG_CONFIG_PATH #使配置生效sudo source /etc/bash.bashrc （该步骤可能会报错找不到命令，原因是source为root命令su（进入root权限） 查看12pkg-config --modversion opencvpkg-config --cflags opencv 运行测试我是用python+opencv的，我这里直接运行opencv自带的python的例子程序，TX1自带摄像头不能用，需要使用外接USB摄像头，插入USB接口即可，无需安装驱动，也无需改动测试代码。 1234cd &lt;opencv3.1.0_dir&gt;/samples/python/python video.pypython edge.pypython facedetect.py 参考 https://www.liaohuqiu.net/cn/posts/ssh-public-key-auto-login/ https://blog.csdn.net/asukasmallriver/article/details/72927860 https://blog.csdn.net/u011440558/article/details/78358447]]></content>
      <categories>
        <category>编程</category>
        <category>Linux</category>
        <category>入门</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu_16.04_env]]></title>
    <url>%2F2018%2F03%2F25%2Fubuntu-16-04-env%2F</url>
    <content type="text"><![CDATA[Ubuntu 16.04 环境配置PCL1.8环境第一步：安装依赖1234567891011121314sudo apt-get update sudo apt-get install git build-essential linux-libc-dev sudo apt-get install cmake cmake-gui sudo apt-get install libusb-1.0-0-dev libusb-dev libudev-dev sudo apt-get install mpi-default-dev openmpi-bin openmpi-common sudo apt-get install libflann1.8 libflann-dev sudo apt-get install libeigen3-dev sudo apt-get install libboost-all-dev sudo apt-get install libvtk5.10-qt4 libvtk5.10 libvtk5-dev sudo apt-get install libqhull* libgtest-dev sudo apt-get install freeglut3-dev pkg-config sudo apt-get install libxmu-dev libxi-dev sudo apt-get install mono-complete sudo apt-get install qt-sdk openjdk-8-jdk openjdk-8-jre 第二步：下载源码1git clone https://github.com/PointCloudLibrary/pcl.git 第三步：编译源码1234567cd pcl mkdir release cd release cmake -DCMAKE_BUILD_TYPE=None -DCMAKE_INSTALL_PREFIX=/usr \ -DBUILD_GPU=ON -DBUILD_apps=ON -DBUILD_examples=ON \ -DCMAKE_INSTALL_PREFIX=/usr .. make 1sudo make install 第四步（可选 and 建议）：如果需要PCLVisualizer安装OpenNI、OpenNI2 12sudo apt-get install libopenni-dev sudo apt-get install libopenni2-dev 安装ensensor 12sudo dpkg -i ensenso-sdk-2.0.147-x64.deb sudo dpkg -i codemeter_6.40.2402.501_amd64.deb 如缺少依赖： 1sudo apt-get -f install 参考： https://blog.csdn.net/dantengc/article/details/78446600 OpenCV环境Caffe 环境]]></content>
      <categories>
        <category>编程</category>
        <category>Linux</category>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[semap论文]]></title>
    <url>%2F2018%2F03%2F25%2Fsemap%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[SeMap: 数字化室内语义地图低成本自动化构建方法场景图重建使用Colmap工具来获取场景图重建结果 Colmap使用方法场景图通过Feature Detection and Extraction和Feature Matching and Geometric Verification两步完成。 首先使用GUI来操作。 打开Colmap（1）Windows: COLMAP.bat Mac: COLMAP.app Linux可选为：./src/exe/colmap gui （2）新建项目-GUI操作 File &gt; New project 在此步，首先确定生成的数据库保存位置，还要确定输入图片位置。为了方便起见，能够通过File &gt; Save project来保存整个项目。 File &gt; Open project为打开现有项目。 （3）新建项目-命令行操作 colmap gui 或 colmap gui –project_path path/to/project.ini （4）例子 12345678/path/to/project/...+── images│ +── image1.jpg│ +── image2.jpg│ +── ...│ +── imageN.jpg+── database.db+── project.ini 其中，/path/to/project/images为图片路径，/path/to/project/database.db为数据库保存路径，/path/to/project/project.ini为项目配置保存路径。 Feature Detection and Extraction(1) GUI操作 Processing &gt; Extract features 如果导入现有的特征提取的文件，需有如下文件：NUM_FEATURES 128 X Y SCALE ORIENTATION D_1 D_2 D_3 … D_128…X Y SCALE ORIENTATION D_1 D_2 D_3 … D_128where X, Y, SCALE, ORIENTATION are floating point numbers and D_1…D_128 values in the range 0…255. The file should have NUM_FEATURES lines with one line per feature. For example, if an image has 4 features, then the text file should look something like this:4 1281.2 2.3 0.1 0.3 1 2 3 4 … 212.2 3.3 1.1 0.3 3 2 3 2 … 320.2 1.3 1.1 0.3 3 2 3 2 … 21.2 2.3 1.1 0.3 3 2 3 2 … 3 Feature Matching and Geometric VerificationProcessing &gt; Match features 查看结果（1）Database Management Processing &gt; Manage database （2）Database Format SQLite database file The database contains the following tables: cameras images keypoints descriptors matches inlier_matches Cameras and Images： The relation between cameras and images is 1-to-N. Keypoints and Descriptors：The detected keypoints are stored as row-major float32 binary blobs, where the first two columns are the X and Y locations in the image, respectively. Matches：Feature matching stores its output in the matches table and geometric verification in the inlier_matches table. COLMAP only uses the data in inlier_matches for reconstruction. Every entry in the two tables stores the feature matches between two unique images, where the pair_id is the row-major, linear index in the upper-triangular match matrix, generated as follows: 12345def image_ids_to_pair_id(image_id1, image_id2): if image_id1 &gt; image_id2: return 2147483647 * image_id2 + image_id1 else: return 2147483647 * image_id1 + image_id2 and image identifiers can be uniquely determined from the pair_id as: 1234def pair_id_to_image_ids(pair_id): image_id2 = pair_id % 2147483647 image_id1 = (pair_id - image_id2) / 2147483647 return image_id1, image_id2 The pair_id enables efficient database queries, as the matches tables may contain several hundred millions of entries. This scheme limits the maximum number of images in a database to 2147483647 (maximum value of signed 32-bit integers), i.e. image_id must be smaller than 2147483647. The binary blobs in the matches tables are row-major uint32 matrices, where the left column are zero-based indices into the features of image_id1 and the second column into the features of image_id2. The column cols must be 2 and the rows column specifies the number of feature matches.]]></content>
      <categories>
        <category>学术</category>
        <category>semap</category>
      </categories>
      <tags>
        <tag>学术</tag>
        <tag>semap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2F2018%2F03%2F24%2Fcategories%2F</url>
    <content type="text"><![CDATA[Categories 编程: programming C++ Java Python Matlab Android Linux Git Latex PHP IOS 数据库 网络编程 多线程 QT编程 MarkDown DP Caffe TF PyTorch 数据结构 算法 学术: science 计算机视觉 ICCV CVPR ECCV 移动计算 MobiCom SigComm InfoCom NSDI SenSys PerCom UbiComp 机器学习&amp;深度学习 AAAI IJCAI 理论: theory 计算机视觉 移动计算 数学 优化算法 机器学习 其他: other Hexo博客 生活 Tag 论文 理论 其他 等等]]></content>
      <categories>
        <category>其他</category>
        <category>categories</category>
      </categories>
      <tags>
        <tag>categories</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双目成像深度原理]]></title>
    <url>%2F2018%2F03%2F24%2F%E5%8F%8C%E7%9B%AE%E6%88%90%E5%83%8F%E6%B7%B1%E5%BA%A6%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[相机内参数与相机自身特性相关的参数，比如相机的焦距、像素大小等； 相机外参数在世界坐标系中的参数，比如相机的位置、旋转方向等 坐标系转换坐标系图像处理、立体视觉等等方向常常涉及到四个坐标系：世界坐标系、相机坐标系、图像坐标系、像素坐标系。例如下图： 坐标系转换世界坐标系与相机坐标系 于是，从世界坐标系到相机坐标系，涉及到旋转和平移（其实所有的运动也可以用旋转矩阵和平移向量来描述）。绕着不同的坐标轴旋转不同的角度，得到相应的旋转矩阵，如下图所示： 从世界坐标系到相机坐标系的转换关系如下所示： 那么点P在相机坐标系的坐标为： 相机坐标系与图像坐标系相机坐标系到图像坐标系，属于透视投影关系，从3D转换到2D。 此时投影点p的单位还是mm，并不是pixel，需要进一步转换到像素坐标系。 图像坐标系与像素坐标系像素坐标系和图像坐标系都在成像平面上，只是各自的原点和度量单位不一样。图像坐标系的原点为相机光轴与成像平面的交点，通常情况下是成像平面的中点或者叫principal point。图像坐标系的单位是mm，属于物理单位，而像素坐标系的单位是pixel，我们平常描述一个像素点都是几行几列。所以这二者之间的转换如下：其中dx和dy表示每一列和每一行分别代表多少mm，即1pixel=dx mm 总结那么通过上面四个坐标系的转换就可以得到一个点从世界坐标系如何转换到像素坐标系的。 其中，为相机的内参，为相机的外参，fx, fy, cx/dx+u0, cy/dy+v0的单位均为像素。相机的内参和外参可以通过张正友标定获取,如下小节介绍。通过最终的转换关系来看，一个三维中的坐标点，的确可以在图像中找到一个对应的像素点，但是反过来，通过图像中的一个点找到它在三维中对应的点就很成了一个问题，因为我们并不知道等式左边的Zc的值。 相机内参K在计算机视觉中，摄像机内参数矩阵如下： 其中 f 为摄像机的焦距，单位一般是mm;dx,dy 为像元尺寸;u0,v0 为图像中心。fx = f/dx, fy = f/dy,分别称为x轴和y轴上的归一化焦距. 为更好的理解，举个实例：现以NiKon D700相机为例进行求解其内参数矩阵：就算大家身边没有这款相机也无所谓，可以在网上百度一下，很方便的就知道其一些参数—— 焦距 f = 35mm 最高分辨率：4256×2832 传感器尺寸：36.0×23.9 mm根据以上定义可以有：u0= 4256/2 = 2128 v0= 2832/2 = 1416 dx = 36.0/4256 dy = 23.9/2832fx = f/dx = 4137.8 fy = f/dy = 4147.3 分辨率可以从显示分辨率与图像分辨率两个方向来分类。[1]显示分辨率（屏幕分辨率）是屏幕图像的精密度，是指显示器所能显示的像素有多少。由于屏幕上的点、线和面都是由像素组成的，显示器可显示的像素越多，画面就越精细，同样的屏幕区域内能显示的信息也越多，所以分辨率是个非常重要的性能指标之一。可以把整个图像想象成是一个大型的棋盘，而分辨率的表示方式就是所有经线和纬线交叉点的数目。显示分辨率一定的情况下，显示屏越小图像越清晰，反之，显示屏大小固定时，显示分辨率越高图像越清晰。[2]图像分辨率则是单位英寸中所包含的像素点数，其定义更趋近于分辨率本身的定义。 畸变参数（与点集如何畸变的2D几何相关。）采用理想针孔模型，由于通过针孔的光线少，摄像机曝光太慢，在实际使用中均采用透镜，可以使图像生成迅速，但代价是引入了畸变。 有两种畸变对投影图像影响较大： 径向畸变和切向畸变。 (1) 对某些透镜，光线在远离透镜中心的地方比靠近中心的地方更加弯曲，产生“筒形”或“鱼眼”现象，称为径向畸变。一般来讲，成像仪中心的径向畸变为0，越向边缘移动，畸变越严重。不过径向畸变可以通过下面的泰勒级数展开式来校正： 这里（x, y）是畸变点在成像仪上的原始位置，r为该点距离成像仪中心的距离，（xcorrected ，ycorrected ）是校正后的新位置。 对于一般的摄像机校正，通常使用泰勒级数中的前两项k1和k2就够了；对畸变很大的摄像机，比如鱼眼透镜，可以使用第三径向畸变项k3. (2) 当成像仪被粘贴在摄像机的时候，会存在一定的误差，使得图像平面和透镜不完全平行，从而产生切向畸变。也就是说，如果一个矩形被投影到成像仪上时，可能会变成一个梯形。切向畸变可以通过如下公式来校正： 这里（x, y）是畸变点在成像仪上的原始位置，r为该点距离成像仪中心的距离，（xcorrected ，ycorrected ）是校正后的新位置。 摄像机的外参数旋转向量（大小为1×3的矢量或旋转矩阵3×3）和平移向量（tx,ty,tz）。 旋转向量:旋转向量是旋转矩阵紧凑的变现形式，旋转向量为1×3的行矢量。 r就是旋转向量，旋转向量的方向是旋转轴 ,旋转向量的模为围绕旋转轴旋转的角度。 通过上面的公式，我们就可以求解出旋转矩阵R。同样的已知旋转矩阵，我们也可以通过下面的公式求解得到旋转向量： 相机标定（或摄像机标定）摄像机标定(Camera calibration)简单来说是从世界坐标系换到图像坐标系的过程，也就是求最终的投影矩阵的过程。 那为什么要做相机标定呢？【1】进行摄像机标定的目的：求出相机的内、外参数，以及畸变参数。【2】标定相机后通常是想做两件事：一个是由于每个镜头的畸变程度各不相同，通过相机标定可以校正这种镜头畸变矫正畸变，生成矫正后的图像；另一个是根据获得的图像重构三维场景。摄像机标定过程，简单的可以简单的描述为通过标定板，如下图，可以得到n个对应的世界坐标三维点Xi和对应的图像坐标二维点xi，这些三维点到二维点的转换都可以通过上面提到的相机内参K，相机外参R和t，以及畸变参数D，经过一系列的矩阵变换得到。 标定原理原理推导请参考张氏法相机标定 标定方法摄像机标定方法分类 使用Matlab相机标定工具箱标定相机的基本步骤参考： http://www.cnblogs.com/star91/p/6012425.html https://blog.csdn.net/wangxiaokun671903/article/details/38925553 使用OpenCV进行相机标定参考： Opencv 张正友相机标定傻瓜教程 使用OpenCV进行相机标定(基于OpenCV2.4.3) 张正友相机标定Opencv实现以及标定流程&amp;&amp;标定结果评价&amp;&amp;图像矫正流程解析（附标定程序和棋盘图） 双目立体视觉深度相机详细原理理想双目相机成像模型首先我们从理想的情况开始分析:假设左右两个相机位于同一平面（光轴平行），且相机参数（如焦距f）一致。那么深度值的推导原理和公式如下。公式只涉及到初中学的三角形相似知识，不难看懂。 根据上述推导，空间点P离相机的距离（深度）z=f*b/d，可以发现如果要计算深度z，必须要知道： （1）、相机焦距f，左右相机基线b。这些参数可以通过先验信息或者相机标定得到。 （2）、视差d。需要知道左相机的每个像素点(xl, yl)和右相机中对应点(xr, yr)的对应关系。这是双目视觉的核心问题。 极线约束那么问题来了，对于左图中的一个像素点，如何确定该点在右图中的位置？是不是需要我们在整个图像中地毯式搜索一个个匹配？ 答案是：不需要。因为有极线约束（名字听着很吓人）。极线约束对于求解图像对中像素点的对应关系非常重要。 那什么是极线呢？如下图所示。C1，C2是两个相机，P是空间中的一个点，P和两个相机中心点C1、C2形成了三维空间中的一个平面PC1C2，称为极平面（Epipolar plane）。极平面和两幅图像相交于两条直线，这两条直线称为极线(Epipolar line)。P在相机C1中的成像点是P1，在相机C2中的成像点是P2，但是P的位置事先是未知的。 我们的目标是：对于左图的P1点，寻找它在右图中的对应点P2，这样就能确定P点的空间位置，也就是我们想要的空间物体和相机的距离（深度）。 所谓极线约束（Epipolar Constraint）就是指当同一个空间点在两幅图像上分别成像时，已知左图投影点p1，那么对应右图投影点p2一定在相对于p1的极线上，这样可以极大的缩小匹配范围。 根据极线约束的定义，我们可以在下图中直观的看到P2一定在对极线上，所以我们只需要沿着极线搜索一定可以找到和P1的对应点P2。 细心的朋友会发现上述过程考虑的情况（两相机共面且光轴平行，参数相同）非常理想，相机C1，C2如果不是在同一直线上怎么办？ 事实上，这种情况非常常见，因为有些场景下两个相机需要独立固定，很难保证光心C1，C2完全水平，即使是固定在同一个基板上也会因为装配的原因导致光心不完全水平。如下图所示。我们看到两个相机的极线不仅不平行，还不共面，之前的理想模型那一套推导结果用不了了，这可咋办呢？ 把不理想情况转化为理想情况不就OK了！这就是图像矫正（Image Rectification）技术。 参考：深度相机原理揭秘–双目立体视觉 参考 http://www.bijishequ.com/detail/397458 https://www.cnblogs.com/Jessica-jie/p/6596450.html https://zhuanlan.zhihu.com/p/32199990 https://cloud.tencent.com/developer/article/1015777]]></content>
      <categories>
        <category>理论</category>
        <category>计算机视觉</category>
        <category>成像原理</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于hexo+github免费搭建个人博客]]></title>
    <url>%2F2018%2F03%2F04%2F%E5%9F%BA%E4%BA%8EHexo%2BGitHub%20Pages%20%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[一、Hexo简介Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。这里我们选用的是GitHub。Hexo同时也是GitHub上的开源项目，参见：hexojs/hexo 如果想要更加全面的了解Hexo，可以到其官网 Hexo 了解更多的细节，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。 搭建步骤三、环境配置1. 本机环境配置 安装Node.js 下载Node.js,注意安装Node.js会包含环境变量及npm的安装，安装后，检测Node.js是否安装成功，在命令行中输入 node -v. 检测npm是否安装成功，在命令行中输入npm -v 安装Hexo Hexo就是我们的个人博客网站的框架， 这里需要自己在电脑常里创建一个文件夹，可以命名为Blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。创建好后，进入文件夹中,使用npm命令安装Hexo，输入： 123npm install -g hexo #等待一会就会完成下载安装。hexo init #该命令会在目标文件夹内建立网站所需要的所有文件npm install #安装依赖包 到这里本地博客就搭建好了。执行以下命令（在你博客的对应文件夹路径下）: 12hexo generate # Or hexo ghexo server # Or hexo s 在浏览器输入http://localhost:4000/ 就可以进行查看了。当然这个博客是本地的，别人是无法访问的，之后我们需要部署到GitHub上。常用的Hexo 命令: 12345678910111213npm install hexo -g #安装Hexonpm update hexo -g #升级hexo init #初始化博客# 命令简写hexo n "我的博客" == hexo new "我的博客" #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令 2. git环境配置 注册Github账号并新建仓库 注册过程就不多说了，注册完成之后需要新建一个仓库。需要注意的是新创建的仓库的名字，必须是username.github.io。例如我的username是XXX，那么新创建的仓库的名字便是XXX.github.io。 配置SSH Key 这一步不是必须的，配置SSH Key的话之后每次更新博客就不用都输入用户名和密码，可以方便一些。 (1)检查本机上是否已经存在SSH Key。打开终端，输入如下命令： 12cd .sshls -la 检查终端输出的文件列表中是否已经存在id_rsa.pub 或 id_dsa.pub 文件，如果文件已经存在，则直接进入第三步。 (2)创建一个SSH Key。在终端输入如下命令: 1ssh-keygen -t rsa -C "your_email@example.com" 按下回车，让你输入文件名，直接回车会创建使用默认文件名的文件(推荐使用默认文件名)，然后会提示你输入两次密码，可以为空。 (3)添加SSH Key到Github 如果你没有指定文件名（也就是使用的默认文件名），那么你的.ssh文件夹下，应该有一个id_rsa.pub文件了，打开该文件，复制里面的文本。然后登录Github，点击右上角头像右边的三角图标，点击Settings，然后在左边菜单栏点击SSH and GPG keys，点击New SSH key，Title 随便填一个，在Key栏填入你复制的内容，点击Add SSH key，就添加成功了。 (4)检验SSH Key是否配置成功。在终端输入如下命令: 1ssh -T git@github.com 如果出现: 1Are you sure you want to continue connecting (yes/no)? 请输入yes再按回车。如果最后出现: 1Hi username! You've successfully authenticated, but GitHub does not provide shell access. 就说明你的SSH Key配置成功了。 3. 同步本地博客到Github 上面只是在本地预览，接下来要做的就是就是推送网站，也就是发布网站，让我们的网站可以被更多的人访问。在设置之前，需要解释一个概念，在blog根目录里的_config.yml文件称为站点配置文件. 我们的Hexo与GitHub关联起来，打开站点的配置文件_config.yml，翻到最后修改为： 1234deploy: type: gitrepo: 这里填入你之前在GitHub上创建仓库的完整路径，记得加上 .gitbranch: master参考如下： 例子： 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:type: gitrepo: https://github.com/xiaoqiangteng/xiaoqiangteng.github.io.gitbrabch: master 保存站点配置文件。其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令： 1npm install hexo-deployer-git --save 这时，我们分别输入三条命令： 123hexo clean hexo g hexo d 其实第三条的 hexo d 就是部署网站命令，d是deploy的缩写。完成后，打开浏览器，在地址栏输入你的放置个人网站的仓库路径 发布新的博客 既然博客已经搭建好了，那么不发几篇博文有就没有意义了，使用下面的命令来新建一篇叫做”brightloong”的文章。 1hexo new 'brightloong' - 命令执行之后，你会在你文件博客根目录的source/_post目录下找到你刚刚新建的md后缀的文件，hexo博客是使用markdown语法来书写的，如果不熟悉markdown语法可以快速的看一下[markdown](https://www.appinn.com/markdown/)语法说明. &gt; 注意：在冒号后面一定要加上一个空格，否则在生成静态文件的时候会报错，并且也不能将其成功推送到github。 123456---title: brightloong #文章标题date: 2017-02-24 12:03:12 #创建时间tags: #文章标签，如果有多个标签可以使用[1,2,3]的形式，还有其他形式自己摸索吧---#这之后是正文 - 文章编写好之后，只用以下命令生成静态文件并推送到github上，执行完成后打开自己的博客页面，是不是发现刚刚编写的文章出现了；如果你想删除某一篇文章，那么在source/_post目录下找到对应的文章将其删除后，同样执行一下命令就OK了。 站点配置文件_config.yml 站点配置文件_config.yml是在你博客保存目录的根目录下，注意将它与主题配置文件进行区分，我使用的主题是Next主题。下面我先介绍下站点配置文件，我将一些主要的配置做了注释，如果你想了解更多的配置的含义和作用，请访问Hexo官方教程查看。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: BrightLoong's Blog #博客的标题subtitle: #子标题description: Remember what should be remembered, and forget what should be forgotten.Alter what is changeable, and accept what is mutable. #博客描述，可以是一段你喜欢的话，也可以是你博客的描述，只要你开心就好。author: BrightLoong #作者language: zh-Hans #语言（我使用的是简体中文）timezone: #时区（默认使用电脑时间）##之下的保持默认就好，没有什么需要更改的# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' #and root as '/child/'url: https://brightloong.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: source #source目录public_dir: publictag_dir: tags #标签目录archive_dir: archives category_dir: categories #分类目录code_dir: downloads/codei18n_dir: :langskip_render: static/** #注意这个属性（跳过渲染），你暂时不用配置，我之后会讲到，这个也是我遇到的坑##之下的保持默认就好，没有什么需要更改的# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next #你设置的主题，接下来我会说到这个# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/BrightLoong/BrightLoong.github.io.git branch: master 设置专属域名博客搭建好后，我们可以通过之前设置好的GitHub仓库地址来访问，比如：http://XXX.github.io，而且GitHub是免费替我们托管的的，如果我们想要设置自己的专属的域名，我们可以去阿里云购买域名，我们点击添加记录，设置主机记录为@，类型为A，到IP 192.30.252.153（固定值）。按照如上设置完成之后， 可能不会立即生效，等个几分钟，在./source目录下新建文件CNAME（没有后缀名），文件中写上我们要绑定的域名，例如: XXX.com.部署到GitHub上。这时就可以通过http://XXX.com访问. 四、Hexo配置主题设置搭建自己的博客，最吸引人的莫过于那千变万化的主题了，大家可以在Hexo官网上看到无数漂亮、大方、简洁的主题。本人使用的是简洁的Next主题，你可以选择你喜欢的下载下来，将其解压放入themes目录中，比如我的目录是.\themes，然后修改我在上面提到的站点配置文件中的theme属性，为你刚刚放入themes目录中文件的名字（最好是对解压文件修改一个名字，否则名字可能会比较长，我把我下载下来的主题改文了next）,做完这些之后并不代表你完成了，你还需要参考你所下载的主题所说的配置步骤进行相关的配置，由于不同的主题配置过程也尽不相同，大家根据自己下载的主题去配置，我在这里只说我使用的Next主题如何配置。 1theme: next 注意：从下面开始所说的都是Next主题的相关配置。 如果你使用的和我一样，也是Next的主题，那么你最好还是看官方提供Next使用文档，并且文档是中文版的,我也仅仅是讲一些容易被忽略的配置，以及我使用的配置，以及在使用过程中遇到的问题;至于如何更换头像，添加分类和标签页面、切换主题样式（Next主题包含3中样式）之类的，大家还是照着官方的做更好。 配置网站图标 如何让网站前能显示自己想要的图标，我当时也是找了很久，最后发现是在主题配置文件（我的是F:\myblog\themes\next_config.yml）的最前面，有一个favicon属性，我把一个名字叫favicon.ico的图片放到了F:\myblog\source下，然后配置如下： 1favicon: /favicon.ico 首页显示阅读全文按钮 首页的文章是不是默认展开了，显示出了整篇文章，怎么才能显示出如下的阅读全文的按钮。在主题配置文件中找到auto_excerpt属性进行配置: 123auto_excerpt: enable: true #改写为true length: 150 #默认展示的高度 你也可以在自己的博文中添加\&lt;!--more--&gt;来决定在首页展示到什么位置（我就喜欢用这种方式），这个标签后的内容就不会展示到首页啦。 修改文章内链接文本样式修改文件 themes\next\source\css_common\components\post\post.styl，在末尾添加如下css样式，： 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 其中选择.post-body 是为了不影响标题，选择 p 是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义。 在每篇文章末尾统一添加“本文结束”标记]]></content>
      <categories>
        <category>其他</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo使用攻略-添加分类及标签]]></title>
    <url>%2F2018%2F03%2F04%2FHexo%E4%BD%BF%E7%94%A8%E6%94%BB%E7%95%A5-%E6%B7%BB%E5%8A%A0%E5%88%86%E7%B1%BB%E5%8F%8A%E6%A0%87%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[Hexo使用攻略-添加分类及标签创建“分类”选项###生成“分类”页并添加tpye属性 打开命令行，进入博客所在文件夹。执行命令: 1$ hexo new page categories 成功后会提示：1INFO Created: ~/Documents/blog/source/categories/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的： 12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;categories&quot;--- 保存并关闭文件。 给文章添加“categories”属性打开需要添加分类的文章，为其添加categories属性。下方的categories: web前端表示添加这篇文章到“web前端”这个分类。注意：hexo一篇文章只能属于一个分类，也就是说如果在“- web前端”下方添加“-xxx”，hexo不会产生两个分类，而是把分类嵌套（即该文章属于 “- web前端”下的 “-xxx ”分类）。 123456---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端--- 至此，成功给文章添加分类，点击首页的“分类”可以看到该分类下的所有文章。当然，只有添加了categories: xxx的文章才会被收录到首页的“分类”中。 注意：如果有启用多说 或者 Disqus 评论，默认页面也会带有评论。需要关闭的话，请添加字段 comments 并将值设置为 false，如： 12345title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false--- 或 设置分类列表在我们编辑文章的时候，直接在categories:项填写属于哪个分类，但如果分类是中文的时候，路径也会包含中文。比如分类我们设置的是： 1categories: 编程 那在生成页面后，分类列表就会出现编程这个选项，他的访问路径是： 1那在生成页面后，分类列表就会出现编程这个选项，他的访问路径是： 如果我们想要把路径名和分类名分别设置，需要怎么办呢？ 打开根目录下的配置文件_config.yml，找到如下位置做更改： 1234567# Category &amp; Tagdefault_category: uncategorizedcategory_map: 编程: programming 生活: life 其他: othertag_map: 在这里category_map:是设置分类的地方，每行一个分类，冒号前面是分类名称，后面是访问路径。 可以提前在这里设置好一些分类，当编辑的文章填写了对应的分类名时，就会自动的按照对应的路径来访问。 创建“标签”选项生成“标签”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令 1$ hexo new page tags 成功后会提示： 1INFO Created: ~/Documents/blog/source/tags/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的： 1234---title: 标签date: 2017-05-27 14:22:08--- 添加type: “tags”到内容中，添加后是这样的： 12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;tags&quot;--- 保存并关闭文件。 给文章添加“tags”属性打开需要添加标签的文章，为其添加tags属性。下方的tags:下方的- jQuery - 表格- 表单验证就是这篇文章的标签了 12345678910---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端tags:- jQuery- 表格- 表单验证--- 至此，成功给文章添加分类，点击首页的“标签”可以看到该标签下的所有文章。当然，只有添加了tags: xxx的文章才会被收录到首页的“标签”中。 新建页面的模板打开scaffolds/post.md文件，在tages:上面加入categories:,保存后，重新执行hexo n ‘name’命令，会发现新建的页面里有categories:项了。 scaffolds目录下，是新建页面的模板，执行新建命令时，是根据这里的模板页来完成的，所以可以在这里根据你自己的需求添加一些默认值。 菜单中添加链接编辑主题的 _config.yml ，将 menu 中的 categories: /categories 注释去掉，如下: 12345menu: home: / categories: /categories archives: /archives tags: /tags 在主题配置文件中添加分类选项 在主题配置文件: 1themes/_config.yml 中添加以下代码（#号后为注释内容）: 12345menu: 主页: / 所有文章: /archives 技巧经验: /categories/技巧经验 # 博客首页展示文本： 访问路径/自定义归档名称 资料总结: /categories/资料总结]]></content>
      <categories>
        <category>其他</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
